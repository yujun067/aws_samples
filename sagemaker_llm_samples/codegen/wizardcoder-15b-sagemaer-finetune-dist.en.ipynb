{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ab02e9-879d-4d94-b387-84f31983a64a",
   "metadata": {},
   "source": [
    "# An sample to finetune WizardCoder-15B distributely on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e54d163-082b-4c55-915c-abe4694429b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51608a1-cec7-40d1-9949-3b8d5b733fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess                     = sagemaker.Session()\n",
    "role                     = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account                  = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region                   = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b765b58-7b9b-4d6f-a4e8-12123c5cdb91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "rm -rf src\n",
    "mkdir src\n",
    "cp s5cmd src/\n",
    "cd src\n",
    "\n",
    "git clone https://github.com/nlpxucan/WizardLM.git\n",
    "cd WizardLM\n",
    "git reset --hard 46d1ce7dbbb1f987ae5e5915c75f33b89a6a17ab\n",
    "\n",
    "cd ../\n",
    "git clone https://github.com/lm-sys/FastChat.git\n",
    "cd FastChat\n",
    "git reset --hard 974537efbd82093b45e64d07904efe7728193a52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa7c5d-204c-45e5-94db-84571f2b2d13",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fe813-2dd8-4e5c-b132-606f9745b633",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ddae4-a0b6-4455-b326-8b41c66714b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2712411e-e64a-45ad-8bb2-9b43989855bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"WizardLM/WizardCoder-15B-V1.0\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\", \"*.py\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    "    revision='926ca1b215c4631bc5f8c3e47173381452c23e5c'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd0fbb-daa8-4115-b3a4-6bd2a31a256c",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98f226-ebe9-4524-b050-1ca1e50bbb84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956123f0-ecaf-48b8-9378-6339bc91153d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llm/models/wizardcoder/WizardLM/WizardLM-15B/\n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f5ad50-c165-4dd3-bc3f-30b1cc1468aa",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd4499-3781-4e8a-bb44-e11e64432805",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN pip3 uninstall -y deepspeed \\\n",
    "    && pip3 install deepspeed==0.10.0 \\\n",
    "    && pip3 install transformers==4.30.1 \\\n",
    "    && pip3 install accelerate==0.21.0\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d81018-ffaa-4eec-86da-8efe049cc9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74b230-9ba2-47d1-a475-88d29d0d2782",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7473062-fe72-4a1b-96f3-973d2e87a36a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-wizardcoder-15b-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054f510-210a-4e13-ae13-060a6e94e601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6666c-4446-41f9-a397-d0a2f5169ace",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the deepspeed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0188f-b255-4ead-b78b-451b176c72d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/ds.json\n",
    "{\n",
    "  \"bf16\": {\n",
    "    \"enabled\": true\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": false,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819db7a8-5d15-486c-8a5b-29feaa9fbb97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/WizardLM/WizardCoder/train_mem.py\n",
    "\n",
    "# Make it more memory efficient by monkey patching the LLaMA model with FlashAttn.\n",
    "\n",
    "# # Need to call this before importing transformers.\n",
    "# from llama_flash_attn_monkey_patch import (\n",
    "#     replace_llama_attn_with_flash_attn,\n",
    "# )\n",
    "\n",
    "# replace_llama_attn_with_flash_attn()\n",
    "\n",
    "from train_wizardcoder import train\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "986354fc-9864-4863-86ea-ab274934944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/ds-train-dist.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/ds-train-dist.sh\n",
    "#!/bin/bash\n",
    "CURRENT_HOST=\"${SM_CURRENT_HOST}\"\n",
    "\n",
    "\n",
    "IFS=',' read -ra hosts_array <<< \"${SM_HOSTS}\"\n",
    "NNODES=${#hosts_array[@]}\n",
    "NODE_RANK=0\n",
    "\n",
    "for i in \"${!hosts_array[@]}\"; do\n",
    "    if [[ \"${hosts_array[$i]}\" == *${CURRENT_HOST}* ]]; then\n",
    "        echo \"host index：$i\"\n",
    "        NODE_RANK=\"$i\" \n",
    "    fi\n",
    "done\n",
    "   \n",
    "    \n",
    "MASTER_PORT=\"13579\"\n",
    "export NCCL_SOCKET_IFNAME=\"eth0\"\n",
    "\n",
    "#Configure the distributed arguments for torch.distributed.launch.\n",
    "GPUS_PER_NODE=\"$SM_NUM_GPUS\"\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES \\\n",
    "                  --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/wizardcoder/WizardLM/WizardLM-15B/* /tmp/wizardcoder_pretrain/\n",
    "\n",
    "\n",
    "DEEPSPEED_OPTS=\"\"\"\n",
    "    WizardLM/WizardCoder/src/train_wizardcoder.py \n",
    "    --deepspeed ds.json \n",
    "    --model_name_or_path \"/tmp/wizardcoder_pretrain/\" \n",
    "    --data_path WizardLM/WizardCoder/data/alpaca_data.json \n",
    "    --output_dir \"/tmp/wizardcoder_out\" \n",
    "    --num_train_epochs 1 \n",
    "    --per_device_train_batch_size 1 \n",
    "    --per_device_eval_batch_size  1 \n",
    "    --gradient_accumulation_steps 4 \n",
    "    --evaluation_strategy \"no\" \n",
    "    --save_strategy \"no\" \n",
    "    --save_steps 2000 \n",
    "    --save_total_limit 1 \n",
    "    --learning_rate 2e-5 \n",
    "    --weight_decay 0. \n",
    "    --warmup_ratio 0.03 \n",
    "    --lr_scheduler_type \"cosine\" \n",
    "    --logging_steps 1 \n",
    "    --cache_dir '/tmp' \n",
    "    --model_max_length 512 \n",
    "    --gradient_checkpointing True \n",
    "    --bf16 True \n",
    "    --tf32 True \n",
    "    --report_to \"none\"\n",
    "\"\"\"    \n",
    "\n",
    "CMD=\"torchrun ${DISTRIBUTED_ARGS} ${DEEPSPEED_OPTS}\"\n",
    "echo ${CMD}\n",
    "${CMD} 2>&1 \n",
    "\n",
    "if [[ \"${CURRENT_HOST}\" == \"${MASTER_ADDR}\" ]]; then  \n",
    "    ./s5cmd sync /tmp/wizardcoder_out s3://$MODEL_S3_BUCKET/llm/models/wizardcoder/output/WizardLM/WizardLM-15B/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c634c0-32a4-4729-b4cd-680af5ec797c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'928808346782.dkr.ecr.us-west-2.amazonaws.com/sagemaker-wizardcoder-15b-finetune:latest'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778cc027-cb2e-412f-abcc-8ba4786f8533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'wizardcoder-15b-finetune'\n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'ml.g5.2xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train-dist.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a112c3-e98e-47c2-bd0c-f08b1126a359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: wizardcoder-15b-finetune-2023-07-31-15-27-59-503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-31 15:28:10 Starting - Starting the training job......\n",
      "2023-07-31 15:28:51 Starting - Preparing the instances for training.....................\n",
      "2023-07-31 15:32:31 Downloading - Downloading input data...\n",
      "2023-07-31 15:32:46 Training - Downloading the training image...............\n",
      "2023-07-31 15:35:27 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:27,757 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:27,815 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:27,825 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:27,826 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:29,609 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:29,677 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:29,745 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:29,755 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"wizardcoder-15b-finetune-2023-07-31-15-27-59-503\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-928808346782/wizardcoder-15b-finetune-2023-07-31-15-27-59-503/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train-dist.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train-dist.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds-train-dist.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds-train-dist.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-928808346782/wizardcoder-15b-finetune-2023-07-31-15-27-59-503/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"wizardcoder-15b-finetune-2023-07-31-15-27-59-503\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-928808346782/wizardcoder-15b-finetune-2023-07-31-15-27-59-503/source/sourcedir.tar.gz\",\"module_name\":\"ds-train-dist.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train-dist.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./ds-train-dist.sh \"\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:36:31,603] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:36:34.401: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:34,405 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-07-31 15:36:34,424 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mhost index：0\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/added_tokens.json /tmp/wizardcoder_pretrain/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/generation_config.json /tmp/wizardcoder_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/config.json /tmp/wizardcoder_pretrain/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/special_tokens_map.json /tmp/wizardcoder_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/tokenizer_config.json /tmp/wizardcoder_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/vocab.json /tmp/wizardcoder_pretrain/vocab.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/tokenizer.json /tmp/wizardcoder_pretrain/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-928808346782/llm/models/wizardcoder/WizardLM/WizardLM-15B/pytorch_model.bin /tmp/wizardcoder_pretrain/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtorchrun --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr algo-1 --master_port 13579 WizardLM/WizardCoder/src/train_wizardcoder.py --deepspeed ds.json --model_name_or_path /tmp/wizardcoder_pretrain/ --data_path WizardLM/WizardCoder/data/alpaca_data.json --output_dir /tmp/wizardcoder_out --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy no --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --cache_dir '/tmp' --model_max_length 512 --gradient_checkpointing True --bf16 True --tf32 True --report_to none\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,135] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,290] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,300] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,306] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,307] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,307] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,308] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:16,308] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,710] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,710] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,873] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,873] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,876] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,877] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,878] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,878] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,878] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,878] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,881] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,881] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,881] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,888] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,888] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,919] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:18,919] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:38:40,007] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 15.82B parameters\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset json/default to /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 9467.95it/s]\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1510.92it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 52002 examples [00:00, 266059.92 examples/s]\u001b[0m\n",
      "\u001b[34mDataset json downloaded and prepared to /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mUsing custom data configuration default-785834b4897ecfca\u001b[0m\n",
      "\u001b[34mFound cached dataset json (/opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #4:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #5:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #6:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #7:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #8:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #9:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #10:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #11:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #12:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #13:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #14:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #15:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #16:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #17:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #18:   0%|          | 0/1 [00:00<?, ?ba/s]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #0: 100%|██████████| 1/1 [00:01<00:00,  1.22s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #0: 100%|██████████| 1/1 [00:01<00:00,  1.22s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #3: 100%|██████████| 1/1 [00:01<00:00,  1.20s/ba]#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #3: 100%|██████████| 1/1 [00:01<00:00,  1.20s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #6: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #6: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #1: 100%|██████████| 1/1 [00:01<00:00,  1.39s/ba]#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #1: 100%|██████████| 1/1 [00:01<00:00,  1.39s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #4: 100%|██████████| 1/1 [00:01<00:00,  1.37s/ba]#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #4: 100%|██████████| 1/1 [00:01<00:00,  1.37s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #8: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #8: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #9: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #9: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #12: 100%|██████████| 1/1 [00:01<00:00,  1.22s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #12: 100%|██████████| 1/1 [00:01<00:00,  1.22s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #2: 100%|██████████| 1/1 [00:01<00:00,  1.59s/ba]#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #2: 100%|██████████| 1/1 [00:01<00:00,  1.59s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #14: 100%|██████████| 1/1 [00:01<00:00,  1.24s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #14: 100%|██████████| 1/1 [00:01<00:00,  1.24s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #15: 100%|██████████| 1/1 [00:01<00:00,  1.24s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #15: 100%|██████████| 1/1 [00:01<00:00,  1.24s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #5: 100%|██████████| 1/1 [00:01<00:00,  1.60s/ba]#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #5: 100%|██████████| 1/1 [00:01<00:00,  1.60s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #20: 100%|██████████| 1/1 [00:01<00:00,  1.25s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #7: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #7: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #11: 100%|██████████| 1/1 [00:01<00:00,  1.58s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #11: 100%|██████████| 1/1 [00:01<00:00,  1.58s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #10: 100%|██████████| 1/1 [00:01<00:00,  1.63s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #10: 100%|██████████| 1/1 [00:01<00:00,  1.63s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #13: 100%|██████████| 1/1 [00:01<00:00,  1.56s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #13: 100%|██████████| 1/1 [00:01<00:00,  1.56s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #17: 100%|██████████| 1/1 [00:01<00:00,  1.40s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #17: 100%|██████████| 1/1 [00:01<00:00,  1.40s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #16: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #16: 100%|██████████| 1/1 [00:01<00:00,  1.69s/ba]\u001b[0m\n",
      "\u001b[34m... (more hidden) ...#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #19: 100%|██████████| 1/1 [00:01<00:00,  1.53s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #18: 100%|██████████| 1/1 [00:01<00:00,  1.64s/ba]#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A#033[A\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #18: 100%|██████████| 1/1 [00:01<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #21: 100%|██████████| 1/1 [00:01<00:00,  1.64s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #22: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #29: 100%|██████████| 1/1 [00:01<00:00,  1.31s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #23: 100%|██████████| 1/1 [00:01<00:00,  1.60s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #28: 100%|██████████| 1/1 [00:01<00:00,  1.40s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #24: 100%|██████████| 1/1 [00:01<00:00,  1.62s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #25: 100%|██████████| 1/1 [00:01<00:00,  1.61s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #26: 100%|██████████| 1/1 [00:01<00:00,  1.61s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #27: 100%|██████████| 1/1 [00:01<00:00,  1.63s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #30: 100%|██████████| 1/1 [00:01<00:00,  1.53s/ba]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on train dataset #31: 100%|██████████| 1/1 [00:01<00:00,  1.51s/ba]\u001b[0m\n",
      "\u001b[34m52002\u001b[0m\n",
      "\u001b[34mSample 45834 of the training set: {'input_ids': [27400, 438, 600, 12404, 688, 18872, 312, 2899, 30, 48076, 623, 600, 1509, 688, 7367, 9686, 1619, 32, 5950, 312, 1789, 688, 36808, 30772, 322, 1326, 32, 203, 203, 1482, 21081, 44, 203, 558, 1591, 2787, 688, 322, 3298, 1957, 646, 1159, 32, 203, 203, 1482, 4237, 44, 203, 1318, 3298, 9792, 544, 322, 8103, 225, 42, 32, 35, 16829, 2664, 461, 1401, 312, 225, 39, 32, 39, 328, 380, 19027, 4809, 32, 203, 203, 1482, 5170, 44, 1318, 3298, 1957, 646, 1159, 312, 13408, 19777, 16802, 32, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1318, 3298, 1957, 646, 1159, 312, 13408, 19777, 16802, 32, 0]}.\u001b[0m\n",
      "\u001b[34mSample 28168 of the training set: {'input_ids': [27400, 438, 600, 12404, 688, 18872, 312, 2899, 32, 5950, 312, 1789, 688, 36808, 30772, 322, 1326, 32, 203, 203, 1482, 21081, 44, 203, 37394, 6916, 41878, 272, 16495, 884, 1654, 328, 22335, 203, 203, 1482, 5170, 44, 73, 4907, 272, 16495, 884, 1654, 328, 22335, 3301, 2953, 884, 29858, 8792, 372, 1968, 461, 883, 526, 37778, 372, 322, 5652, 432, 322, 2074, 32, 32980, 30, 41878, 438, 312, 12101, 461, 16049, 444, 7037, 688, 438, 18214, 8277, 5874, 2784, 1604, 9038, 12953, 2124, 26322, 461, 14680, 342, 32, 624, 4907, 438, 2329, 312, 35008, 444, 2869, 9018, 561, 312, 4644, 11655, 436, 309, 35170, 444, 22335, 32, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73, 4907, 272, 16495, 884, 1654, 328, 22335, 3301, 2953, 884, 29858, 8792, 372, 1968, 461, 883, 526, 37778, 372, 322, 5652, 432, 322, 2074, 32, 32980, 30, 41878, 438, 312, 12101, 461, 16049, 444, 7037, 688, 438, 18214, 8277, 5874, 2784, 1604, 9038, 12953, 2124, 26322, 461, 14680, 342, 32, 624, 4907, 438, 2329, 312, 35008, 444, 2869, 9018, 561, 312, 4644, 11655, 436, 309, 35170, 444, 22335, 32, 0]}.\u001b[0m\n",
      "\u001b[34mSample 38729 of the training set: {'input_ids': [27400, 438, 600, 12404, 688, 18872, 312, 2899, 30, 48076, 623, 600, 1509, 688, 7367, 9686, 1619, 32, 5950, 312, 1789, 688, 36808, 30772, 322, 1326, 32, 203, 203, 1482, 21081, 44, 203, 20763, 322, 10858, 29179, 39522, 436, 312, 3134, 31, 3011, 5978, 1845, 14288, 32, 203, 203, 1482, 4237, 44, 203, 1318, 10858, 39522, 436, 312, 3134, 31, 3011, 5978, 1845, 14288, 328, 2148, 12691, 438, 398, 36, 30, 34, 34, 34, 30, 2218, 322, 10858, 39522, 436, 312, 3134, 31, 3011, 5978, 1845, 14288, 328, 21100, 48633, 438, 398, 37, 30, 39, 34, 34, 32, 203, 203, 1482, 5170, 44, 1318, 10858, 29179, 39522, 436, 312, 3134, 31, 3011, 5978, 1845, 14288, 438, 398, 36, 30, 41, 39, 34, 1886, 36, 30, 34, 34, 34, 474, 398, 37, 30, 39, 34, 34, 4165, 36, 32, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1318, 10858, 29179, 39522, 436, 312, 3134, 31, 3011, 5978, 1845, 14288, 438, 398, 36, 30, 41, 39, 34, 1886, 36, 30, 34, 34, 34, 474, 398, 37, 30, 39, 34, 34, 4165, 36, 32, 0]}.\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e264a28599133223.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-859911bb91d2f84e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8f378cb7e90780b4.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-6188f4507b574f5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2efbfa2469f9e3e1.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-c437cb38d1ac93c5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-0046267baa440b10.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-29301f55b12fb24a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-196d1fe94321c3f7.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-346b396b24acab78.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-b1dd269f864fc1e9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e64c67c322ab7f38.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a04e077784f92976.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-8416be222a612745.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ab717147cd358672.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-9c936474ec6167bb.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-90301def13b54528.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e8d7fadb8a043091.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-186f89db7e2e63ce.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2a0dbb8dd0f10f68.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2d797282ba4552b9.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-958e54859984a5e5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-122e77321558a9f0.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-04c9477839b9630e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-7342471705122dbf.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-2260a7a489358a5e.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-3038d0978a7c9c4c.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-290397500cb94eed.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-40b1278d2a3edb1a.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-cffa56c9cbf9ac62.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ad773902db686c74.arrow\u001b[0m\n",
      "\u001b[34mLoading cached processed dataset at /opt/ml/code/'/tmp'/json/default-785834b4897ecfca/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-12594b2205bd26a5.arrow\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.33751893043518 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.333430767059326 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.33321237564087 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.332343578338623 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.33354353904724 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.331711530685425 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.333083152770996 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.332193851470947 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 2725888 in 322 params\u001b[0m\n",
      "\u001b[34m0%|          | 0/1625 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.546: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.548: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.548: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.559: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.559: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.560: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.564: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.572 algo-1:232 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.574 algo-1:230 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.575 algo-1:228 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.585 algo-1:226 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.585 algo-1:231 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.586 algo-1:229 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.590 algo-1:227 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.601 algo-1:232 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.602 algo-1:230 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.602 algo-1:228 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.612 algo-1:226 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.612: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.30.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.612 algo-1:231 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.613 algo-1:229 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.618 algo-1:227 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.638 algo-1:225 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:39:51.667 algo-1:225 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34malgo-1:232:851 [7] include/alloc.h:99 NCCL WARN Cuda failure 'out of memory'\u001b[0m\n",
      "\u001b[34malgo-1:232:851 [7] include/alloc.h:105 NCCL WARN Failed to CUDA calloc 67108864 bytes\u001b[0m\n",
      "\u001b[34malgo-1:232:851 [7] proxy.cc:1119 NCCL WARN [Proxy Service 7] Failed to execute operation SharedInit from rank 7, retcode 1\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:05,839] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 1/1625 [00:14<6:27:27, 14.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.271, 'learning_rate': 0.0, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1625 [00:14<6:27:27, 14.31s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:16,196] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 2/1625 [00:24<5:24:15, 11.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7021, 'learning_rate': 3.562071871080222e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1625 [00:24<5:24:15, 11.99s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:25,597] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 3/1625 [00:34<4:52:06, 10.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5779, 'learning_rate': 5.6457503405357975e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1625 [00:34<4:52:06, 10.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:34,408] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 4/1625 [00:42<4:30:39, 10.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3489, 'learning_rate': 7.124143742160444e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1625 [00:42<4:30:39, 10.02s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:42,050] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 5/1625 [00:50<4:07:21,  9.16s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2132, 'learning_rate': 8.270874753469162e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1625 [00:50<4:07:21,  9.16s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:49,893] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 6/1625 [00:58<3:55:06,  8.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5462, 'learning_rate': 9.207822211616019e-06, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1625 [00:58<3:55:06,  8.71s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:40:58,041] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 7/1625 [01:06<3:49:58,  8.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5867, 'learning_rate': 1e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 7/1625 [01:06<3:49:58,  8.53s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:05,982] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m0%|          | 8/1625 [01:14<3:44:48,  8.34s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6393, 'learning_rate': 1.0686215613240667e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 8/1625 [01:14<3:44:48,  8.34s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:13,844] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 9/1625 [01:22<3:40:37,  8.19s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8161, 'learning_rate': 1.1291500681071595e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1625 [01:22<3:40:37,  8.19s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:20,737] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 10/1625 [01:29<3:29:42,  7.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1888, 'learning_rate': 1.1832946624549387e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1625 [01:29<3:29:42,  7.79s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:28,104] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 11/1625 [01:36<3:26:04,  7.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2, 'learning_rate': 1.2322744058673439e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 11/1625 [01:36<3:26:04,  7.66s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:34,666] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 12/1625 [01:43<3:16:58,  7.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4791, 'learning_rate': 1.2769894082696241e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 12/1625 [01:43<3:16:58,  7.33s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:40,907] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 13/1625 [01:49<3:08:00,  7.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0556, 'learning_rate': 1.318123223061841e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 13/1625 [01:49<3:08:00,  7.00s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:47,172] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 14/1625 [01:55<3:01:56,  6.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4321, 'learning_rate': 1.3562071871080222e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 14/1625 [01:55<3:01:56,  6.78s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:41:54,036] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 15/1625 [02:02<3:02:32,  6.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5088, 'learning_rate': 1.3916625094004962e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 15/1625 [02:02<3:02:32,  6.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:00,836] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 16/1625 [02:09<3:02:24,  6.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3376, 'learning_rate': 1.4248287484320888e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 16/1625 [02:09<3:02:24,  6.80s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:06,861] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 17/1625 [02:15<2:56:01,  6.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6972, 'learning_rate': 1.4559836410903478e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 17/1625 [02:15<2:56:01,  6.57s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:11,690] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 18/1625 [02:20<2:41:55,  6.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.321, 'learning_rate': 1.4853572552151816e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 18/1625 [02:20<2:41:55,  6.05s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:18,610] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 19/1625 [02:27<2:48:50,  6.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2512, 'learning_rate': 1.5131423106025147e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 19/1625 [02:27<2:48:50,  6.31s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:25,308] [WARNING] [stage3.py:1898:step] 8 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|          | 20/1625 [02:33<2:51:52,  6.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3983, 'learning_rate': 1.5395018495629606e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 20/1625 [02:33<2:51:52,  6.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:30,817] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 21/1625 [02:39<2:44:25,  6.15s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3263, 'learning_rate': 1.56457503405358e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 21/1625 [02:39<2:44:25,  6.15s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:37,603] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 22/1625 [02:46<2:49:24,  6.34s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5068, 'learning_rate': 1.588481592975366e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 22/1625 [02:46<2:49:24,  6.34s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:43,715] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 23/1625 [02:52<2:47:28,  6.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4696, 'learning_rate': 1.6113252800759313e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 23/1625 [02:52<2:47:28,  6.27s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:50,518] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m1%|▏         | 24/1625 [02:58<2:51:37,  6.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4985, 'learning_rate': 1.6331965953776466e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 24/1625 [02:58<2:51:37,  6.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:42:56,577] [WARNING] [stage3.py:1898:step] 6 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 25/1625 [03:05<2:48:31,  6.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4783, 'learning_rate': 1.6541749506938325e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 25/1625 [03:05<2:48:31,  6.32s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:43:00,715] [WARNING] [stage3.py:1898:step] 3 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 26/1625 [03:09<2:30:58,  5.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2322, 'learning_rate': 1.6743304101698635e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 26/1625 [03:09<2:30:58,  5.67s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:43:06,663] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 27/1625 [03:15<2:33:08,  5.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3137, 'learning_rate': 1.693725102160739e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 27/1625 [03:15<2:33:08,  5.75s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:43:14,151] [WARNING] [stage3.py:1898:step] 5 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 28/1625 [03:22<2:46:55,  6.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3279, 'learning_rate': 1.7124143742160445e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 28/1625 [03:22<2:46:55,  6.27s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:43:19,328] [WARNING] [stage3.py:1898:step] 4 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 29/1625 [03:27<2:38:05,  5.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.691, 'learning_rate': 1.730447745298623e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 29/1625 [03:27<2:38:05,  5.94s/it]\u001b[0m\n",
      "\n",
      "2023-07-31 15:43:36 Stopping - Stopping the training job\n",
      "2023-07-31 15:43:36 Uploading - Uploading generated training model\u001b[34m[2023-07-31 15:43:25,155] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 30/1625 [03:33<2:37:03,  5.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4167, 'learning_rate': 1.747869696508518e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 30/1625 [03:33<2:37:03,  5.91s/it]\u001b[0m\n",
      "\u001b[34m[2023-07-31 15:43:32,295] [WARNING] [stage3.py:1898:step] 7 pytorch allocator cache flushes since last step. this happens when there is high memory pressure and is detrimental to performance. if this is happening frequently consider adjusting settings to reduce memory consumption. If you are unable to make the cache flushes go away consider adding get_accelerator().empty_cache() calls in your training loop to ensure that all ranks flush their caches at the same time\u001b[0m\n",
      "\u001b[34m2%|▏         | 31/1625 [03:40<2:46:46,  6.28s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5522, 'learning_rate': 1.7647203321038512e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 31/1625 [03:40<2:46:46,  6.28s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fcb24-f969-44df-b950-5de7a606d1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66975bf2-14ed-4f06-bd34-feab6e8b1551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
