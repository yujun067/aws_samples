{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e940e8-c5ed-4d5f-89a7-62bc05b45c58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 初始化rds并导入ddl（如果已经做过则跳过）\n",
    " * 确保你的aws configure正确设置aksk及region\n",
    " * 确保网络在同一VPC且入站规则互联互通"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f8e582-080a-44ba-ad8d-367210bf0133",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.0.305 (from -r requirements.txt (line 1))\n",
      "  Downloading langchain-0.0.305-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting opensearch-py==2.2.0 (from -r requirements.txt (line 2))\n",
      "  Downloading opensearch_py-2.2.0-py2.py3-none-any.whl (291 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests_aws4auth==1.2.2 (from -r requirements.txt (line 3))\n",
      "  Downloading requests_aws4auth-1.2.2-py2.py3-none-any.whl (24 kB)\n",
      "Collecting openai==0.27.6 (from -r requirements.txt (line 4))\n",
      "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tiktoken==0.3.3 (from -r requirements.txt (line 5))\n",
      "  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.28.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.28.80)\n",
      "Requirement already satisfied: botocore>=1.31.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.31.80)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (2.0.22)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (3.8.6)\n",
      "Collecting anyio<4.0 (from langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading dataclasses_json-0.6.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading langsmith-0.0.63-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (1.22.4)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading pydantic-2.4.2-py3-none-any.whl.metadata (158 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain==0.0.305->-r requirements.txt (line 1)) (8.2.3)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from openai==0.27.6->-r requirements.txt (line 4)) (4.66.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from tiktoken==0.3.3->-r requirements.txt (line 5)) (2023.10.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.28.52->-r requirements.txt (line 6)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.28.52->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.305->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.305->-r requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.305->-r requirements.txt (line 1)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.305->-r requirements.txt (line 1)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.305->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.305->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.305->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.305->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.305->-r requirements.txt (line 1)) (1.1.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.305->-r requirements.txt (line 1)) (2.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.10.1 (from pydantic<3,>=1->langchain==0.0.305->-r requirements.txt (line 1))\n",
      "  Downloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3,>=1->langchain==0.0.305->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.305->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.305->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.305->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.305->-r requirements.txt (line 1)) (3.1.1)\n",
      "Downloading langchain-0.0.305-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.0.63-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.1/384.1 kB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.4.2-py3-none-any.whl (395 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.8/395.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Installing collected packages: typing-inspect, pydantic-core, numexpr, jsonpatch, anyio, annotated-types, tiktoken, requests_aws4auth, pydantic, opensearch-py, marshmallow, openai, langsmith, dataclasses-json, langchain\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.7.3\n",
      "    Uninstalling numexpr-2.7.3:\n",
      "      Successfully uninstalled numexpr-2.7.3\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 anyio-3.7.1 dataclasses-json-0.6.1 jsonpatch-1.33 langchain-0.0.305 langsmith-0.0.63 marshmallow-3.20.1 numexpr-2.8.7 openai-0.27.6 opensearch-py-2.2.0 pydantic-2.4.2 pydantic-core-2.10.1 requests_aws4auth-1.2.2 tiktoken-0.3.3 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b889c-f472-4838-9da4-0ac020a5682d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!chmod 777 ./setup.sh\n",
    "#!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b2040ea-8885-40f9-9498-01fc6bc30858",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::687912291502:role/webui-notebook-stack-ExecutionRole-62U5FV4LJQS\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12708178-4074-42f3-932d-dfb93a2f258c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 初始化langchain环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d422ac-bbbe-4514-8e98-299aa3558b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory,ConversationBufferMemory\n",
    "from langchain import LLMChain\n",
    "from typing import Any, Dict, List, Union,Mapping, Optional, TypeVar, Union\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "aos_endpoint=\"vpc-domain66ac69e0-ckglcu7bnpvw-x4tbe22ur5fbqe44iex4phvvwm.us-west-2.es.amazonaws.com\"\n",
    "embedding_endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\"\n",
    "region='us-west-2'\n",
    "username=\"admin\"\n",
    "passwd=\"(OL>0p;/\"\n",
    "index_name=\"chatbot-example-index\"\n",
    "size=10\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = boto3.Session().region_name\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, 'es', session_token=credentials.token)\n",
    "pwdauth = (username, passwd)\n",
    "\n",
    "\n",
    "## for embedding\n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0,\n",
    "        \"min_length\": 10,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"sentence_embeddings\"]\n",
    "\n",
    "embedding_content_handler=EmbeddingContentHandler()\n",
    "    \n",
    "sm_embeddings = SagemakerEndpointEmbeddings(\n",
    "    # endpoint_name=\"endpoint-name\", \n",
    "    # credentials_profile_name=\"credentials-profile-name\", \n",
    "    #endpoint_name=\"huggingface-textembedding-bloom-7b1-fp1-2023-04-17-03-31-12-148\", \n",
    "    endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\",\n",
    "    region_name=\"us-west-2\", \n",
    "    content_handler=embedding_content_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038be316-1977-4f16-bc81-403752ff35ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\n",
    "    parameters = {\n",
    "    }\n",
    "\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"parameters\": parameters,\n",
    "                \"is_query\" : True,\n",
    "                \"instruction\" :  \"为这个句子生成表示以用于检索相关文章：\"\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    current_date = datetime.utcnow()  # 使用 utcnow() 获取当前 UTC 时间\n",
    "    formatted_date = current_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': aos_endpoint, 'port': 443}],\n",
    "        http_auth = awsauth ,\n",
    "        #http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        intention = doc['intention']\n",
    "        query = doc['query']\n",
    "        reply = doc['reply']\n",
    "        doc_title = doc[\"doc_title\"]\n",
    "        embedding = doc[\"embedding\"]\n",
    "        publish_date = formatted_date\n",
    "        document = { \"intention\": intention, 'query':query, \"reply\": reply,\"doc_title\":doc_title,\"embedding\":embedding}\n",
    "        search.index(index=index, body=document)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f51c0-1089-41c1-b6cd-9dfd8f62a550",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 意图索引创建及ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8141f-3934-4a9f-b5dc-448b4f318e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "auth = (username, passwd)\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region)\n",
    "index_name=\"chatbot-example-index\"\n",
    "schema={\n",
    "    \"settings\" : {\n",
    "        \"index\":{\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 0,\n",
    "            \"knn\": \"true\",\n",
    "            \"knn.algo_param.ef_search\": 32\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"publish_date\" : {\n",
    "                \"type\": \"date\",\n",
    "                \"format\": \"yyyy-MM-dd HH:mm:ss\"\n",
    "            },\n",
    "            \"intention\" : {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ik_max_word\",\n",
    "                \"search_analyzer\": \"ik_smart\"\n",
    "            },\n",
    "            \"reply\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "           \"doc_title\": {\n",
    "                \"type\": \"keyword\"\n",
    "            },\n",
    "            \"embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"nmslib\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 512,\n",
    "                        \"m\": 32\n",
    "                    }\n",
    "                }            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "search = OpenSearch(\n",
    "    hosts = [{'host': aos_endpoint, 'port': 443}],\n",
    "    ##http_auth = awsauth ,\n",
    "    http_auth = auth ,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "search.indices.create(index=index_name, body=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066cfe14-3997-4767-a744-7d5dc49de46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "intentions = \"\"\"闲聊\n",
    "数据分析\n",
    "知识问答\n",
    "\"\"\"\n",
    "intention = intentions.split(\"\\n\")\n",
    "\n",
    "querys = \"\"\"早上好，你好吗?\n",
    "车牌归属城市为'成都'的车辆累计有多少\n",
    "什么是波长\"\"\"\n",
    "query=querys.split(\"\\n\")\n",
    "\n",
    "doc_titles = \"\"\"sample_intentions\n",
    "sample_intentions\n",
    "sample_intentions\n",
    "\"\"\"\n",
    "doc_title=doc_titles.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36b7782c-4be4-4e78-adc8-bd62f175daf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "index_name=\"chatbot-example-index\"\n",
    "embedding_endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "intention_vectors = []\n",
    "intention_vectors=get_vector_by_sm_endpoint(query,sm_client,embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81227dc4-f788-4d22-82f0-fda8fe55bc5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, intention_vectors in enumerate(intention_vectors):\n",
    "    #print(index, sentence_vector)\n",
    "    #doc = {\n",
    "    #    \"metadata_type\":\"table\",\n",
    "    #    \"database_name\":dbs[index],\n",
    "    #    \"table_name\": tables[index],\n",
    "    #    \"query_desc_text\":querys[index],\n",
    "    #    \"query_desc_embedding\": sentence_vector\n",
    "    #      }\n",
    "    doc = {\n",
    "        \"intention\":intention[index],\n",
    "        \"query\":query[index],\n",
    "        \"doc_title\": doc_title[index],\n",
    "        \"reply\":\"\",\n",
    "        \"embedding\": intention_vectors\n",
    "          }\n",
    "    docs.append(doc)\n",
    "\n",
    "#print((doc[\"database_name\"]))\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos(docs,index_name,aos_endpoint,username,passwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f71b77-0bdf-4254-aafd-a551534ceb32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 测试intentsion调用 #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8605c23a-298d-49cb-b3a8-a1a3e4d5a3c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from boto3 import client as boto3_client \n",
    "lambda_client = boto3_client('lambda')\n",
    "question = '归属地为昆明的车牌号有多少？'\n",
    "msg = {\n",
    "    \"fewshot_cnt\":5,\n",
    "    \"query\": question,\n",
    "    \"use_bedrock\" : \"True\", \n",
    "    \"embedding_endpoint\":embedding_endpoint_name, \n",
    "    \"region\":\"us-west-2\",\n",
    "    \"aos_endpoint\":aos_endpoint,\n",
    "    \"index_name\":\"chatbot-example-index\",\n",
    "    \"llm_model_name\":\"anthropic.claude-v2\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#invoke_response = lambda_client.invoke(\n",
    "#    FunctionName=\"Detect_Intention\", \n",
    "#    InvocationType='RequestResponse',\n",
    "#    Payload=json.dumps(msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ab3aa-052b-4cc7-b2c5-5c12242c6b63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from typing import Any, Dict, List, Union,Mapping, Optional, TypeVar, Union\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from botocore.exceptions import ClientError\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "import boto3\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "class APIException(Exception):\n",
    "    def __init__(self, message, code: str = None):\n",
    "        if code:\n",
    "            super().__init__(\"[{}] {}\".format(code, message))\n",
    "        else:\n",
    "            super().__init__(message)\n",
    "\n",
    "\n",
    "def handle_error(func):\n",
    "    \"\"\"Decorator for exception handling\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except APIException as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise RuntimeError(\n",
    "                \"Unknown exception, please check Lambda log for more details\"\n",
    "            )\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: List[str], model_kwargs: Dict) -> bytes:\n",
    "        instruction_zh = \"为这个句子生成表示以用于检索相关文章：\"\n",
    "        instruction_en = \"Represent this sentence for searching relevant passages:\"\n",
    "        input_str = json.dumps({\"inputs\": inputs, \"parameters\":{}, \"is_query\":False, \"instruction\":instruction_en})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"sentence_embeddings\"]\n",
    "\n",
    "class llmContentHandler(LLMContentHandler):\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({'inputs': prompt,'history':[],**model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"outputs\"]\n",
    "\n",
    "def create_intention_prompt_templete():\n",
    "    prompt_template = \"\"\"{instruction}\\n\\n{fewshot}\\n\\nHuman: \\\"{query}\\\"，这个问题的提问意图是啥？可选项[{options}]\\nAssistant: \"\"\"\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "        template=prompt_template, \n",
    "        input_variables=['fewshot','query', 'instruction', 'options']\n",
    "    )\n",
    "    return PROMPT\n",
    "    \n",
    "def get_bedrock_aksk(secret_name='chatbot_bedrock', region_name = \"us-west-2\"):\n",
    "    # Create a Secrets Manager client\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        # For a list of exceptions thrown, see\n",
    "        # https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_GetSecretValue.html\n",
    "        raise e\n",
    "\n",
    "    # Decrypts secret using the associated KMS key.\n",
    "    secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    return secret['BEDROCK_ACCESS_KEY'],secret['BEDROCK_SECRET_KEY']\n",
    "    \n",
    "def lambda_handler(event):\n",
    "    \n",
    "    embedding_endpoint = event['embedding_endpoint']\n",
    "    region = event['region']\n",
    "    aos_endpoint = event['aos_endpoint']\n",
    "    index_name = event['index_name']\n",
    "    query = event['query']\n",
    "    fewshot_cnt = event['fewshot_cnt']\n",
    "    use_bedrock = event['use_bedrock']\n",
    "    #llm_model_endpoint = event['llm_model_endpoint']\n",
    "    llm_model_name = event['llm_model_name']\n",
    "    \n",
    "    logger.info(\"embedding_endpoint: {}\".format(embedding_endpoint))\n",
    "    logger.info(\"region:{}\".format(region))\n",
    "    logger.info(\"aos_endpoint:{}\".format(aos_endpoint))\n",
    "    logger.info(\"index_name:{}\".format(index_name))\n",
    "    logger.info(\"fewshot_cnt:{}\".format(fewshot_cnt))\n",
    "    #logger.info(\"llm_model_endpoint:{}\".format(llm_model_endpoint))\n",
    "\n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    embeddings = SagemakerEndpointEmbeddings(\n",
    "        endpoint_name=embedding_endpoint,\n",
    "        region_name=region,\n",
    "        content_handler=content_handler\n",
    "    )\n",
    "\n",
    "    #auth = AWSV4SignerAuth(credentials, region)\n",
    "    #pwdauth = (\"test\", passwd) \n",
    "    docsearch = OpenSearchVectorSearch(\n",
    "        index_name=index_name,\n",
    "        embedding_function=embeddings,\n",
    "        opensearch_url=\"https://{}\".format(aos_endpoint),\n",
    "        http_auth = auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    \n",
    "    docs = docsearch.similarity_search_with_score(\n",
    "        query=query, \n",
    "        k = fewshot_cnt,\n",
    "        space_type=\"cosinesimil\",\n",
    "        vector_field=\"embedding\",\n",
    "        text_field=\"query\",\n",
    "        metadata_field='*'\n",
    "    )\n",
    "\n",
    "    docs_simple = [ {\"query\" : doc[0].page_content, \"intention\" : doc[0].metadata['intention'], \"score\":doc[1]} for doc in docs]\n",
    "\n",
    "    intention_list = [doc['intention'] for doc in docs_simple ]\n",
    "    intention_counter = Counter(intention_list)\n",
    "    options = set(intention_list)\n",
    "    options_str = \", \".join(options)\n",
    "\n",
    "    instruction = \"参考下列Example，回答下列选择题：\"\n",
    "    examples = [ \"Human: \\\"{}\\\"，这个问题的提问意图是啥？可选项[{}]\\nAssistant: {}\".format(doc['query'], options_str, doc['intention']) for doc in docs_simple ]\n",
    "    fewshot_str = \"{}\\n{}\\n{}\".format(\"<example>\", \"\\n\\n\".join(examples), \"</example>\")\n",
    "    \n",
    "    parameters = {\n",
    "        \"temperature\": 0.01,\n",
    "    }\n",
    "\n",
    "    llm = None\n",
    "    if not use_bedrock:\n",
    "        llmcontent_handler = llmContentHandler()\n",
    "        llm=SagemakerEndpoint(\n",
    "                endpoint_name=llm_model_endpoint, \n",
    "                region_name=region, \n",
    "                model_kwargs={'parameters':parameters},\n",
    "                content_handler=llmcontent_handler\n",
    "            )\n",
    "    else:\n",
    "        boto3_bedrock = boto3.client(\n",
    "            service_name=\"bedrock-runtime\",\n",
    "            region_name=region\n",
    "        )\n",
    "    \n",
    "        parameters = {\n",
    "            \"max_tokens_to_sample\": 20,\n",
    "            \"stop_sequences\": [\"\\n\\n\"],\n",
    "            \"temperature\":0.01,\n",
    "            \"top_p\":1\n",
    "        }\n",
    "        \n",
    "        model_id =\"anthropic.claude-instant-v1\" if llm_model_name == 'claude-instant' else \"anthropic.claude-v2\"\n",
    "        llm = Bedrock(model_id=model_id, client=boto3_bedrock, model_kwargs=parameters)\n",
    "\n",
    "    prompt_template = create_intention_prompt_templete()\n",
    "    prompt = prompt_template.format(fewshot=fewshot_str, instruction=instruction, query=query, options=options_str)\n",
    "    print(\"prompt==\"+prompt)\n",
    "    if len(options) == 1:\n",
    "        logger.info(\"Notice: Only Single latent Intention detected.\")\n",
    "        answer = options.pop()\n",
    "        log_dict = { \"prompt\" : prompt, \"answer\" : answer, \"examples\": docs_simple }\n",
    "        log_dict_str = json.dumps(log_dict, ensure_ascii=False)\n",
    "        logger.info(log_dict_str)\n",
    "        return answer\n",
    "        \n",
    "    llmchain = LLMChain(llm=llm, verbose=False, prompt=prompt_template)\n",
    "    answer = llmchain.run({'fewshot':fewshot_str, \"instruction\":instruction, \"query\":query, \"options\": options_str})\n",
    "    answer = answer.strip()\n",
    "\n",
    "    log_dict = { \"prompt\" : prompt, \"answer\" : answer , \"examples\": docs_simple }\n",
    "    log_dict_str = json.dumps(log_dict, ensure_ascii=False)\n",
    "    logger.info(log_dict_str)\n",
    "\n",
    "    if answer not in options:\n",
    "        answer = intention_counter.most_common(1)[0]\n",
    "        for opt in options:\n",
    "            if opt in answer:\n",
    "                answer = opt\n",
    "                break\n",
    "        \n",
    "    return answer\n",
    "\n",
    "lambda_handler(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25131122-9535-4b09-9147-a132e4442aa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 精准prompt ingestion #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219172dd-a784-40b0-b7d9-6e4622a7acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\n",
    "    parameters = {\n",
    "    }\n",
    "\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"parameters\": parameters,\n",
    "                \"is_query\" : True,\n",
    "                \"instruction\" :  \"为这个句子生成表示以用于检索相关文章：\"\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': aos_endpoint, 'port': 443}],\n",
    "        ##http_auth = awsauth ,\n",
    "        http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        query_desc_embedding = doc['query_desc_embedding']\n",
    "        database_name = doc['database_name']\n",
    "        table_name = doc['table_name']\n",
    "        query_desc_text = doc[\"query_desc_text\"]\n",
    "        document = { \"query_desc_embedding\": query_desc_embedding, 'database_name':database_name, \"table_name\": table_name,\"query_desc_text\":query_desc_text}\n",
    "        search.index(index=index, body=document)\n",
    "        \n",
    "def k_nn_ingestion_by_aos_v2(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': aos_endpoint, 'port': 443}],\n",
    "        ##http_auth = awsauth ,\n",
    "        http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        exactly_query_embedding = doc['exactly_query_embedding']\n",
    "        database_name = doc['database_name']\n",
    "        table_name = doc['table_name']\n",
    "        exactly_query_text = doc[\"exactly_query_text\"]\n",
    "        document = { \"exactly_query_embedding\": exactly_query_embedding, 'database_name':database_name, \"table_name\": table_name,\"exactly_query_text\":exactly_query_text}\n",
    "        search.index(index=index, body=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8defa6bb-c6b8-48aa-8f79-13606de07cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data process\n",
    "all_querys = \"\"\"2023年7月派车单数量超过26次的4.2米车辆一共有多少辆\n",
    "请统计历史累计派车单数量、干线派车单数量、城配派车单数量。历史累计的意思是不限定时间范围\n",
    "奶茶品牌的站点数量和运输货品数量统计\n",
    "2022年的运输总量是多少吨？请注意：traff_weight的单位是千克，请把单位转换为吨\n",
    "车牌为'黑RG6696'的车辆的GPS最近定位上传时间、GPS最近定位省份、GPS最近定位城市、GPS最近定位区县、APP最近定位上传时间、APP最近定位省份、APP最近定位城市、APP最近定位区县。给出sql中字段名不要带上库名\n",
    "车牌归属城市为'成都'的车辆累计有多少？\n",
    "取货地城市名称为'北京市'的历史累计不重复的车牌有多少？请注意车牌号有可能有重复\n",
    "货主-行业列表相似于'西餐连锁'的一共有多少个品牌？多少个客户？\n",
    "品牌名称为'星巴克'的不重复的站点一共有多少个？\n",
    "查看租户简称为云南,车辆的车厢长为9.6米和15米的外廓车长、核定载重\"\"\"\n",
    "querys = all_querys.split(\"\\n\")\n",
    "\n",
    "all_tables = \"\"\"ads_bi_quality_monitor_shipping_detail\n",
    "dws_ots_waybill_info_da\n",
    "dws_station_portrait_index_sum_da\n",
    "dws_ots_waybill_info_da\n",
    "dws_truck_portrait_index_sum_da\n",
    "dws_truck_portrait_index_sum_da\n",
    "dws_ots_waybill_info_da\n",
    "ads_customer_portrait_index_sum_da\n",
    "dim_customer_enterprise_station_base_info\n",
    "dim_pub_truck_tenant,dim_pub_truck_info\"\"\"\n",
    "tables=all_tables.split(\"\\n\")\n",
    "\n",
    "all_dbs = \"\"\"llm\n",
    "llm\n",
    "llm\n",
    "llm\n",
    "llm\n",
    "llm\n",
    "llm\n",
    "llm\n",
    "llm\n",
    "llm\"\"\"\n",
    "dbs=all_dbs.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181db99-50c1-4260-8e4c-603cfe048235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "index_name=\"prompt-optimal-index\"\n",
    "embedding_endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "sentense_vectors = []\n",
    "sentense_vectors=get_vector_by_sm_endpoint(querys,sm_client,embedding_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03547686-3dc6-4924-a6c0-23e2993e1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, sentence_vector in enumerate(sentense_vectors):\n",
    "    #print(index, sentence_vector)\n",
    "    #doc = {\n",
    "    #    \"metadata_type\":\"table\",\n",
    "    #    \"database_name\":dbs[index],\n",
    "    #    \"table_name\": tables[index],\n",
    "    #    \"query_desc_text\":querys[index],\n",
    "    #    \"query_desc_embedding\": sentence_vector\n",
    "    #      }\n",
    "    doc = {\n",
    "        \"metadata_type\":\"table\",\n",
    "        \"database_name\":dbs[index],\n",
    "        \"table_name\": tables[index],\n",
    "        \"exactly_query_text\":querys[index],\n",
    "        \"exactly_query_embedding\": sentence_vector\n",
    "          }\n",
    "    docs.append(doc)\n",
    "\n",
    "#print((doc[\"database_name\"]))\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos_v2(docs,index_name,aos_endpoint,username,passwd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
