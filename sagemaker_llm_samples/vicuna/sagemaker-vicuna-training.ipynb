{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4416c96",
   "metadata": {},
   "source": [
    "# An sample to finetune vicuna on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95febd44",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.183.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.191.0.tar.gz (896 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m896.6/896.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.28.41)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.24.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.23.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.18.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.9.1)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.41 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.31.41)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.26.131->sagemaker) (0.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.16.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: dill>=0.3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.41->boto3<2.0,>=1.26.131->sagemaker) (1.26.14)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.191.0-py2.py3-none-any.whl size=1197726 sha256=a7ae9fe2dc93e5a4d87ce3807edc1f760381154bec76326357be40fad1f1e2f0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bf/bf/fd/8a8025e445e9eab163f030b5a1bbd5af40390ef9bde97b3251\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.183.0\n",
      "    Uninstalling sagemaker-2.183.0:\n",
      "      Successfully uninstalled sagemaker-2.183.0\n",
      "Successfully installed sagemaker-2.191.0\n"
     ]
    }
   ],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "158e94ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82a0c829",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'FastChat'...\n",
      "remote: Enumerating objects: 3587, done.\u001b[K\n",
      "remote: Counting objects: 100% (1627/1627), done.\u001b[K\n",
      "remote: Compressing objects: 100% (461/461), done.\u001b[K\n",
      "remote: Total 3587 (delta 1431), reused 1222 (delta 1164), pack-reused 1960\u001b[K\n",
      "Receiving objects: 100% (3587/3587), 30.06 MiB | 38.24 MiB/s, done.\n",
      "Resolving deltas: 100% (2519/2519), done.\n"
     ]
    }
   ],
   "source": [
    "## download training script from github\n",
    "!rm -rf ./FastChat\n",
    "!git clone https://github.com/lm-sys/FastChat.git\n",
    "!cp ./s5cmd ./FastChat/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5a450",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2513fac",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f942fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m121102723\u001b[0m (\u001b[33mjeff-llama-finetune\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd25c4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "#model_name = \"pinkmanlove/llama-7b-hf\"#decapoda-research/llama-13b-hf\n",
    "model_name = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    #allow_patterns=allow_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a2647",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918df42f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model download may failed, please check prior step!\n"
     ]
    }
   ],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38de4454",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR \"sync None s3://sagemaker-us-west-2-687912291502/llama2/pretrain/TheBloke/Llama-2-13B-chat-GGML/\": given object not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\n",
      "  sync - sync objects\n",
      "\n",
      "Usage:\n",
      "  sync [options] source destination\n",
      "\n",
      "Options:\n",
      "  --delete                       delete objects in destination but not in source (default: false)\n",
      "  --size-only                    make size of object only criteria to decide whether an object should be synced (default: false)\n",
      "  --no-follow-symlinks           do not follow symbolic links (default: false)\n",
      "  --storage-class value          set storage class for target ('STANDARD','REDUCED_REDUNDANCY','GLACIER','STANDARD_IA','ONEZONE_IA','INTELLIGENT_TIERING','DEEP_ARCHIVE')\n",
      "  --concurrency value, -c value  number of concurrent parts transferred between host and remote server (default: 5)\n",
      "  --part-size value, -p value    size of each part transferred between host and remote server, in MiB (default: 50)\n",
      "  --sse value                    perform server side encryption of the data at its destination, e.g. aws:kms\n",
      "  --sse-kms-key-id value         customer master key (CMK) id for SSE-KMS encryption; leave it out if server-side generated key is desired\n",
      "  --acl value                    set acl for target: defines granted accesses and their types on different accounts/groups, e.g. cp --acl 'public-read'\n",
      "  --cache-control value          set cache control for target: defines cache control header for object, e.g. cp --cache-control 'public, max-age=345600'\n",
      "  --expires value                set expires for target (uses RFC3339 format): defines expires header for object, e.g. cp  --expires '2024-10-01T20:30:00Z'\n",
      "  --force-glacier-transfer       force transfer of glacier objects whether they are restored or not (default: false)\n",
      "  --ignore-glacier-warnings      turns off glacier warnings: ignore errors encountered during copying, downloading and moving glacier objects (default: false)\n",
      "  --source-region value          set the region of source bucket; the region of the source bucket will be automatically discovered if --source-region is not specified\n",
      "  --destination-region value     set the region of destination bucket: the region of the destination bucket will be automatically discovered if --destination-region is not specified\n",
      "  --exclude value                exclude objects with given pattern\n",
      "  --raw                          disable the wildcard operations, useful with filenames that contains glob characters (default: false)\n",
      "  --help, -h                     show help (default: false)\n",
      "  \n",
      "Examples:\n",
      "  01. Sync local folder to s3 bucket\n",
      "     > s5cmd sync folder/ s3://bucket/\n",
      "\n",
      "  02. Sync S3 bucket to local folder\n",
      "     > s5cmd sync s3://bucket/* folder/\n",
      "\n",
      "  03. Sync S3 bucket objects under prefix to S3 bucket.\n",
      "     > s5cmd sync s3://sourcebucket/prefix/* s3://destbucket/\n",
      "\n",
      "  04. Sync local folder to S3 but delete the files that S3 bucket has but local does not have.\n",
      "     > s5cmd sync --delete folder/ s3://bucket/\n",
      "\n",
      "  05. Sync S3 bucket to local folder but use size as only comparison criteria.\n",
      "     > s5cmd sync --size-only s3://bucket/* folder/\n",
      "\n",
      "  06. Sync a file to S3 bucket\n",
      "     > s5cmd sync myfile.gz s3://bucket/\n",
      "\n",
      "  07. Sync matching S3 objects to another bucket\n",
      "     > s5cmd sync s3://bucket/*.gz s3://target-bucket/prefix/\n",
      "\n",
      "  08. Perform KMS Server Side Encryption of the object(s) at the destination\n",
      "     > s5cmd sync --sse aws:kms s3://bucket/object s3://target-bucket/prefix/object\n",
      "\n",
      "  09. Perform KMS-SSE of the object(s) at the destination using customer managed Customer Master Key (CMK) key id\n",
      "     > s5cmd sync --sse aws:kms --sse-kms-key-id <your-kms-key-id> s3://bucket/object s3://target-bucket/prefix/object\n",
      "\n",
      "  10. Sync all files to S3 bucket but exclude the ones with txt and gz extension\n",
      "     > s5cmd sync --exclude \"*.txt\" --exclude \"*.gz\" dir/ s3://bucket\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\nchmod +x ./s5cmd\\n#./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama/pretrain/pinkmanlove/llama-7b-hf/ \\n./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama2/pretrain/TheBloke/Llama-2-13B-chat-GGML/ \\n\\n#rm -rf model\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscript\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menv sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mchmod +x ./s5cmd\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m#./s5cmd sync $\u001b[39;49m\u001b[38;5;132;43;01m{local_model_path}\u001b[39;49;00m\u001b[38;5;124;43m s3://$\u001b[39;49m\u001b[38;5;132;43;01m{sagemaker_default_bucket}\u001b[39;49;00m\u001b[38;5;124;43m/llama/pretrain/pinkmanlove/llama-7b-hf/ \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m./s5cmd sync $\u001b[39;49m\u001b[38;5;132;43;01m{local_model_path}\u001b[39;49;00m\u001b[38;5;124;43m s3://$\u001b[39;49m\u001b[38;5;132;43;01m{sagemaker_default_bucket}\u001b[39;49;00m\u001b[38;5;124;43m/llama2/pretrain/TheBloke/Llama-2-13B-chat-GGML/ \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m#rm -rf model\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2477\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2478\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nchmod +x ./s5cmd\\n#./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama/pretrain/pinkmanlove/llama-7b-hf/ \\n./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama2/pretrain/TheBloke/Llama-2-13B-chat-GGML/ \\n\\n#rm -rf model\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "#./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama/pretrain/pinkmanlove/llama-7b-hf/ \n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llama2/pretrain/TheBloke/Llama-2-13B-chat-GGML/ \n",
    "\n",
    "#rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2057f24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "#From pytorch/pytorch:1.5-cuda10.1-cudnn7-runtime\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "# RUN python3 -m pip install git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\n",
    "\n",
    "RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
    "RUN python3 -m pip install transformers==4.28.0\n",
    "RUN pip3 install wandb\n",
    "\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8ee553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b0fb6",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53800617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-vicuna-demo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d98c7c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  22.62GB\n",
      "Step 1/8 : From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04\n",
      " ---> c5a6ef695006\n",
      "Step 2/8 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> e432c8f16beb\n",
      "Step 3/8 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> c41f001f1409\n",
      "Step 4/8 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 3d719afe4e47\n",
      "Step 5/8 : RUN pip3 uninstall -y deepspeed && pip3 install deepspeed\n",
      " ---> Using cache\n",
      " ---> b8cb74241a30\n",
      "Step 6/8 : RUN python3 -m pip install transformers==4.28.0\n",
      " ---> Using cache\n",
      " ---> 29bfb16a3318\n",
      "Step 7/8 : RUN pip3 install wandb\n",
      " ---> Using cache\n",
      " ---> b0ba97a7c9ae\n",
      "Step 8/8 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Using cache\n",
      " ---> 24035db66d27\n",
      "Successfully built 24035db66d27\n",
      "Successfully tagged sagemaker-vicuna-demo:latest\n",
      "The push refers to repository [687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-vicuna-demo]\n",
      "e03ba71c8eb0: Preparing\n",
      "f0b1730152aa: Preparing\n",
      "77a9f77d6ed6: Preparing\n",
      "f8dae5c3df1e: Preparing\n",
      "e3221f18601a: Preparing\n",
      "b6f286626882: Preparing\n",
      "76fe97d80cdb: Preparing\n",
      "f5f76489fff8: Preparing\n",
      "621c3f07daa7: Preparing\n",
      "9b484bb42e11: Preparing\n",
      "54c7c0b58471: Preparing\n",
      "c34adc3ab668: Preparing\n",
      "bbf651e48b84: Preparing\n",
      "f61045791108: Preparing\n",
      "4e2ac0cda74a: Preparing\n",
      "658a33d555eb: Preparing\n",
      "bd16d9a61a98: Preparing\n",
      "f0c0cd2accfa: Preparing\n",
      "1275469c066c: Preparing\n",
      "b802dd3babf4: Preparing\n",
      "a3834ec63558: Preparing\n",
      "63edcef6dedf: Preparing\n",
      "0154e84cc2dd: Preparing\n",
      "7085d1c151f6: Preparing\n",
      "a77a2104cfb6: Preparing\n",
      "6808e7f9da2f: Preparing\n",
      "3bc059a9dec6: Preparing\n",
      "76fe97d80cdb: Waiting\n",
      "de783f3fec23: Preparing\n",
      "f5f76489fff8: Waiting\n",
      "18ca52d74b2f: Preparing\n",
      "73df6ccd636c: Preparing\n",
      "6738b73ff7a8: Preparing\n",
      "2a8292d9bfcc: Preparing\n",
      "5b75a5ef32a7: Preparing\n",
      "25a5f55a11f0: Preparing\n",
      "707f484816ae: Preparing\n",
      "0430aa1e47d4: Preparing\n",
      "65448e793131: Preparing\n",
      "15af6e2d42ba: Preparing\n",
      "b46caef92993: Preparing\n",
      "53ce33a12646: Preparing\n",
      "aad68760f4ce: Preparing\n",
      "323d67ab1719: Preparing\n",
      "621c3f07daa7: Waiting\n",
      "e72743a0fdfe: Preparing\n",
      "3996353f5820: Preparing\n",
      "ea87e0b9c30f: Preparing\n",
      "af18356cdf10: Preparing\n",
      "f6e30dd4497e: Preparing\n",
      "a3834ec63558: Waiting\n",
      "99832d04a153: Preparing\n",
      "63edcef6dedf: Waiting\n",
      "0154e84cc2dd: Waiting\n",
      "a5981ed7a378: Preparing\n",
      "b802dd3babf4: Waiting\n",
      "250519a2f830: Preparing\n",
      "7085d1c151f6: Waiting\n",
      "9b484bb42e11: Waiting\n",
      "6cadbde53f94: Preparing\n",
      "0002c93bdb37: Preparing\n",
      "54c7c0b58471: Waiting\n",
      "c34adc3ab668: Waiting\n",
      "f0c0cd2accfa: Waiting\n",
      "a77a2104cfb6: Waiting\n",
      "658a33d555eb: Waiting\n",
      "bbf651e48b84: Waiting\n",
      "f61045791108: Waiting\n",
      "2a8292d9bfcc: Waiting\n",
      "4e2ac0cda74a: Waiting\n",
      "1275469c066c: Waiting\n",
      "65448e793131: Waiting\n",
      "b6f286626882: Waiting\n",
      "6808e7f9da2f: Waiting\n",
      "25a5f55a11f0: Waiting\n",
      "af18356cdf10: Waiting\n",
      "e72743a0fdfe: Waiting\n",
      "6cadbde53f94: Waiting\n",
      "0430aa1e47d4: Waiting\n",
      "707f484816ae: Waiting\n",
      "3bc059a9dec6: Waiting\n",
      "f6e30dd4497e: Waiting\n",
      "3996353f5820: Waiting\n",
      "de783f3fec23: Waiting\n",
      "18ca52d74b2f: Waiting\n",
      "0002c93bdb37: Waiting\n",
      "250519a2f830: Waiting\n",
      "a5981ed7a378: Waiting\n",
      "99832d04a153: Waiting\n",
      "ea87e0b9c30f: Waiting\n",
      "aad68760f4ce: Waiting\n",
      "323d67ab1719: Waiting\n",
      "73df6ccd636c: Waiting\n",
      "53ce33a12646: Waiting\n",
      "5b75a5ef32a7: Waiting\n",
      "6738b73ff7a8: Waiting\n",
      "77a9f77d6ed6: Layer already exists\n",
      "f8dae5c3df1e: Layer already exists\n",
      "f0b1730152aa: Layer already exists\n",
      "e03ba71c8eb0: Layer already exists\n",
      "e3221f18601a: Layer already exists\n",
      "b6f286626882: Layer already exists\n",
      "f5f76489fff8: Layer already exists\n",
      "621c3f07daa7: Layer already exists\n",
      "76fe97d80cdb: Layer already exists\n",
      "9b484bb42e11: Layer already exists\n",
      "54c7c0b58471: Layer already exists\n",
      "c34adc3ab668: Layer already exists\n",
      "f61045791108: Layer already exists\n",
      "bbf651e48b84: Layer already exists\n",
      "4e2ac0cda74a: Layer already exists\n",
      "658a33d555eb: Layer already exists\n",
      "1275469c066c: Layer already exists\n",
      "bd16d9a61a98: Layer already exists\n",
      "f0c0cd2accfa: Layer already exists\n",
      "b802dd3babf4: Layer already exists\n",
      "a3834ec63558: Layer already exists\n",
      "63edcef6dedf: Layer already exists\n",
      "0154e84cc2dd: Layer already exists\n",
      "7085d1c151f6: Layer already exists\n",
      "a77a2104cfb6: Layer already exists\n",
      "6808e7f9da2f: Layer already exists\n",
      "3bc059a9dec6: Layer already exists\n",
      "de783f3fec23: Layer already exists\n",
      "18ca52d74b2f: Layer already exists\n",
      "73df6ccd636c: Layer already exists\n",
      "6738b73ff7a8: Layer already exists\n",
      "2a8292d9bfcc: Layer already exists\n",
      "5b75a5ef32a7: Layer already exists\n",
      "25a5f55a11f0: Layer already exists\n",
      "707f484816ae: Layer already exists\n",
      "0430aa1e47d4: Layer already exists\n",
      "15af6e2d42ba: Layer already exists\n",
      "65448e793131: Layer already exists\n",
      "53ce33a12646: Layer already exists\n",
      "b46caef92993: Layer already exists\n",
      "3996353f5820: Layer already exists\n",
      "ea87e0b9c30f: Layer already exists\n",
      "aad68760f4ce: Layer already exists\n",
      "323d67ab1719: Layer already exists\n",
      "e72743a0fdfe: Layer already exists\n",
      "af18356cdf10: Layer already exists\n",
      "f6e30dd4497e: Layer already exists\n",
      "99832d04a153: Layer already exists\n",
      "250519a2f830: Layer already exists\n",
      "a5981ed7a378: Layer already exists\n",
      "6cadbde53f94: Layer already exists\n",
      "0002c93bdb37: Layer already exists\n",
      "latest: digest: sha256:aa201f7d086f97417e3a5f8e39d6266dfebf34b26d483ce222d54c5999d81dc6 size: 11255\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd6dc0d",
   "metadata": {},
   "source": [
    "### Generate the deepspeed config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b56758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ds.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds.json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"auto_cast\": false,\n",
    "    \"loss_scale\": 0,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": \"auto\",\n",
    "      \"betas\": \"auto\",\n",
    "      \"eps\": \"auto\",\n",
    "      \"weight_decay\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": \"auto\",\n",
    "      \"warmup_max_lr\": \"auto\",\n",
    "      \"warmup_num_steps\": \"auto\"\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \"overlap_comm\": true,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"sub_group_size\": 1e9,\n",
    "    \"reduce_bucket_size\": \"auto\",\n",
    "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "    \"stage3_param_persistence_threshold\": \"auto\",\n",
    "    \"stage3_max_live_parameters\": 1e9,\n",
    "    \"stage3_max_reuse_distance\": 1e9,\n",
    "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "  },\n",
    "  \"gradient_accumulation_steps\": \"auto\",\n",
    "  \"gradient_clipping\": \"auto\",\n",
    "  \"steps_per_print\": 2000,\n",
    "  \"train_batch_size\": \"auto\",\n",
    "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "  \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5fc567",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683e3d70",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/7B/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c51767",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Notice\n",
    "\n",
    "We modified some parts of ```FastChat/fastchat/train/train.py```, such as how to save model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f94daee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv FastChat/fastchat/train/train.py FastChat/fastchat/train/train_bak.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "286ccd3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting FastChat/fastchat/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile FastChat/fastchat/train/train.py\n",
    "# Adopted from tatsu-lab@stanford_alpaca. Below is the original copyright:\n",
    "#    Copyright 2023 Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li\n",
    "#\n",
    "#    Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#    you may not use this file except in compliance with the License.\n",
    "#    You may obtain a copy of the License at\n",
    "#\n",
    "#        http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#    Unless required by applicable law or agreed to in writing, software\n",
    "#    distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#    See the License for the specific language governing permissions and\n",
    "#    limitations under the License.\n",
    "\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import Trainer\n",
    "####\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "from transformers.models.llama.tokenization_llama import LlamaTokenizer\n",
    "####\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from fastchat.conversation import get_conv_template, SeparatorStyle\n",
    "from transformers import TrainerCallback\n",
    "import wandb\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "\n",
    "\n",
    "node_rank = os.environ['CURRENT_HOST']\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "tags = os.getenv(\"TAG\")\n",
    "wandb.login(key=api_key)\n",
    "wandb.init(project=\"monitor_demo\",tags=[tags])\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "local_rank = None\n",
    "\n",
    "\n",
    "def rank0_print(*args):\n",
    "    if local_rank == 0:\n",
    "        print(*args)\n",
    "\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu()\n",
    "            for key, value in state_dict.items()\n",
    "        }\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    conv = get_conv_template(\"vicuna_v1.1\").copy()\n",
    "    roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "    # Apply prompt templates\n",
    "    conversations = []\n",
    "    for i, source in enumerate(sources):\n",
    "        if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "            # Skip the first one if it is not from human\n",
    "            source = source[1:]\n",
    "\n",
    "        conv.messages = []\n",
    "        for j, sentence in enumerate(source):\n",
    "            role = roles[sentence[\"from\"]]\n",
    "            assert role == conv.roles[j % 2], f\"{i}\"\n",
    "            conv.append_message(role, sentence[\"value\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "\n",
    "    # Tokenize conversations\n",
    "    input_ids = tokenizer(\n",
    "        conversations,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    ).input_ids\n",
    "    targets = input_ids.clone()\n",
    "\n",
    "    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO\n",
    "\n",
    "    # Mask targets\n",
    "    sep = conv.sep + conv.roles[1] + \": \"\n",
    "    for conversation, target in zip(conversations, targets):\n",
    "        total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "        rounds = conversation.split(conv.sep2)\n",
    "        cur_len = 1\n",
    "        for i, rou in enumerate(rounds):\n",
    "            if rou == \"\":\n",
    "                break\n",
    "\n",
    "            parts = rou.split(sep)\n",
    "            if len(parts) != 2:\n",
    "                break\n",
    "            parts[0] += sep\n",
    "            round_len = len(tokenizer(rou).input_ids)\n",
    "            instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "            target[cur_len:cur_len+instruction_len] = (\n",
    "                IGNORE_TOKEN_ID)\n",
    "\n",
    "            #rank0_print(tokenizer.decode(target[cur_len+instruction_len:cur_len+round_len]))\n",
    "\n",
    "            cur_len += round_len\n",
    "        target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "        if cur_len < tokenizer.model_max_length:\n",
    "            if cur_len != total_len:\n",
    "                rank0_print(f\"WARNING: tokenization mismatch \"\n",
    "                            f\"{cur_len} vs. {total_len}\")\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets,\n",
    "                attention_mask=input_ids.ne(tokenizer.pad_token_id))\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        rank0_print(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        rank0_print(\"Formatting inputs...\")\n",
    "        sources = [example[\"conversations\"] for example in list_data_dict]\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        self.attention_mask = data_dict[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i],\n",
    "                    labels=self.labels[i],\n",
    "                    attention_mask=self.attention_mask[i])\n",
    "\n",
    "\n",
    "class LazySupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(LazySupervisedDataset, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        rank0_print(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.list_data_dict = list_data_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_data_dict)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.list_data_dict[i]\n",
    "        if isinstance(i, int):\n",
    "            sources = [sources]\n",
    "        data_dict = preprocess([e[\"conversations\"] for e in sources],\n",
    "            self.tokenizer)\n",
    "        if isinstance(i, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0],\n",
    "                             labels=data_dict[\"labels\"][0],\n",
    "                             attention_mask=data_dict[\"attention_mask\"][0])\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_args) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = (LazySupervisedDataset\n",
    "                   if data_args.lazy_preprocess else SupervisedDataset)\n",
    "    train_dataset = dataset_cls(tokenizer=tokenizer,\n",
    "                                data_path=data_args.data_path)\n",
    "    return dict(train_dataset=train_dataset,\n",
    "                eval_dataset=None)\n",
    "\n",
    "\n",
    "class MyCustomCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        # 在每个训练轮次开始时执行的操作\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        distribute_dict={\"epoch\":state.epoch,\"total_flos\":state.total_flos}\n",
    "        wandb.log(data=distribute_dict,step=state.global_step)\n",
    "            \n",
    "\n",
    "    def on_batch_end(self, args, state, control, **kwargs):\n",
    "        distribute_dict={\"epoch\":state.epoch,\"total_flos\":state.total_flos}\n",
    "        for key, value in kwargs.items():\n",
    "            if \"scheduler\" in key:\n",
    "                print(\"lr_scheduler info is \"+key+\":\"+str(value))\n",
    "\n",
    "    \n",
    "    def on_batch_begin(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    global local_rank\n",
    "\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    local_rank = training_args.local_rank\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained( #transformers.AutoTokenizer\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "#####\n",
    "#     tokenizer.pad_token = tokenizer.unk_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"-----------no pad token and add special token PAD----\")\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "######\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                              data_args=data_args)\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      args=training_args,\n",
    "                      **data_module)\n",
    "    \n",
    "    custom_callback = MyCustomCallback()\n",
    "    trainer = Trainer(model=model,\n",
    "                      tokenizer=tokenizer,\n",
    "                      args=training_args,\n",
    "                      **data_module,\n",
    "                      callbacks=[custom_callback]\n",
    "                     )\n",
    "\n",
    "    if list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\")):\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train()\n",
    "#     trainer.save_state()\n",
    "#     safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "#                                    output_dir=training_args.output_dir)\n",
    "\n",
    "\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef55e2",
   "metadata": {},
   "source": [
    "Here we use sample dataset - sharegpt_test.json for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a3d106-42f5-4241-a8ae-000939f11c5e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 单机多卡 deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55cedcb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./FastChat/ds-train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./FastChat/ds-train.sh\n",
    "#!/bin/bash\n",
    "export WANDB_API_KEY=\"298b59ce8a416fd45b5fa9ffc17fe72327854e0c\"\n",
    "export WANDB_WATCH=\"all\"\n",
    "export WANDB_PROJECT=\"llama-finetune\" \n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "\n",
    "#cd FastChat && pip install -e . && cd ..\n",
    "pip install -e .\n",
    "\n",
    "deepspeed --num_gpus=8 ./fastchat/train/train_mem.py \\\n",
    "    --deepspeed ds.json \\\n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \\\n",
    "    --data_path data/dummy_conversation.json \\\n",
    "    --output_dir \"/tmp/llama_out\" \\\n",
    "    --num_train_epochs 10 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size  8 \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"no\" \\\n",
    "    --save_steps 100 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 1e-6 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --cache_dir '/tmp' \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True \\\n",
    "    --fp16 True \\\n",
    "    --tf32 False\n",
    "    --report_to \"wandb\"\n",
    "\n",
    "if [ $? -eq 1 ]; then\n",
    "    echo \"Training script error, please check CloudWatch logs\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517889b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-vicuna-demo:latest'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad795754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## set train_data_path to your training dataset path in s3\n",
    "train_data_path = f's3://{sagemaker_default_bucket}/llama/train_data/'\n",
    "inputs = {'train': train_data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab36100",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: vicuna-demo-2023-10-17-09-20-23-781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-17 09:20:28 Starting - Starting the training job\n",
      "2023-10-17 09:20:28 Pending - Training job waiting for capacity......\n",
      "2023-10-17 09:21:08 Pending - Preparing the instances for training........................\n",
      "2023-10-17 09:25:17 Downloading - Downloading input data...\n",
      "2023-10-17 09:25:42 Training - Downloading the training image.................................\n",
      "2023-10-17 09:31:29 Training - Training image download completed. Training in progress........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:24,663 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:24,721 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:24,730 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:24,732 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:26,142 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:26,210 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:26,277 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:26,288 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"vicuna-demo-2023-10-17-09-20-23-781\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-20-23-781/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds-train.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds-train.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-20-23-781/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"vicuna-demo-2023-10-17-09-20-23-781\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-20-23-781/source/sourcedir.tar.gz\",\"module_name\":\"ds-train.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./ds-train.sh \"\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:32:27,629] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:32:31.260: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:31,265 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-17 09:32:31,284 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00002-of-00002.bin /tmp/llama_pretrain/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00001-of-00002.bin /tmp/llama_pretrain/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.15.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (3.0.36)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 36.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting shortuuid\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers<4.29.0,>=4.28.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (4.28.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<=2.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.10.4)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.103.2-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.3/66.3 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (12.6.0)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.48.0-py3-none-any.whl (20.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/20.3 MB 73.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.16.0)\u001b[0m\n",
      "\u001b[34mCollecting markdown2[all]\u001b[0m\n",
      "\u001b[34mDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.13.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting peft\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.7/75.7 kB 25.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nh3\u001b[0m\n",
      "\u001b[34mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 98.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.19) (0.2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<=2.0->fschat==0.2.19) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (0.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate->fschat==0.2.19) (5.9.4)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 20.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting anyio<4.0.0,>=3.7.1\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 24.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting websockets<12.0,>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 33.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.1.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==0.6.1\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 55.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources<7.0,>=1.3\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.6/138.6 kB 37.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 58.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting altair<6.0,>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.1.2-py3-none-any.whl (516 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 516.2/516.2 kB 75.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (9.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (2.1.2)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from gradio-client==0.6.1->gradio->fschat==0.2.19) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn->fschat==0.2.19) (8.1.2)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 21.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.19.0,>=0.18.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 26.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wavedrom\u001b[0m\n",
      "\u001b[34mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 40.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting safetensors\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 101.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.1.37)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat==0.2.19) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.2.19) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->fschat==0.2.19) (4.0.10)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 46.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio->fschat==0.2.19) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->fschat==0.2.19) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting svgwrite\u001b[0m\n",
      "\u001b[34mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->fschat==0.2.19) (5.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.19.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (22.2.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fschat, ffmpy, wavedrom\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fschat: filename=fschat-0.2.19-0.editable-py3-none-any.whl size=12792 sha256=05afc917cd1d087d858b7cc6034eab00ed59240162ef4b66d34c3dea2c8793da\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-wgnte_wa/wheels/40/03/3a/5f39818cea87b3c154b54d046a775b3da4b8ed9b642b8d50e6\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=94ab4fe8e91687a4041cafd6cb282e6a562008aa2f90067e435693ff3e5140cf\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30051 sha256=547c332282a5ffe3aae823c71e6f06f46d1ebb5f83c75f9488a1a873f5085cbf\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/81/08/ec/3e7bb60504c4ebf08e1d5c88e9abb85b0a3549d9f8d031113f\u001b[0m\n",
      "\u001b[34mSuccessfully built fschat ffmpy wavedrom\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, nh3, ffmpy, websockets, typing-extensions, svgwrite, sniffio, shortuuid, semantic-version, safetensors, python-multipart, orjson, markdown2, importlib-resources, h11, fsspec, aiofiles, wavedrom, uvicorn, tiktoken, huggingface-hub, anyio, starlette, httpcore, altair, peft, httpx, fastapi, gradio-client, gradio, fschat\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.4.0\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.4.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.1.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.2.1 altair-5.1.2 anyio-3.7.1 fastapi-0.103.2 ffmpy-0.3.1 fschat-0.2.19 fsspec-2023.9.2 gradio-3.48.0 gradio-client-0.6.1 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 importlib-resources-6.1.0 markdown2-2.4.10 nh3-0.2.14 orjson-3.9.9 peft-0.5.0 pydub-0.25.1 python-multipart-0.0.6 safetensors-0.4.0 semantic-version-2.10.0 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 tiktoken-0.5.1 typing-extensions-4.8.0 uvicorn-0.23.2 wavedrom-2.0.3.post3 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:49,042] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:50,818] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:50,894] [INFO] [runner.py:570:main] cmd = /opt/conda/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ./fastchat/train/train_mem.py --deepspeed ds.json --model_name_or_path /tmp/llama_pretrain/ --data_path data/dummy_conversation.json --output_dir /tmp/llama_out --num_train_epochs 10 --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --gradient_accumulation_steps 4 --evaluation_strategy no --save_strategy no --save_steps 100 --save_total_limit 1 --learning_rate 1e-6 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --cache_dir /tmp --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --fp16 True --tf32 False\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:51,821] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.14.3\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:138:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:138:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:138:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:163:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:53,562] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,182] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,195] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,195] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,195] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,232] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,232] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,234] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:33:55,243] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: - Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: \\ Waiting for wandb.init()...\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093400-vicuna-demo-2023-10-17-09-20-23-781-ok6nct-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-ok6nct-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-ok6nct-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:03,449] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-3mk6ey-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-3mk6ey-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-3mk6ey-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:09,651] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:09,652] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-f5zxol-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-f5zxol-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-f5zxol-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:09,701] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-14ahgx-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-14ahgx-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-14ahgx-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:09,772] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-841rhg-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-841rhg-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-841rhg-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:09,884] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-0rlgzy-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-0rlgzy-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-0rlgzy-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:09,942] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093400-vicuna-demo-2023-10-17-09-20-23-781-bczawn-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-bczawn-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-bczawn-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:10,062] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_093400-vicuna-demo-2023-10-17-09-20-23-781-weeoct-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-20-23-781-weeoct-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-weeoct-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:10,252] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:14,300] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.06s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.70s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.19s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.83s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.84s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.37s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.93s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading data...\u001b[0m\n",
      "\u001b[34mFormatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.45924687385559 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...Loading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.43461561203003 seconds\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.435103178024292 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.43292546272278 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...Loading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.43462038040161 secondsTime to load fused_adam op: 26.43488907814026 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.434237957000732 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.532487869262695 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.900: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.901: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.902: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.903: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.903: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.903: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.915: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.927 algo-1:363 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.928 algo-1:362 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.929 algo-1:358 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.931 algo-1:361 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.931 algo-1:359 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.932 algo-1:364 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.943 algo-1:360 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m0%|          | 0/140 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.949 algo-1:363 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.949 algo-1:362 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.950 algo-1:358 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.953 algo-1:361 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.953 algo-1:359 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.953 algo-1:364 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:58.965 algo-1:360 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:59.029: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:59.070 algo-1:357 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:34:59.096 algo-1:357 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m1%|          | 1/140 [00:09<22:21,  9.65s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7057, 'learning_rate': 0, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m1%|          | 1/140 [00:09<22:21,  9.65s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/140 [00:15<16:37,  7.23s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.744, 'learning_rate': 0, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m1%|▏         | 2/140 [00:15<16:37,  7.23s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/140 [00:20<14:43,  6.45s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.6965, 'learning_rate': 0, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m2%|▏         | 3/140 [00:20<14:43,  6.45s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/140 [00:26<13:46,  6.08s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7522, 'learning_rate': 0, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m3%|▎         | 4/140 [00:26<13:46,  6.08s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m4%|▎         | 5/140 [00:31<13:13,  5.88s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7031, 'learning_rate': 0, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m4%|▎         | 5/140 [00:31<13:13,  5.88s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/140 [00:37<12:51,  5.76s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7057, 'learning_rate': 0, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m4%|▍         | 6/140 [00:37<12:51,  5.76s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m5%|▌         | 7/140 [00:42<12:34,  5.68s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.6654, 'learning_rate': 0, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m5%|▌         | 7/140 [00:42<12:34,  5.68s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m6%|▌         | 8/140 [00:48<12:22,  5.62s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.6456, 'learning_rate': 0, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m6%|▌         | 8/140 [00:48<12:22,  5.62s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m6%|▋         | 9/140 [00:53<12:12,  5.59s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7385, 'learning_rate': 0, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m6%|▋         | 9/140 [00:53<12:12,  5.59s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m7%|▋         | 10/140 [00:59<12:03,  5.56s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.6585, 'learning_rate': 0, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m7%|▋         | 10/140 [00:59<12:03,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34m8%|▊         | 11/140 [01:04<11:58,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.7054, 'learning_rate': 0.0, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m8%|▊         | 11/140 [01:04<11:58,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m9%|▊         | 12/140 [01:10<11:52,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.616, 'learning_rate': 4.30676558073393e-07, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m9%|▊         | 12/140 [01:10<11:52,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m9%|▉         | 13/140 [01:15<11:45,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.6154, 'learning_rate': 6.826061944859853e-07, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m9%|▉         | 13/140 [01:15<11:45,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m10%|█         | 14/140 [01:21<11:40,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.5475, 'learning_rate': 8.61353116146786e-07, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m10%|█         | 14/140 [01:21<11:40,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m11%|█         | 15/140 [01:27<11:35,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 3.503, 'learning_rate': 1e-06, 'epoch': 1.05}\u001b[0m\n",
      "\u001b[34m11%|█         | 15/140 [01:27<11:35,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m11%|█▏        | 16/140 [01:32<11:29,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.9951, 'learning_rate': 1e-06, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 16/140 [01:32<11:29,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m12%|█▏        | 17/140 [01:38<11:23,  5.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.827, 'learning_rate': 1e-06, 'epoch': 1.19}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 17/140 [01:38<11:23,  5.55s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m13%|█▎        | 18/140 [01:43<11:17,  5.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7558, 'learning_rate': 1e-06, 'epoch': 1.26}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 18/140 [01:43<11:17,  5.55s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m14%|█▎        | 19/140 [01:49<11:12,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5092, 'learning_rate': 1e-06, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 19/140 [01:49<11:12,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m14%|█▍        | 20/140 [01:54<11:06,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.2064, 'learning_rate': 1e-06, 'epoch': 1.4}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 20/140 [01:54<11:06,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m15%|█▌        | 21/140 [02:00<11:01,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.1053, 'learning_rate': 1e-06, 'epoch': 1.47}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 21/140 [02:00<11:01,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m16%|█▌        | 22/140 [02:06<10:55,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.582, 'learning_rate': 1e-06, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 22/140 [02:06<10:55,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m16%|█▋        | 23/140 [02:11<10:50,  5.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3386, 'learning_rate': 1e-06, 'epoch': 1.61}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 23/140 [02:11<10:50,  5.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m17%|█▋        | 24/140 [02:17<10:44,  5.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3024, 'learning_rate': 1e-06, 'epoch': 1.68}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 24/140 [02:17<10:44,  5.55s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m18%|█▊        | 25/140 [02:50<26:31, 13.84s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2725, 'learning_rate': 1e-06, 'epoch': 1.75}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 25/140 [02:50<26:31, 13.84s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m19%|█▊        | 26/140 [02:55<21:34, 11.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2249, 'learning_rate': 1e-06, 'epoch': 1.82}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 26/140 [02:55<21:34, 11.36s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m19%|█▉        | 27/140 [03:01<18:06,  9.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1862, 'learning_rate': 1e-06, 'epoch': 1.89}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 27/140 [03:01<18:06,  9.62s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m20%|██        | 28/140 [03:06<15:42,  8.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1805, 'learning_rate': 1e-06, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m20%|██        | 28/140 [03:06<15:42,  8.41s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m21%|██        | 29/140 [03:12<13:59,  7.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1537, 'learning_rate': 1e-06, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m21%|██        | 29/140 [03:12<13:59,  7.56s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m21%|██▏       | 30/140 [03:18<12:46,  6.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1653, 'learning_rate': 1e-06, 'epoch': 2.11}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 30/140 [03:18<12:46,  6.97s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m22%|██▏       | 31/140 [03:23<11:54,  6.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1469, 'learning_rate': 1e-06, 'epoch': 2.18}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 31/140 [03:23<11:54,  6.55s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m23%|██▎       | 32/140 [03:29<11:16,  6.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1829, 'learning_rate': 1e-06, 'epoch': 2.25}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 32/140 [03:29<11:16,  6.27s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m24%|██▎       | 33/140 [03:34<10:48,  6.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1604, 'learning_rate': 1e-06, 'epoch': 2.32}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 33/140 [03:34<10:48,  6.06s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m24%|██▍       | 34/140 [03:40<10:26,  5.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1562, 'learning_rate': 1e-06, 'epoch': 2.39}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 34/140 [03:40<10:26,  5.91s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34m25%|██▌       | 35/140 [03:46<10:09,  5.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1661, 'learning_rate': 1e-06, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 35/140 [03:46<10:09,  5.81s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m26%|██▌       | 36/140 [03:51<09:56,  5.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1635, 'learning_rate': 1e-06, 'epoch': 2.53}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 36/140 [03:51<09:56,  5.74s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m26%|██▋       | 37/140 [03:57<09:45,  5.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1335, 'learning_rate': 1e-06, 'epoch': 2.6}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 37/140 [03:57<09:45,  5.68s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m27%|██▋       | 38/140 [04:02<09:36,  5.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2239, 'learning_rate': 1e-06, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 38/140 [04:02<09:36,  5.65s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m28%|██▊       | 39/140 [04:08<09:28,  5.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1563, 'learning_rate': 1e-06, 'epoch': 2.74}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 39/140 [04:08<09:28,  5.63s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m29%|██▊       | 40/140 [04:13<09:21,  5.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1503, 'learning_rate': 1e-06, 'epoch': 2.81}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 40/140 [04:13<09:21,  5.61s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m29%|██▉       | 41/140 [04:19<09:14,  5.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1335, 'learning_rate': 1e-06, 'epoch': 2.88}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 41/140 [04:19<09:14,  5.60s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m30%|███       | 42/140 [04:25<09:07,  5.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1359, 'learning_rate': 1e-06, 'epoch': 2.95}\u001b[0m\n",
      "\u001b[34m30%|███       | 42/140 [04:25<09:07,  5.59s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34m31%|███       | 43/140 [04:30<09:01,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1401, 'learning_rate': 1e-06, 'epoch': 3.02}\u001b[0m\n",
      "\u001b[34m31%|███       | 43/140 [04:30<09:01,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m31%|███▏      | 44/140 [04:36<08:55,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1516, 'learning_rate': 1e-06, 'epoch': 3.09}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 44/140 [04:36<08:55,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34m32%|███▏      | 45/140 [04:41<08:49,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1327, 'learning_rate': 1e-06, 'epoch': 3.16}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 45/140 [04:41<08:49,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m33%|███▎      | 46/140 [04:47<08:44,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1416, 'learning_rate': 1e-06, 'epoch': 3.23}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 46/140 [04:47<08:44,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m34%|███▎      | 47/140 [04:52<08:38,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1339, 'learning_rate': 1e-06, 'epoch': 3.3}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 47/140 [04:52<08:38,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m34%|███▍      | 48/140 [04:58<08:32,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1394, 'learning_rate': 1e-06, 'epoch': 3.37}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 48/140 [04:58<08:32,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m35%|███▌      | 49/140 [05:04<08:26,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1352, 'learning_rate': 1e-06, 'epoch': 3.44}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 49/140 [05:04<08:26,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m36%|███▌      | 50/140 [05:36<20:16, 13.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1288, 'learning_rate': 1e-06, 'epoch': 3.51}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 50/140 [05:36<20:16, 13.52s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 64 vs. 66\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m36%|███▋      | 51/140 [05:41<16:31, 11.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1252, 'learning_rate': 1e-06, 'epoch': 3.58}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 51/140 [05:41<16:31, 11.14s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m37%|███▋      | 52/140 [05:47<13:53,  9.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1319, 'learning_rate': 1e-06, 'epoch': 3.65}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 52/140 [05:47<13:53,  9.47s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m38%|███▊      | 53/140 [05:52<12:00,  8.28s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1873, 'learning_rate': 1e-06, 'epoch': 3.72}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 53/140 [05:52<12:00,  8.28s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m39%|███▊      | 54/140 [05:58<10:42,  7.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1697, 'learning_rate': 1e-06, 'epoch': 3.79}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 54/140 [05:58<10:42,  7.47s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m39%|███▉      | 55/140 [06:03<09:46,  6.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1668, 'learning_rate': 1e-06, 'epoch': 3.86}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 55/140 [06:03<09:46,  6.90s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m40%|████      | 56/140 [06:09<09:05,  6.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1654, 'learning_rate': 1e-06, 'epoch': 3.93}\u001b[0m\n",
      "\u001b[34m40%|████      | 56/140 [06:09<09:05,  6.50s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m41%|████      | 57/140 [06:15<08:36,  6.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1587, 'learning_rate': 1e-06, 'epoch': 4.0}\u001b[0m\n",
      "\u001b[34m41%|████      | 57/140 [06:15<08:36,  6.22s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m41%|████▏     | 58/140 [06:20<08:13,  6.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.134, 'learning_rate': 1e-06, 'epoch': 4.07}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 58/140 [06:20<08:13,  6.02s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34m42%|████▏     | 59/140 [06:26<07:56,  5.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.152, 'learning_rate': 1e-06, 'epoch': 4.14}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 59/140 [06:26<07:56,  5.89s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m43%|████▎     | 60/140 [06:31<07:43,  5.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.154, 'learning_rate': 1e-06, 'epoch': 4.21}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 60/140 [06:31<07:43,  5.79s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m44%|████▎     | 61/140 [06:37<07:30,  5.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1753, 'learning_rate': 1e-06, 'epoch': 4.28}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 61/140 [06:37<07:30,  5.71s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m44%|████▍     | 62/140 [06:42<07:21,  5.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1835, 'learning_rate': 1e-06, 'epoch': 4.35}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 62/140 [06:42<07:21,  5.66s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m45%|████▌     | 63/140 [06:48<07:13,  5.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1484, 'learning_rate': 1e-06, 'epoch': 4.42}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 63/140 [06:48<07:13,  5.63s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m46%|████▌     | 64/140 [06:53<07:06,  5.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1585, 'learning_rate': 1e-06, 'epoch': 4.49}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 64/140 [06:53<07:06,  5.61s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m46%|████▋     | 65/140 [06:59<07:00,  5.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1357, 'learning_rate': 1e-06, 'epoch': 4.56}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 65/140 [06:59<07:00,  5.60s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m47%|████▋     | 66/140 [07:05<06:54,  5.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1349, 'learning_rate': 1e-06, 'epoch': 4.63}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 66/140 [07:05<06:54,  5.60s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m48%|████▊     | 67/140 [07:10<06:48,  5.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1259, 'learning_rate': 1e-06, 'epoch': 4.7}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 67/140 [07:10<06:48,  5.59s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m49%|████▊     | 68/140 [07:16<06:42,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1205, 'learning_rate': 1e-06, 'epoch': 4.77}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 68/140 [07:16<06:42,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m49%|████▉     | 69/140 [07:21<06:36,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1181, 'learning_rate': 1e-06, 'epoch': 4.84}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 69/140 [07:21<06:36,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m50%|█████     | 70/140 [07:27<06:30,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1189, 'learning_rate': 1e-06, 'epoch': 4.91}\u001b[0m\n",
      "\u001b[34m50%|█████     | 70/140 [07:27<06:30,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m51%|█████     | 71/140 [07:32<06:24,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1197, 'learning_rate': 1e-06, 'epoch': 4.98}\u001b[0m\n",
      "\u001b[34m51%|█████     | 71/140 [07:32<06:24,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 72/140 [07:38<06:18,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1239, 'learning_rate': 1e-06, 'epoch': 5.05}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 72/140 [07:38<06:18,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 73/140 [07:44<06:13,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1345, 'learning_rate': 1e-06, 'epoch': 5.12}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 73/140 [07:44<06:13,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 74/140 [07:49<06:07,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1174, 'learning_rate': 1e-06, 'epoch': 5.19}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 74/140 [07:49<06:07,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 75/140 [08:21<14:32, 13.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1187, 'learning_rate': 1e-06, 'epoch': 5.26}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 75/140 [08:21<14:32, 13.42s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 76/140 [08:26<11:48, 11.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.119, 'learning_rate': 1e-06, 'epoch': 5.33}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 76/140 [08:26<11:48, 11.07s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 77/140 [08:32<09:53,  9.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1187, 'learning_rate': 1e-06, 'epoch': 5.4}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 77/140 [08:32<09:53,  9.42s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 78/140 [08:38<08:32,  8.26s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1169, 'learning_rate': 1e-06, 'epoch': 5.47}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 78/140 [08:38<08:32,  8.26s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 79/140 [08:43<07:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1196, 'learning_rate': 1e-06, 'epoch': 5.54}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 79/140 [08:43<07:34,  7.46s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 80/140 [08:49<06:53,  6.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1201, 'learning_rate': 1e-06, 'epoch': 5.61}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 80/140 [08:49<06:53,  6.89s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 81/140 [08:54<06:23,  6.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.123, 'learning_rate': 1e-06, 'epoch': 5.68}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 81/140 [08:54<06:23,  6.50s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 82/140 [09:00<06:01,  6.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1275, 'learning_rate': 1e-06, 'epoch': 5.75}\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 82/140 [09:00<06:01,  6.23s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 83/140 [09:06<05:44,  6.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1197, 'learning_rate': 1e-06, 'epoch': 5.82}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 83/140 [09:06<05:44,  6.04s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m60%|██████    | 84/140 [09:11<05:30,  5.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1255, 'learning_rate': 1e-06, 'epoch': 5.89}\u001b[0m\n",
      "\u001b[34m60%|██████    | 84/140 [09:11<05:30,  5.90s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m61%|██████    | 85/140 [09:17<05:19,  5.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1241, 'learning_rate': 1e-06, 'epoch': 5.96}\u001b[0m\n",
      "\u001b[34m61%|██████    | 85/140 [09:17<05:19,  5.80s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 86/140 [09:22<05:09,  5.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1222, 'learning_rate': 1e-06, 'epoch': 6.04}\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 86/140 [09:22<05:09,  5.73s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 87/140 [09:28<05:01,  5.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1205, 'learning_rate': 1e-06, 'epoch': 6.11}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 87/140 [09:28<05:01,  5.68s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 88/140 [09:33<04:53,  5.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1271, 'learning_rate': 1e-06, 'epoch': 6.18}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 88/140 [09:33<04:53,  5.64s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 89/140 [09:39<04:46,  5.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1267, 'learning_rate': 1e-06, 'epoch': 6.25}\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 89/140 [09:39<04:46,  5.62s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 90/140 [09:45<04:40,  5.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1238, 'learning_rate': 1e-06, 'epoch': 6.32}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 90/140 [09:45<04:40,  5.60s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 91/140 [09:50<04:33,  5.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1248, 'learning_rate': 1e-06, 'epoch': 6.39}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 91/140 [09:50<04:33,  5.59s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 92/140 [09:56<04:28,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1161, 'learning_rate': 1e-06, 'epoch': 6.46}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 92/140 [09:56<04:28,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 93/140 [10:01<04:22,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1181, 'learning_rate': 1e-06, 'epoch': 6.53}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 93/140 [10:01<04:22,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 94/140 [10:07<04:16,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1176, 'learning_rate': 1e-06, 'epoch': 6.6}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 94/140 [10:07<04:16,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 95/140 [10:12<04:11,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1121, 'learning_rate': 1e-06, 'epoch': 6.67}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 95/140 [10:12<04:11,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 96/140 [10:18<04:05,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1083, 'learning_rate': 1e-06, 'epoch': 6.74}\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 96/140 [10:18<04:05,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 97/140 [10:24<04:00,  5.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1251, 'learning_rate': 1e-06, 'epoch': 6.81}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 97/140 [10:24<04:00,  5.59s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m70%|███████   | 98/140 [10:29<03:54,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1168, 'learning_rate': 1e-06, 'epoch': 6.88}\u001b[0m\n",
      "\u001b[34m70%|███████   | 98/140 [10:29<03:54,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m71%|███████   | 99/140 [10:35<03:48,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1194, 'learning_rate': 1e-06, 'epoch': 6.95}\u001b[0m\n",
      "\u001b[34m71%|███████   | 99/140 [10:35<03:48,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 100/140 [11:07<09:08, 13.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1078, 'learning_rate': 1e-06, 'epoch': 7.02}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 100/140 [11:07<09:08, 13.71s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 63\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 101/140 [11:13<07:19, 11.26s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1172, 'learning_rate': 1e-06, 'epoch': 7.09}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 101/140 [11:13<07:19, 11.26s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 102/140 [11:18<06:03,  9.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1138, 'learning_rate': 1e-06, 'epoch': 7.16}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 102/140 [11:18<06:03,  9.55s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 103/140 [11:24<05:09,  8.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.119, 'learning_rate': 1e-06, 'epoch': 7.23}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 103/140 [11:24<05:09,  8.36s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 104/140 [11:30<04:30,  7.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1103, 'learning_rate': 1e-06, 'epoch': 7.3}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 104/140 [11:30<04:30,  7.52s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 105/140 [11:35<04:02,  6.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1137, 'learning_rate': 1e-06, 'epoch': 7.37}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 105/140 [11:35<04:02,  6.94s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 106/140 [11:41<03:41,  6.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1178, 'learning_rate': 1e-06, 'epoch': 7.44}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 106/140 [11:41<03:41,  6.53s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 107/140 [11:46<03:25,  6.24s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1115, 'learning_rate': 1e-06, 'epoch': 7.51}\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 107/140 [11:46<03:25,  6.24s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 108/140 [11:52<03:12,  6.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1166, 'learning_rate': 1e-06, 'epoch': 7.58}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 108/140 [11:52<03:12,  6.03s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 109/140 [11:57<03:02,  5.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1086, 'learning_rate': 1e-06, 'epoch': 7.65}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 109/140 [11:57<03:02,  5.89s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 110/140 [12:03<02:53,  5.79s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1091, 'learning_rate': 1e-06, 'epoch': 7.72}\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 110/140 [12:03<02:53,  5.79s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 111/140 [12:09<02:46,  5.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1091, 'learning_rate': 1e-06, 'epoch': 7.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 111/140 [12:09<02:46,  5.73s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 88 vs. 90\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m80%|████████  | 112/140 [12:14<02:38,  5.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1169, 'learning_rate': 1e-06, 'epoch': 7.86}\u001b[0m\n",
      "\u001b[34m80%|████████  | 112/140 [12:14<02:38,  5.68s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m81%|████████  | 113/140 [12:20<02:32,  5.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1176, 'learning_rate': 1e-06, 'epoch': 7.93}\u001b[0m\n",
      "\u001b[34m81%|████████  | 113/140 [12:20<02:32,  5.65s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 114/140 [12:25<02:26,  5.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1158, 'learning_rate': 1e-06, 'epoch': 8.0}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 114/140 [12:25<02:26,  5.63s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 115/140 [12:31<02:20,  5.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1144, 'learning_rate': 1e-06, 'epoch': 8.07}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 115/140 [12:31<02:20,  5.61s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 116/140 [12:36<02:14,  5.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1155, 'learning_rate': 1e-06, 'epoch': 8.14}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 116/140 [12:36<02:14,  5.59s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 117/140 [12:42<02:08,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1113, 'learning_rate': 1e-06, 'epoch': 8.21}\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 117/140 [12:42<02:08,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 118/140 [12:48<02:02,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1115, 'learning_rate': 1e-06, 'epoch': 8.28}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 118/140 [12:48<02:02,  5.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 119/140 [12:53<01:57,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1077, 'learning_rate': 1e-06, 'epoch': 8.35}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 119/140 [12:53<01:57,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 120/140 [12:59<01:51,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.111, 'learning_rate': 1e-06, 'epoch': 8.42}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 120/140 [12:59<01:51,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 121/140 [13:04<01:45,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1164, 'learning_rate': 1e-06, 'epoch': 8.49}\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 121/140 [13:04<01:45,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 122/140 [13:10<01:40,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1121, 'learning_rate': 1e-06, 'epoch': 8.56}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 122/140 [13:10<01:40,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 123/140 [13:15<01:34,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1131, 'learning_rate': 1e-06, 'epoch': 8.63}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 123/140 [13:15<01:34,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 124/140 [13:21<01:29,  5.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1134, 'learning_rate': 1e-06, 'epoch': 8.7}\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 124/140 [13:21<01:29,  5.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 125/140 [13:53<03:24, 13.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1113, 'learning_rate': 1e-06, 'epoch': 8.77}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 125/140 [13:53<03:24, 13.64s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 77\u001b[0m\n",
      "\u001b[34m90%|█████████ | 126/140 [13:59<02:37, 11.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.108, 'learning_rate': 1e-06, 'epoch': 8.84}\u001b[0m\n",
      "\u001b[34m90%|█████████ | 126/140 [13:59<02:37, 11.23s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34m91%|█████████ | 127/140 [14:05<02:03,  9.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1109, 'learning_rate': 1e-06, 'epoch': 8.91}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 127/140 [14:05<02:03,  9.54s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 128/140 [14:10<01:40,  8.35s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1222, 'learning_rate': 1e-06, 'epoch': 8.98}\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 128/140 [14:10<01:40,  8.35s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 129/140 [14:16<01:22,  7.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1169, 'learning_rate': 1e-06, 'epoch': 9.05}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 129/140 [14:16<01:22,  7.53s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 130/140 [14:21<01:09,  6.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1154, 'learning_rate': 1e-06, 'epoch': 9.12}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 130/140 [14:21<01:09,  6.95s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 131/140 [14:27<00:58,  6.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1214, 'learning_rate': 1e-06, 'epoch': 9.19}\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 131/140 [14:27<00:58,  6.54s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 132/140 [14:33<00:50,  6.26s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1119, 'learning_rate': 1e-06, 'epoch': 9.26}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 132/140 [14:33<00:50,  6.26s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 133/140 [14:38<00:42,  6.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1155, 'learning_rate': 1e-06, 'epoch': 9.33}\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 133/140 [14:38<00:42,  6.06s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 64 vs. 66\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 134/140 [14:44<00:35,  5.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1094, 'learning_rate': 1e-06, 'epoch': 9.4}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 134/140 [14:44<00:35,  5.92s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 135/140 [14:49<00:29,  5.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1092, 'learning_rate': 1e-06, 'epoch': 9.47}\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 135/140 [14:49<00:29,  5.82s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 136/140 [14:55<00:23,  5.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1095, 'learning_rate': 1e-06, 'epoch': 9.54}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 136/140 [14:55<00:23,  5.75s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 137/140 [15:01<00:17,  5.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1097, 'learning_rate': 1e-06, 'epoch': 9.61}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 137/140 [15:01<00:17,  5.71s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 138/140 [15:06<00:11,  5.68s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1099, 'learning_rate': 1e-06, 'epoch': 9.68}\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 138/140 [15:06<00:11,  5.68s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 139/140 [15:12<00:05,  5.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1096, 'learning_rate': 1e-06, 'epoch': 9.75}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 139/140 [15:12<00:05,  5.65s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m100%|██████████| 140/140 [15:17<00:00,  5.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1082, 'learning_rate': 1e-06, 'epoch': 9.82}\u001b[0m\n",
      "\u001b[34m100%|██████████| 140/140 [15:17<00:00,  5.63s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 917.8833, 'train_samples_per_second': 9.914, 'train_steps_per_second': 0.153, 'train_loss': 0.592300203868321, 'epoch': 9.82}\u001b[0m\n",
      "\u001b[34m100%|██████████| 140/140 [15:17<00:00,  5.63s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 140/140 [15:17<00:00,  6.56s/it]\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: - 0.012 MB of 0.032 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: - 0.012 MB of 0.032 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: \\ 0.013 MB of 0.032 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: \\ 0.013 MB of 0.032 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: | 0.028 MB of 0.032 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: | 0.032 MB of 0.032 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-841rhg-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-841rhg-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-841rhg-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-bczawn-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-bczawn-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093400-vicuna-demo-2023-10-17-09-20-23-781-bczawn-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-weeoct-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-weeoct-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093400-vicuna-demo-2023-10-17-09-20-23-781-weeoct-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-ok6nct-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-ok6nct-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093400-vicuna-demo-2023-10-17-09-20-23-781-ok6nct-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-f5zxol-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-f5zxol-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-f5zxol-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-14ahgx-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-14ahgx-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-14ahgx-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-0rlgzy-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-0rlgzy-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-0rlgzy-algo-1/logs\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:32,611] [INFO] [launch.py:347:main] Process 361 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:33,612] [INFO] [launch.py:347:main] Process 358 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:34,614] [INFO] [launch.py:347:main] Process 359 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:34,614] [INFO] [launch.py:347:main] Process 364 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:35,615] [INFO] [launch.py:347:main] Process 363 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:35,615] [INFO] [launch.py:347:main] Process 360 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:35,616] [INFO] [launch.py:347:main] Process 362 exits successfully.\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: - 0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: \\ 0.012 MB of 0.909 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: | 0.904 MB of 0.909 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: / 0.909 MB of 0.909 MB uploaded (0.000 MB deduped)\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\u001b[0m\n",
      "\u001b[34mwandb:            train/learning_rate ▁▁▁▄████████████████████████████████████\u001b[0m\n",
      "\u001b[34mwandb:                     train/loss ████▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos ▁\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss ▁\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime ▁\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch 9.82\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step 140\u001b[0m\n",
      "\u001b[34mwandb:            train/learning_rate 0.0\u001b[0m\n",
      "\u001b[34mwandb:                     train/loss 0.1082\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos 29260806881280.0\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss 0.5923\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime 917.8833\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second 9.914\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second 0.153\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-20-23-781-3mk6ey-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-20-23-781-3mk6ey-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_093359-vicuna-demo-2023-10-17-09-20-23-781-3mk6ey-algo-1/logs\u001b[0m\n",
      "\u001b[34m[2023-10-17 09:50:45,626] [INFO] [launch.py:347:main] Process 357 exits successfully.\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/added_tokens.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/generation_config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/training_args.bin s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer.model s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/tokenizer.model\u001b[0m\n",
      "\n",
      "2023-10-17 09:51:39 Uploading - Uploading generated training model\u001b[34mcp /tmp/llama_out/pytorch_model.bin s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-09-50-47/llama_out/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2023-10-17 09:51:32,004 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-17 09:51:32,004 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-17 09:51:32,005 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-17 09:51:45 Completed - Training job completed\n",
      "Training seconds: 1588\n",
      "Billable seconds: 1588\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'TAG': \"single node\",\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'vicuna-demo'         \n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train.sh',\n",
    "                      source_dir='./FastChat/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "#estimator.fit()\n",
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134d55e-cdb1-4e9d-be23-252755bbffce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 多机多卡 torch distribute + deepspeed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcca39f-029e-4c95-81f8-a5cabf717607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./FastChat/ds-train-distribute.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./FastChat/ds-train-distribute.sh\n",
    "#!/bin/bash\n",
    "export WANDB_API_KEY=\"298b59ce8a416fd45b5fa9ffc17fe72327854e0c\"\n",
    "export WANDB_WATCH=\"all\"\n",
    "export WANDB_PROJECT=\"llama-finetune\" \n",
    "\n",
    "\n",
    "SM_MASTER=\"${SM_MASTER}\"\n",
    "SM_MASTER_ADDR=\"${SM_MASTER_ADDR}\"\n",
    "CURRENT_HOST=\"${SM_CURRENT_HOST}\"\n",
    "\n",
    "\n",
    "IFS=',' read -ra hosts_array <<< \"${SM_HOSTS}\"\n",
    "NNODES=${#hosts_array[@]}\n",
    "NODE_RANK=0\n",
    "\n",
    "for i in \"${!hosts_array[@]}\"; do\n",
    "    if [[ \"${hosts_array[$i]}\" == *${CURRENT_HOST}* ]]; then\n",
    "        echo \"host index：$i\"\n",
    "        NODE_RANK=\"$i\" \n",
    "    fi\n",
    "done\n",
    "   \n",
    "    \n",
    "MASTER_PORT=\"23456\"\n",
    "export NCCL_SOCKET_IFNAME=\"eth0\"\n",
    "\n",
    "#Configure the distributed arguments for torch.distributed.launch.\n",
    "GPUS_PER_NODE=\"$SM_NUM_GPUS\"\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "\n",
    "SAVE_PATH=\"${SM_WORKING_DIR}/results\"\n",
    "LOG_FILE=\"${SAVE_PATH}/log.txt\"\n",
    "\n",
    "echo \"model dir on S3:\"\n",
    "aws s3 ls s3://${MODEL_S3_BUCKET}/llama/pretrain/pinkmanlove/llama-7b-hf/\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llama/pretrain/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "\n",
    "#cd FastChat && pip install -e . && cd ..\n",
    "pip install -e .\n",
    "\n",
    "\n",
    "DEEPSPEED_OPTS=\"\"\"\n",
    "  ./fastchat/train/train_mem.py \n",
    "    --deepspeed ds.json \n",
    "    --model_name_or_path \"/tmp/llama_pretrain/\" \n",
    "    --data_path data/dummy_conversation.json \n",
    "    --output_dir \"/tmp/llama_out\" \n",
    "    --num_train_epochs 10 \n",
    "    --per_device_train_batch_size 2 \n",
    "    --per_device_eval_batch_size  8 \n",
    "    --gradient_accumulation_steps 8 \n",
    "    --evaluation_strategy \"no\" \n",
    "    --save_strategy \"no\" \n",
    "    --save_steps 2000 \n",
    "    --save_total_limit 1 \n",
    "    --learning_rate 2e-5 \n",
    "    --weight_decay 0. \n",
    "    --warmup_ratio 0.03 \n",
    "    --lr_scheduler_type \"cosine\" \n",
    "    --logging_steps 2 \n",
    "    --cache_dir '/tmp' \n",
    "    --model_max_length 2048 \n",
    "    --gradient_checkpointing True \n",
    "    --lazy_preprocess True \n",
    "    --fp16 True \n",
    "    --tf32 True \n",
    "\"\"\"    \n",
    "\n",
    "CMD=\"torchrun ${DISTRIBUTED_ARGS} ${DEEPSPEED_OPTS}\"\n",
    "echo ${CMD}\n",
    "${CMD} 2>&1 \n",
    "echo \"begin to upload trained model\"\n",
    "echo \"current host==\"${CURRENT_HOST}\n",
    "echo \"master host==\"${MASTER_ADDR}\n",
    "if [[ \"${CURRENT_HOST}\" == \"${MASTER_ADDR}\" ]]; then  \n",
    "    ./s5cmd sync /tmp/llama_out s3://$MODEL_S3_BUCKET/llama/output/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3b0782-a59a-4457-b8a1-f44888f6c82a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.interactive_apps.base_interactive_app:NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: vicuna-demo-2023-10-17-09-56-01-459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-17 09:56:05 Starting - Starting the training job\n",
      "2023-10-17 09:56:05 Pending - Training job waiting for capacity......\n",
      "2023-10-17 09:56:43 Pending - Preparing the instances for training........................\n",
      "2023-10-17 10:00:54 Downloading - Downloading input data...\n",
      "2023-10-17 10:01:14 Training - Downloading the training image.................................\n",
      "2023-10-17 10:06:56 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:51,599 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:51,656 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:51,032 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:51,089 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:51,099 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:51,101 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:51,665 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:51,667 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:52,529 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:52,599 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:52,666 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:52,677 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"vicuna-demo-2023-10-17-09-56-01-459\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-56-01-459/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train-distribute.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train-distribute.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds-train-distribute.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds-train-distribute.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-56-01-459/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"vicuna-demo-2023-10-17-09-56-01-459\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-56-01-459/source/sourcedir.tar.gz\",\"module_name\":\"ds-train-distribute.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train-distribute.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./ds-train-distribute.sh \"\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:53,020 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:53,088 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:53,155 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:53,166 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"vicuna-demo-2023-10-17-09-56-01-459\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-56-01-459/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train-distribute.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train-distribute.sh\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=ds-train-distribute.sh\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=ds-train-distribute.sh\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-56-01-459/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"vicuna-demo-2023-10-17-09-56-01-459\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/vicuna-demo-2023-10-17-09-56-01-459/source/sourcedir.tar.gz\",\"module_name\":\"ds-train-distribute.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train-distribute.sh\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35m/bin/sh -c \"./ds-train-distribute.sh \"\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:07:54,510] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:07:54,039] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:07:57.702: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:57,707 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m2023-10-17 10:07:57,726 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mhost index：0\u001b[0m\n",
      "\u001b[34mmodel dir on S3:\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:07:58.213: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:58,218 root         INFO     Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m2023-10-17 10:07:58,237 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mhost index：1\u001b[0m\n",
      "\u001b[35mmodel dir on S3:\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45        472 config.json\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45        137 generation_config.json\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45 9976634558 pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45 3500315539 pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45      26788 pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45          2 special_tokens_map.json\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45     499723 tokenizer.model\u001b[0m\n",
      "\u001b[34m2023-07-19 09:38:45        141 tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45        472 config.json\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45        137 generation_config.json\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45 9976634558 pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45 3500315539 pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45      26788 pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45          2 special_tokens_map.json\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45     499723 tokenizer.model\u001b[0m\n",
      "\u001b[35m2023-07-19 09:38:45        141 tokenizer_config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00002-of-00002.bin /tmp/llama_pretrain/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00002-of-00002.bin /tmp/llama_pretrain/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[35mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00001-of-00002.bin /tmp/llama_pretrain/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llama/pretrain/pinkmanlove/llama-7b-hf/pytorch_model-00001-of-00002.bin /tmp/llama_pretrain/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[35mObtaining file:///opt/ml/code\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mChecking if build backend supports build_editable: started\u001b[0m\n",
      "\u001b[35mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mChecking if build backend supports build_editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build editable: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting shortuuid\u001b[0m\n",
      "\u001b[35mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (3.0.36)\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing editable metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (3.0.36)\u001b[0m\n",
      "\u001b[34mCollecting fastapi\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.103.2-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.3/66.3 kB 4.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.15.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.23.5)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 8.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic<=2.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.10.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mCollecting markdown2[all]\u001b[0m\n",
      "\u001b[35mDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.15.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.23.5)\u001b[0m\n",
      "\u001b[35mCollecting gradio\u001b[0m\n",
      "\u001b[35mDownloading gradio-3.48.0-py3-none-any.whl (20.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/20.3 MB 80.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting httpx\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.7/75.7 kB 24.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting nh3\u001b[0m\n",
      "\u001b[35mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 103.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.13.2)\u001b[0m\n",
      "\u001b[35mCollecting uvicorn\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (12.6.0)\u001b[0m\n",
      "\u001b[35mCollecting fastapi\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.103.2-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.3/66.3 kB 20.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: transformers<4.29.0,>=4.28.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (4.28.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic<=2.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.10.4)\u001b[0m\n",
      "\u001b[35mCollecting tiktoken\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 105.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.1.97)\u001b[0m\n",
      "\u001b[35mCollecting peft\u001b[0m\n",
      "\u001b[35mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 27.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (1.13.1+cu117)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.19) (0.2.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<=2.0->fschat==0.2.19) (4.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (0.9.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (2.14.0)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers<4.29.0,>=4.28.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (4.28.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (12.6.0)\u001b[0m\n",
      "\u001b[34mCollecting peft\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 21.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpx\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.7/75.7 kB 25.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nh3\u001b[0m\n",
      "\u001b[34mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 26.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting shortuuid\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.1.97)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.13.2)\u001b[0m\n",
      "\u001b[34mCollecting gradio\u001b[0m\n",
      "\u001b[34mDownloading gradio-3.48.0-py3-none-any.whl (20.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.3/20.3 MB 52.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from fschat==0.2.19) (2.28.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (23.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (3.9.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (4.64.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (2022.10.31)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate->fschat==0.2.19) (5.9.4)\u001b[0m\n",
      "\u001b[35mCollecting typing-extensions>=4.2.0\u001b[0m\n",
      "\u001b[35mDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\u001b[0m\n",
      "\u001b[35mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting anyio<4.0.0,>=3.7.1\u001b[0m\n",
      "\u001b[35mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 29.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting importlib-resources<7.0,>=1.3\u001b[0m\n",
      "\u001b[35mDownloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[35mCollecting altair<6.0,>=4.2.0\u001b[0m\n",
      "\u001b[35mDownloading altair-5.1.2-py3-none-any.whl (516 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 516.2/516.2 kB 79.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting python-multipart\u001b[0m\n",
      "\u001b[35mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.6.3)\u001b[0m\n",
      "\u001b[35mCollecting websockets<12.0,>=10.0\u001b[0m\n",
      "\u001b[35mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 34.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pydub\u001b[0m\n",
      "\u001b[35mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 67.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting gradio-client==0.6.1\u001b[0m\n",
      "\u001b[35mDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 52.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting markdown2[all]\u001b[0m\n",
      "\u001b[34mDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.19) (0.2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.9/site-packages (from pydantic<=2.0->fschat==0.2.19) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (0.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.19) (2.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (4.64.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (3.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (2022.10.31)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from transformers<4.29.0,>=4.28.0->fschat==0.2.19) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate->fschat==0.2.19) (5.9.4)\u001b[0m\n",
      "\u001b[34mCollecting anyio<4.0.0,>=3.7.1\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 23.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting ffmpy\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.3.1.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting orjson~=3.0\u001b[0m\n",
      "\u001b[35mDownloading orjson-3.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.6/138.6 kB 36.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.1.2)\u001b[0m\n",
      "\u001b[35mCollecting aiofiles<24.0,>=22.0\u001b[0m\n",
      "\u001b[35mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (2.1.2)\u001b[0m\n",
      "\u001b[35mCollecting ffmpy\u001b[0m\n",
      "\u001b[35mDownloading ffmpy-0.3.1.tar.gz (5.5 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (1.5.3)\u001b[0m\n",
      "\u001b[35mCollecting semantic-version~=2.0\u001b[0m\n",
      "\u001b[35mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from gradio-client==0.6.1->gradio->fschat==0.2.19) (2023.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (1.26.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2022.12.7)\u001b[0m\n",
      "\u001b[35mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 19.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn->fschat==0.2.19) (8.1.2)\u001b[0m\n",
      "\u001b[35mCollecting httpcore<0.19.0,>=0.18.0\u001b[0m\n",
      "\u001b[35mDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 16.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting sniffio\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting wavedrom\u001b[0m\n",
      "\u001b[35mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 39.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pydub\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mCollecting websockets<12.0,>=10.0\u001b[0m\n",
      "\u001b[34mDownloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.7/129.7 kB 39.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.6.3)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.9.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.6/138.6 kB 38.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==0.6.1\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 299.2/299.2 kB 57.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (1.5.3)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources<7.0,>=1.3\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.1.0-py3-none-any.whl (33 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.9/site-packages (from gradio->fschat==0.2.19) (9.4.0)\u001b[0m\n",
      "\u001b[34mCollecting altair<6.0,>=4.2.0\u001b[0m\n",
      "\u001b[34mDownloading altair-5.1.2-py3-none-any.whl (516 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 516.2/516.2 kB 80.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.0/302.0 kB 72.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from gradio-client==0.6.1->gradio->fschat==0.2.19) (2023.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->fschat==0.2.19) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from uvicorn->fschat==0.2.19) (8.1.2)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 20.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.19.0,>=0.18.0\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 28.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sniffio\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting wavedrom\u001b[0m\n",
      "\u001b[34mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 45.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting safetensors\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 108.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.20.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (0.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (0.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.1.37)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (65.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (4.17.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat==0.2.19) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.2.19) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->fschat==0.2.19) (4.0.10)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting safetensors\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35mCollecting fsspec\u001b[0m\n",
      "\u001b[35mDownloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 45.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio->fschat==0.2.19) (3.13.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (0.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.0.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (4.38.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->fschat==0.2.19) (2022.7.1)\u001b[0m\n",
      "\u001b[35mCollecting svgwrite\u001b[0m\n",
      "\u001b[35mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 19.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->fschat==0.2.19) (5.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (22.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.19.3)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: fschat, ffmpy, wavedrom\u001b[0m\n",
      "\u001b[35mBuilding editable for fschat (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding editable for fschat (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fschat: filename=fschat-0.2.19-0.editable-py3-none-any.whl size=12792 sha256=206739cf7d2eb1d6d12a7715f78c08e4b1e8f3fef39d8e4b3007b9987ef6c9cc\u001b[0m\n",
      "\u001b[35mStored in directory: /tmp/pip-ephem-wheel-cache-zjwfntkh/wheels/40/03/3a/5f39818cea87b3c154b54d046a775b3da4b8ed9b642b8d50e6\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 93.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathtools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.1.37)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb->fschat==0.2.19) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (4.17.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat==0.2.19) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb->fschat==0.2.19) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->fschat==0.2.19) (4.0.10)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\u001b[0m\n",
      "\u001b[34mDownloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 39.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio->fschat==0.2.19) (3.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (4.38.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from matplotlib~=3.0->gradio->fschat==0.2.19) (1.0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio->fschat==0.2.19) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting svgwrite\u001b[0m\n",
      "\u001b[34mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->fschat==0.2.19) (5.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->fschat==0.2.19) (0.19.3)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fschat, ffmpy, wavedrom\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=6154ae7835ef9687d5578aaadee16a5edb9b7bddffb91fa8f6d93d1ccc3bdd3c\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\u001b[0m\n",
      "\u001b[35mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30051 sha256=1b46e8e76ccef65d80fcb810e0ebf93541e7286709e2e8c7d04e9c6f49452077\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/81/08/ec/3e7bb60504c4ebf08e1d5c88e9abb85b0a3549d9f8d031113f\u001b[0m\n",
      "\u001b[35mSuccessfully built fschat ffmpy wavedrom\u001b[0m\n",
      "\u001b[34mBuilding editable for fschat (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fschat: filename=fschat-0.2.19-0.editable-py3-none-any.whl size=12792 sha256=206739cf7d2eb1d6d12a7715f78c08e4b1e8f3fef39d8e4b3007b9987ef6c9cc\u001b[0m\n",
      "\u001b[34mStored in directory: /tmp/pip-ephem-wheel-cache-9wkgxg7f/wheels/40/03/3a/5f39818cea87b3c154b54d046a775b3da4b8ed9b642b8d50e6\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for ffmpy (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=116a3f8f979ddc936b70637e4074c565ec016d9ce146b4f648e3d5936cac754b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/1f/f1/8d/367922b023b526b7c2ced5db30932def7b18cf39d7ac6e8572\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30051 sha256=1b46e8e76ccef65d80fcb810e0ebf93541e7286709e2e8c7d04e9c6f49452077\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/81/08/ec/3e7bb60504c4ebf08e1d5c88e9abb85b0a3549d9f8d031113f\u001b[0m\n",
      "\u001b[34mSuccessfully built fschat ffmpy wavedrom\u001b[0m\n",
      "\u001b[35mInstalling collected packages: pydub, nh3, ffmpy, websockets, typing-extensions, svgwrite, sniffio, shortuuid, semantic-version, safetensors, python-multipart, orjson, markdown2, importlib-resources, h11, fsspec, aiofiles, wavedrom, uvicorn, tiktoken, huggingface-hub, anyio, starlette, httpcore, altair, peft, httpx, fastapi, gradio-client, gradio, fschat\u001b[0m\n",
      "\u001b[35mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[35mFound existing installation: typing_extensions 4.4.0\u001b[0m\n",
      "\u001b[35mUninstalling typing_extensions-4.4.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled typing_extensions-4.4.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[35mFound existing installation: fsspec 2023.1.0\u001b[0m\n",
      "\u001b[35mUninstalling fsspec-2023.1.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled fsspec-2023.1.0\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydub, nh3, ffmpy, websockets, typing-extensions, svgwrite, sniffio, shortuuid, semantic-version, safetensors, python-multipart, orjson, markdown2, importlib-resources, h11, fsspec, aiofiles, wavedrom, uvicorn, tiktoken, huggingface-hub, anyio, starlette, httpcore, altair, peft, httpx, fastapi, gradio-client, gradio, fschat\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.4.0\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.4.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.4.0\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fsspec\u001b[0m\n",
      "\u001b[34mFound existing installation: fsspec 2023.1.0\u001b[0m\n",
      "\u001b[34mUninstalling fsspec-2023.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fsspec-2023.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.12.0\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.12.0\u001b[0m\n",
      "\u001b[35mSuccessfully installed aiofiles-23.2.1 altair-5.1.2 anyio-3.7.1 fastapi-0.103.2 ffmpy-0.3.1 fschat-0.2.19 fsspec-2023.9.2 gradio-3.48.0 gradio-client-0.6.1 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 importlib-resources-6.1.0 markdown2-2.4.10 nh3-0.2.14 orjson-3.9.9 peft-0.5.0 pydub-0.25.1 python-multipart-0.0.6 safetensors-0.4.0 semantic-version-2.10.0 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 tiktoken-0.5.1 typing-extensions-4.8.0 uvicorn-0.23.2 wavedrom-2.0.3.post3 websockets-11.0.3\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 23.3\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mtorchrun --nproc_per_node 8 --nnodes 2 --node_rank 1 --master_addr algo-1 --master_port 23456 ./fastchat/train/train_mem.py --deepspeed ds.json --model_name_or_path /tmp/llama_pretrain/ --data_path data/dummy_conversation.json --output_dir /tmp/llama_out --num_train_epochs 10 --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 2 --cache_dir '/tmp' --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --fp16 True --tf32 True\u001b[0m\n",
      "\u001b[34mSuccessfully installed aiofiles-23.2.1 altair-5.1.2 anyio-3.7.1 fastapi-0.103.2 ffmpy-0.3.1 fschat-0.2.19 fsspec-2023.9.2 gradio-3.48.0 gradio-client-0.6.1 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.18.0 importlib-resources-6.1.0 markdown2-2.4.10 nh3-0.2.14 orjson-3.9.9 peft-0.5.0 pydub-0.25.1 python-multipart-0.0.6 safetensors-0.4.0 semantic-version-2.10.0 shortuuid-1.0.11 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 tiktoken-0.5.1 typing-extensions-4.8.0 uvicorn-0.23.2 wavedrom-2.0.3.post3 websockets-11.0.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mtorchrun --nproc_per_node 8 --nnodes 2 --node_rank 0 --master_addr algo-1 --master_port 23456 ./fastchat/train/train_mem.py --deepspeed ds.json --model_name_or_path /tmp/llama_pretrain/ --data_path data/dummy_conversation.json --output_dir /tmp/llama_out --num_train_epochs 10 --per_device_train_batch_size 2 --per_device_eval_batch_size 8 --gradient_accumulation_steps 8 --evaluation_strategy no --save_strategy no --save_steps 2000 --save_total_limit 1 --learning_rate 2e-5 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 2 --cache_dir '/tmp' --model_max_length 2048 --gradient_checkpointing True --lazy_preprocess True --fp16 True --tf32 True\u001b[0m\n",
      "\u001b[35mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[35mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35m*****************************************\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,049] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,049] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,049] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,049] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,049] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,051] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,052] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:18,052] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,008] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,046] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:18,057] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[35mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[35m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[34mgit root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/opt/ml/code'\u001b[0m\n",
      "\u001b[34mTo add an exception for this directory, call:\u001b[0m\n",
      "\u001b[34m#011git config --global --add safe.directory /opt/ml/code'\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[35mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[35mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-veltq9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-veltq9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-veltq9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-m2bmd8-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:31,899] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-m2bmd8-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-m2bmd8-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:31,917] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-06nvfx-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-06nvfx-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-06nvfx-algo-1\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-jkmqov-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-jkmqov-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-jkmqov-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:31,995] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-qg2j5b-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-qg2j5b-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-qg2j5b-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,022] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-9n605f-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-9n605f-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-9n605f-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,048] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-483x00-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-483x00-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-483x00-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-jrmxef-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-jrmxef-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-jrmxef-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,070] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,074] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-4v7w9s-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-4v7w9s-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-4v7w9s-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,119] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-z33jnp-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-z33jnp-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-z33jnp-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,152] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[35mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[35mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-chhnz0-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[35mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-chhnz0-algo-2\u001b[0m\n",
      "\u001b[35mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-chhnz0-algo-2\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:09:32,217] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-nfh00z-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-nfh00z-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-nfh00z-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:31,972] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:31,986] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-mubsk0-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-mubsk0-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-mubsk0-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:32,012] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-s0uxs3-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-s0uxs3-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-s0uxs3-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:32,120] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-e4mhk3-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-e4mhk3-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-e4mhk3-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:32,149] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-x08k36-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run vicuna-demo-2023-10-17-09-56-01-459-x08k36-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/monitor_demo\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-x08k36-algo-1\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:32,216] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:32,216] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:09:38,505] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.97s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.97s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.99s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.01s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.04s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.04s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:11<00:11, 11.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.20s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.21s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.37s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.26s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.97s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.26s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.97s/it]\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.27s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.98s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.98s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.99s/it]\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  8.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.29s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  8.00s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.29s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.00s/it]\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.97s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  8.00s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  7.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.02s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.72s/it]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[35mLoading data...\u001b[0m\n",
      "\u001b[35mFormatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[34mLoading data...\u001b[0m\n",
      "\u001b[34mFormatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module fused_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[34m[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\u001b[0m\n",
      "\u001b[35m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[34m[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\u001b[0m\n",
      "\u001b[35m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.510759592056274 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.432808876037598 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.532726526260376 seconds\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.533066272735596 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.531214475631714 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.532766819000244 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.53197717666626 seconds\u001b[0m\n",
      "\u001b[35mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[35mTime to load fused_adam op: 26.532736778259277 seconds\u001b[0m\n",
      "\u001b[34m[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.80689764022827 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.733564853668213 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.733469009399414 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.83600950241089 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.83656406402588 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.833505392074585 seconds\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.83374834060669 seconds\u001b[0m\n",
      "\u001b[34mLoading extension module fused_adam...\u001b[0m\n",
      "\u001b[34mTime to load fused_adam op: 26.83338165283203 seconds\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.040: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.040: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.040: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.040: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.067 algo-2:285 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.067 algo-2:280 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.067 algo-2:284 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.067 algo-2:282 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.068 algo-2:279 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.068 algo-2:281 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.068 algo-2:283 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.069 algo-2:286 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.089 algo-2:283 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.089 algo-2:281 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.089 algo-2:285 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.089 algo-2:284 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.089 algo-2:280 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.090 algo-2:282 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.089 algo-2:279 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35m[2023-10-17 10:10:31.091 algo-2:286 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.037: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.038: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.039: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.044: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34mINFO:root:Using NamedTuple = typing._NamedTuple instead.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.066 algo-1:281 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.066 algo-1:280 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.068 algo-1:283 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.068 algo-1:277 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.068 algo-1:282 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.068 algo-1:279 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.068 algo-1:278 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.071 algo-1:276 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.088 algo-1:280 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.088 algo-1:281 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.090 algo-1:283 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.090 algo-1:279 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.090 algo-1:282 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.090 algo-1:278 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.090 algo-1:277 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34m[2023-10-17 10:10:31.092 algo-1:276 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34m3%|▎         | 1/30 [01:08<33:17, 68.89s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35m3%|▎         | 1/30 [01:08<33:17, 68.89s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35m7%|▋         | 2/30 [02:10<30:07, 64.57s/it]\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35m{'loss': 3.7016, 'learning_rate': 0, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[35m7%|▋         | 2/30 [02:10<30:07, 64.57s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m7%|▋         | 2/30 [02:10<30:07, 64.57s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7016, 'learning_rate': 0, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m7%|▋         | 2/30 [02:10<30:07, 64.57s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34m10%|█         | 3/30 [03:11<28:25, 63.18s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35m10%|█         | 3/30 [03:11<28:25, 63.18s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35m13%|█▎        | 4/30 [04:13<27:04, 62.49s/it]\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35m{'loss': 3.7136, 'learning_rate': 0, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[35m13%|█▎        | 4/30 [04:13<27:04, 62.49s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m13%|█▎        | 4/30 [04:13<27:04, 62.49s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7136, 'learning_rate': 0, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 4/30 [04:13<27:04, 62.49s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m17%|█▋        | 5/30 [05:14<25:52, 62.11s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35m17%|█▋        | 5/30 [05:14<25:52, 62.11s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35m20%|██        | 6/30 [06:16<24:46, 61.92s/it]\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35m{'loss': 3.7004, 'learning_rate': 0, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[35m20%|██        | 6/30 [06:16<24:46, 61.92s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m20%|██        | 6/30 [06:16<24:45, 61.92s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7004, 'learning_rate': 0, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[34m20%|██        | 6/30 [06:16<24:45, 61.92s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34m23%|██▎       | 7/30 [07:17<23:41, 61.81s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35m23%|██▎       | 7/30 [07:17<23:41, 61.82s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35m27%|██▋       | 8/30 [08:19<22:37, 61.70s/it]\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35m{'loss': 3.7022, 'learning_rate': 0, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[35m27%|██▋       | 8/30 [08:19<22:37, 61.70s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m27%|██▋       | 8/30 [08:19<22:37, 61.70s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7022, 'learning_rate': 0, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 8/30 [08:19<22:37, 61.70s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34m30%|███       | 9/30 [09:21<21:35, 61.69s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35m30%|███       | 9/30 [09:21<21:35, 61.69s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 76 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35m33%|███▎      | 10/30 [10:22<20:33, 61.67s/it]\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[35m{'loss': 3.7079, 'learning_rate': 0, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[35m33%|███▎      | 10/30 [10:22<20:33, 61.67s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m33%|███▎      | 10/30 [10:22<20:33, 61.67s/it]\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34mtried to get lr value before scheduler/optimizer started stepping, returning lr=0\u001b[0m\n",
      "\u001b[34m{'loss': 3.7079, 'learning_rate': 0, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 10/30 [10:22<20:33, 61.67s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35m37%|███▋      | 11/30 [11:24<19:31, 61.65s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m37%|███▋      | 11/30 [11:24<19:31, 61.65s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34m40%|████      | 12/30 [12:26<18:30, 61.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2618, 'learning_rate': 2e-05, 'epoch': 3.31}\u001b[0m\n",
      "\u001b[34m40%|████      | 12/30 [12:26<18:30, 61.67s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35m40%|████      | 12/30 [12:26<18:30, 61.67s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 2.2618, 'learning_rate': 2e-05, 'epoch': 3.31}\u001b[0m\n",
      "\u001b[35m40%|████      | 12/30 [12:26<18:30, 61.67s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 64 vs. 66\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35m43%|████▎     | 13/30 [16:12<31:37, 111.62s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m43%|████▎     | 13/30 [16:12<31:37, 111.62s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m47%|████▋     | 14/30 [17:14<25:43, 96.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6621, 'learning_rate': 2e-05, 'epoch': 3.86}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 14/30 [17:14<25:43, 96.48s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35m47%|████▋     | 14/30 [17:14<25:43, 96.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.6621, 'learning_rate': 2e-05, 'epoch': 3.86}\u001b[0m\n",
      "\u001b[35m47%|████▋     | 14/30 [17:14<25:43, 96.48s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 61 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35m50%|█████     | 15/30 [18:15<21:29, 85.95s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34m50%|█████     | 15/30 [18:15<21:29, 85.95s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 16/30 [19:17<18:20, 78.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 4.41}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 16/30 [19:17<18:20, 78.58s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 16/30 [19:17<18:20, 78.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.4214, 'learning_rate': 2e-05, 'epoch': 4.41}\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 16/30 [19:17<18:20, 78.58s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 17/30 [20:18<15:54, 73.44s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 17/30 [20:18<15:54, 73.44s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m60%|██████    | 18/30 [21:20<13:59, 69.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2581, 'learning_rate': 2e-05, 'epoch': 4.97}\u001b[0m\n",
      "\u001b[34m60%|██████    | 18/30 [21:20<13:59, 69.92s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35m60%|██████    | 18/30 [21:20<13:59, 69.92s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.2581, 'learning_rate': 2e-05, 'epoch': 4.97}\u001b[0m\n",
      "\u001b[35m60%|██████    | 18/30 [21:20<13:59, 69.92s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 19/30 [22:21<12:21, 67.41s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 19/30 [22:21<12:21, 67.41s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 20/30 [23:23<10:56, 65.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1809, 'learning_rate': 2e-05, 'epoch': 5.52}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 20/30 [23:23<10:56, 65.64s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 20/30 [23:23<10:56, 65.64s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1809, 'learning_rate': 2e-05, 'epoch': 5.52}\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 20/30 [23:23<10:56, 65.64s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 76 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35m70%|███████   | 21/30 [24:24<09:39, 64.40s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m70%|███████   | 21/30 [24:24<09:39, 64.40s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 22/30 [25:26<08:27, 63.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1646, 'learning_rate': 2e-05, 'epoch': 6.07}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 22/30 [25:26<08:27, 63.48s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 22/30 [25:26<08:27, 63.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1646, 'learning_rate': 2e-05, 'epoch': 6.07}\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 22/30 [25:26<08:27, 63.48s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 60 vs. 62\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 23/30 [26:27<07:20, 62.89s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 23/30 [26:27<07:20, 62.89s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34m80%|████████  | 24/30 [27:29<06:14, 62.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1541, 'learning_rate': 2e-05, 'epoch': 6.62}\u001b[0m\n",
      "\u001b[34m80%|████████  | 24/30 [27:29<06:14, 62.47s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35m80%|████████  | 24/30 [27:29<06:14, 62.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1541, 'learning_rate': 2e-05, 'epoch': 6.62}\u001b[0m\n",
      "\u001b[35m80%|████████  | 24/30 [27:29<06:14, 62.47s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 25/30 [31:16<09:18, 111.78s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 25/30 [31:16<09:18, 111.78s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 62 vs. 63\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 26/30 [32:17<06:26, 96.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1277, 'learning_rate': 2e-05, 'epoch': 7.17}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 26/30 [32:17<06:26, 96.67s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 26/30 [32:17<06:26, 96.67s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1277, 'learning_rate': 2e-05, 'epoch': 7.17}\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 26/30 [32:17<06:26, 96.67s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35m90%|█████████ | 27/30 [33:18<04:18, 86.10s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34m90%|█████████ | 27/30 [33:18<04:18, 86.10s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 62 vs. 64\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 28/30 [34:20<02:37, 78.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1222, 'learning_rate': 2e-05, 'epoch': 7.72}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 28/30 [34:20<02:37, 78.70s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 28/30 [34:20<02:37, 78.70s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1222, 'learning_rate': 2e-05, 'epoch': 7.72}\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 28/30 [34:20<02:37, 78.70s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 88 vs. 90\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 65 vs. 67\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 29/30 [35:21<01:13, 73.51s/it]\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 29/30 [35:21<01:13, 73.51s/it]\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 76 vs. 78\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 63 vs. 65\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 76\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 68 vs. 70\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 66 vs. 68\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 67 vs. 69\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 71 vs. 73\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 73 vs. 75\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 69 vs. 71\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 75 vs. 77\u001b[0m\n",
      "\u001b[34mWARNING: tokenization mismatch 74 vs. 76\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 72 vs. 74\u001b[0m\n",
      "\u001b[35mWARNING: tokenization mismatch 70 vs. 72\u001b[0m\n",
      "\u001b[35m100%|██████████| 30/30 [36:23<00:00, 69.89s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1181, 'learning_rate': 2e-05, 'epoch': 8.28}\u001b[0m\n",
      "\u001b[35m100%|██████████| 30/30 [36:23<00:00, 69.89s/it]\u001b[0m\n",
      "\u001b[35m{'train_runtime': 2183.7707, 'train_samples_per_second': 4.167, 'train_steps_per_second': 0.014, 'train_loss': 1.5331090211868286, 'epoch': 8.28}\u001b[0m\n",
      "\u001b[35m100%|██████████| 30/30 [36:23<00:00, 69.89s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 30/30 [36:23<00:00, 72.77s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 30/30 [36:23<00:00, 69.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1181, 'learning_rate': 2e-05, 'epoch': 8.28}\u001b[0m\n",
      "\u001b[34m100%|██████████| 30/30 [36:23<00:00, 69.89s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2183.6551, 'train_samples_per_second': 4.167, 'train_steps_per_second': 0.014, 'train_loss': 1.5331090211868286, 'epoch': 8.28}\u001b[0m\n",
      "\u001b[34m100%|██████████| 30/30 [36:23<00:00, 69.89s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 30/30 [36:23<00:00, 72.77s/it]\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[35mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-m2bmd8-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-m2bmd8-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-m2bmd8-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-06nvfx-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-06nvfx-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-06nvfx-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-s0uxs3-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-s0uxs3-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-s0uxs3-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-nfh00z-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-nfh00z-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-nfh00z-algo-1/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-483x00-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-483x00-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-483x00-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-4v7w9s-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-4v7w9s-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-4v7w9s-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-z33jnp-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-z33jnp-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-z33jnp-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-jrmxef-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-jrmxef-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-jrmxef-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-chhnz0-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-chhnz0-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-chhnz0-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-qg2j5b-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-qg2j5b-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-qg2j5b-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-jkmqov-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-jkmqov-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-jkmqov-algo-2/logs\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run history:\u001b[0m\n",
      "\u001b[35mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[35mwandb: \u001b[0m\n",
      "\u001b[35mwandb: Run summary:\u001b[0m\n",
      "\u001b[35mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[35mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[35mwandb:\u001b[0m\n",
      "\u001b[35mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-9n605f-algo-2 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-9n605f-algo-2\u001b[0m\n",
      "\u001b[35mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[35mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-9n605f-algo-2/logs\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-veltq9-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-veltq9-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-veltq9-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-e4mhk3-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-e4mhk3-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-e4mhk3-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:      epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:      epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb: total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-mubsk0-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-mubsk0-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-mubsk0-algo-1/logs\u001b[0m\n",
      "\u001b[34mwandb: Waiting for W&B process to finish... (success).\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run history:\u001b[0m\n",
      "\u001b[34mwandb:                          epoch ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb:                     total_flos ▁▂▃▄▅▆▇██\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch ▁▁▂▃▃▃▄▄▅▆▆▆▇▇██\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step ▁▁▂▃▃▃▄▅▅▅▆▇▇▇▇██\u001b[0m\n",
      "\u001b[34mwandb:            train/learning_rate ▁▁▁▁▁██████████\u001b[0m\n",
      "\u001b[34mwandb:                     train/loss █████▅▂▂▁▁▁▁▁▁▁\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos ▁\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss ▁\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime ▁\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second ▁\u001b[0m\n",
      "\u001b[34mwandb: \u001b[0m\n",
      "\u001b[34mwandb: Run summary:\u001b[0m\n",
      "\u001b[34mwandb:                          epoch 8.27586\u001b[0m\n",
      "\u001b[34mwandb:                     total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:                    train/epoch 8.28\u001b[0m\n",
      "\u001b[34mwandb:              train/global_step 30\u001b[0m\n",
      "\u001b[34mwandb:            train/learning_rate 2e-05\u001b[0m\n",
      "\u001b[34mwandb:                     train/loss 0.1181\u001b[0m\n",
      "\u001b[34mwandb:               train/total_flos 24602109542400.0\u001b[0m\n",
      "\u001b[34mwandb:               train/train_loss 1.53311\u001b[0m\n",
      "\u001b[34mwandb:            train/train_runtime 2183.6551\u001b[0m\n",
      "\u001b[34mwandb: train/train_samples_per_second 4.167\u001b[0m\n",
      "\u001b[34mwandb:   train/train_steps_per_second 0.014\u001b[0m\n",
      "\u001b[34mwandb:\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run vicuna-demo-2023-10-17-09-56-01-459-x08k36-algo-1 at: https://wandb.ai/jeff-llama-finetune/monitor_demo/runs/vicuna-demo-2023-10-17-09-56-01-459-x08k36-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[0m\n",
      "\u001b[34mwandb: Find logs at: ./wandb/run-20231017_100922-vicuna-demo-2023-10-17-09-56-01-459-x08k36-algo-1/logs\u001b[0m\n",
      "\u001b[35mbegin to upload trained model\u001b[0m\n",
      "\u001b[35mcurrent host==algo-2\u001b[0m\n",
      "\u001b[35mmaster host==algo-1\u001b[0m\n",
      "\u001b[35m2023-10-17 10:47:33,779 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-10-17 10:47:33,779 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-10-17 10:47:33,780 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34mbegin to upload trained model\u001b[0m\n",
      "\u001b[34mcurrent host==algo-1\u001b[0m\n",
      "\u001b[34mmaster host==algo-1\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/added_tokens.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/generation_config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/generation_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/training_args.bin s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/training_args.bin\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer.model s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/special_tokens_map.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/tokenizer_config.json s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp /tmp/llama_out/pytorch_model.bin s3://sagemaker-us-west-2-687912291502/llama/output/2023-10-17-10-47-33/llama_out/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m2023-10-17 10:48:17,774 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-17 10:48:17,775 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-17 10:48:17,775 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-17 10:49:14 Uploading - Uploading generated training model\n",
      "2023-10-17 10:49:14 Completed - Training job completed\n",
      "Training seconds: 5800\n",
      "Billable seconds: 5800\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "environment = {\n",
    "              'TAG': \"mutiple node\",\n",
    "              'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'vicuna-demo'         \n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train-distribute.sh',\n",
    "                      source_dir='./FastChat/',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=2,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      KeepAlivePeriodInSeconds=1800,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False,\n",
    "                      max_run=24*60*60*2)\n",
    "\n",
    "estimator.fit()\n",
    "#estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e673c",
   "metadata": {},
   "source": [
    "You could find the model path in S3 from above logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b535a9-a683-473b-a769-8d3b3354d89a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-19 15:07:45         21 added_tokens.json\n",
      "2023-07-19 15:07:45        545 config.json\n",
      "2023-07-19 15:07:45        132 generation_config.json\n",
      "2023-07-19 15:07:45 13476958625 pytorch_model.bin\n",
      "2023-07-19 15:07:45        423 special_tokens_map.json\n",
      "2023-07-19 15:07:45     499723 tokenizer.model\n",
      "2023-07-19 15:07:45        736 tokenizer_config.json\n",
      "2023-07-19 15:07:45       4795 training_args.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://sagemaker-us-west-2-687912291502/llama/output/2023-07-19-15-07-44/llama_out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ff2be0-bd27-478f-89a5-e95c6f9b8b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
