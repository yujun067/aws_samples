{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37930da-e0a6-406c-8a6a-a062746c7077",
   "metadata": {},
   "source": [
    "# An sample to finetune Baichuan2-7B-Base distruibutely on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb561e-2329-42bd-b280-119a8f06f6fe",
   "metadata": {},
   "source": [
    "**To avoid 'no left space' error, move the docker root directory**\n",
    "\n",
    "sudo systemctl stop docker\n",
    "\n",
    "sudo systemctl stop docker.socket \n",
    "\n",
    "sudo mv /var/lib/docker /home/ec2-user/SageMaker \n",
    "\n",
    "sudo ln -s /home/ec2-user/SageMaker/docker /var/lib/docker \n",
    "\n",
    "sudo systemctl start docker.socket\n",
    "\n",
    "sudo systemctl start docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eedcfaa-83cf-4645-a5d3-8eaa5a0dae30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df8dbba-4382-4fe5-9e6d-b75e1ca72bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess                     = sagemaker.Session()\n",
    "role                     = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account                  = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region                   = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07aa5a-b488-4294-8262-83023fc75adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "rm -rf s5cmd*\n",
    "wget https://github.com/peak/s5cmd/releases/download/v2.1.0/s5cmd_2.1.0_Linux-64bit.tar.gz\n",
    "tar xvzf s5cmd_2.1.0_Linux-64bit.tar.gz\n",
    "\n",
    "rm -rf src\n",
    "mkdir src\n",
    "cp s5cmd src/\n",
    "cd src\n",
    "\n",
    "# The version is 2023.09.09\n",
    "git clone https://github.com/baichuan-inc/Baichuan2.git\n",
    "cd Baichuan2\n",
    "git reset --hard f620769c667ce6380770398af1d8449e775ec271"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ee865-66c0-4897-843d-194b4cec10e3",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b62370-89f7-4d8f-b5ee-a83781c88326",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f4ec6-3033-4cb3-845a-1f2e1fc0a080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac9937-a3f8-49fc-94ad-faadfb7752c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"baichuan-inc/Baichuan2-7B-Base\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\", \"*.py\", \"*.txt\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    "    revision='f2cc3a689c5eba7dc7fd3757d0175d312d167604'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c20ac2-b016-4edd-81eb-d0f865f9234c",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e753bc-1f5d-4e5a-8994-b079e267fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0e690-fa83-4952-81cf-78aa3d173ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/ \n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76be42-cc9b-48dd-8d12-ab60d5c2d3e2",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37430e5c-9124-4e84-b16a-481d5ff36a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'sagemaker-stablediffusion-quick-kit'...\n",
      "remote: Enumerating objects: 897, done.\u001b[K\n",
      "remote: Counting objects: 100% (199/199), done.\u001b[K\n",
      "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
      "remote: Total 897 (delta 112), reused 122 (delta 99), pack-reused 698\u001b[K\n",
      "Receiving objects: 100% (897/897), 10.90 MiB | 34.99 MiB/s, done.\n",
      "Resolving deltas: 100% (450/450), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aws-samples/sagemaker-stablediffusion-quick-kit.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23458d2c-eb6a-4e94-a56e-32392e9dd699",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "# From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN pip3 install deepspeed==0.10.0 \\\n",
    "    && pip3 install transformers==4.29.2 \\\n",
    "    && pip3 install jsonlines==3.1.0 \\\n",
    "    && pip3 install accelerate==0.22.0 \\\n",
    "    && pip3 install xformers==0.0.21 \\\n",
    "    && pip3 install peft==0.5.0 \\\n",
    "    && pip3 install wandb\n",
    "\n",
    "COPY src/Baichuan2/fine-tune/requirements.txt .\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5d016e-2b6b-453d-a4a5-84062d2aed0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2e4ea-4ae9-473d-92d8-e11a56c1d499",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "075cd597-cf50-4c90-a2ac-110b72c9f202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name=\"sagemaker-baichuan2-7b-base-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa8c4c49-f234-4356-802c-616b79e3975e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  45.54MB\n",
      "Step 1/8 : From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
      " ---> 1f37d018af76\n",
      "Step 2/8 : ENV LANG=C.UTF-8\n",
      " ---> Using cache\n",
      " ---> 9cdd0c4b4c35\n",
      "Step 3/8 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> 7d0700ee0c80\n",
      "Step 4/8 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> f0079091e634\n",
      "Step 5/8 : RUN pip3 install deepspeed==0.10.0     && pip3 install transformers==4.29.2     && pip3 install jsonlines==3.1.0     && pip3 install accelerate==0.22.0     && pip3 install xformers==0.0.21     && pip3 install peft==0.5.0     && pip3 install wandb\n",
      " ---> Running in deff3140f331\n",
      "Collecting deepspeed==0.10.0\n",
      "  Downloading deepspeed-0.10.0.tar.gz (836 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 836.6/836.6 kB 20.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (1.11.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (5.9.5)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (1.10.7)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.10.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.0.0->deepspeed==0.10.0) (4.5.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.10.0) (3.12.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.10.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.10.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.10.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->deepspeed==0.10.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.10.0) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py): started\n",
      "  Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "  Created wheel for deepspeed: filename=deepspeed-0.10.0-py3-none-any.whl size=877457 sha256=6c24c5e641b21ba30cd2d3ad3c0b368b4b91ea494c324addcb3bda0dcc7e5e04\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/7b/3f/2807682bad2fba40ed888e6309597a5fda545ab30964c835aa\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: deepspeed\n",
      "  Attempting uninstall: deepspeed\n",
      "    Found existing installation: deepspeed 0.6.1+1ea3d4b\n",
      "    Uninstalling deepspeed-0.6.1+1ea3d4b:\n",
      "      Successfully uninstalled deepspeed-0.6.1+1ea3d4b\n",
      "Successfully installed deepspeed-0.10.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting transformers==4.29.2\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.1/7.1 MB 73.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.29.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.2) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.29.2) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.29.2) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.29.2) (2023.5.7)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.28.1\n",
      "    Uninstalling transformers-4.28.1:\n",
      "      Successfully uninstalled transformers-4.28.1\n",
      "Successfully installed transformers-4.29.2\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting jsonlines==3.1.0\n",
      "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines==3.1.0) (22.2.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-3.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting accelerate==0.22.0\n",
      "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.2/251.2 kB 10.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.19.0\n",
      "    Uninstalling accelerate-0.19.0:\n",
      "      Successfully uninstalled accelerate-0.19.0\n",
      "Successfully installed accelerate-0.22.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting xformers==0.0.21\n",
      "  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.0/167.0 MB 16.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xformers==0.0.21) (1.23.5)\n",
      "Collecting torch==2.0.1 (from xformers==0.0.21)\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.21) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.21) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.21) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.21) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->xformers==0.0.21) (3.1.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.0/21.0 MB 84.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 849.3/849.3 kB 78.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 113.2 MB/s eta 0:00:00\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 557.1/557.1 MB 4.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 317.1/317.1 MB 10.8 MB/s eta 0:00:00\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.4/168.4 MB 19.3 MB/s eta 0:00:00\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 MB 48.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.6/102.6 MB 30.1 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 MB 18.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.1/177.1 MB 13.7 MB/s eta 0:00:00\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.6/98.6 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting triton==2.0.0 (from torch==2.0.1->xformers==0.0.21)\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.3/63.3 MB 17.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->xformers==0.0.21) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->xformers==0.0.21) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.21) (3.26.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->xformers==0.0.21) (16.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->xformers==0.0.21) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->xformers==0.0.21) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, xformers\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0.dev20221202\n",
      "    Uninstalling triton-2.0.0.dev20221202:\n",
      "      Successfully uninstalled triton-2.0.0.dev20221202\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0\n",
      "    Uninstalling torch-2.0.0:\n",
      "      Successfully uninstalled torch-2.0.0\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0 xformers-0.0.21\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting peft==0.5.0\n",
      "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (2.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.29.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.65.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (0.22.0)\n",
      "Collecting safetensors (from peft==0.5.0)\n",
      "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 26.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.5.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.5.0) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.13.0->peft==0.5.0) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0) (3.26.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.13.0->peft==0.5.0) (16.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.5.0) (0.13.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.5.0) (2023.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.5.0) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft==0.5.0) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.5.0) (1.3.0)\n",
      "Installing collected packages: safetensors, peft\n",
      "Successfully installed peft-0.5.0 safetensors-0.4.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mCollecting wandb\n",
      "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 35.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 40.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.0/241.0 kB 43.9 MB/s eta 0:00:00\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (5.4.1)\n",
      "Collecting pathtools (from wandb)\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 14.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py): started\n",
      "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=acda26464fb3870aef75af3066107cc2bec8596196c990864eba2bd838768f85\n",
      "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
      "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 sentry-sdk-1.32.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container deff3140f331\n",
      " ---> e7e89cd3228d\n",
      "Step 6/8 : COPY src/Baichuan2/fine-tune/requirements.txt .\n",
      " ---> 53329c90eadc\n",
      "Step 7/8 : RUN pip3 install -r requirements.txt\n",
      " ---> Running in b932311d6278\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.29.2)\n",
      "Collecting torch==2.0.0 (from -r requirements.txt (line 3))\n",
      "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 619.9/619.9 MB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.99)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (0.13.3)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.22.0)\n",
      "Requirement already satisfied: deepspeed in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 3)) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->-r requirements.txt (line 3)) (0.40.0)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 3)) (3.26.3)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.0->-r requirements.txt (line 3)) (16.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2023.5.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 6)) (5.9.5)\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 7)) (1.10.7)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->-r requirements.txt (line 2)) (2023.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.0->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 2)) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xformers 0.0.21 requires torch==2.0.1, but you have torch 2.0.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed torch-2.0.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container b932311d6278\n",
      " ---> 455480183f2e\n",
      "Step 8/8 : ENV NVIDIA_VISIBLE_DEVICES=\"all\"\n",
      " ---> Running in 62d5934a6dc4\n",
      "Removing intermediate container 62d5934a6dc4\n",
      " ---> 314bae9f6fad\n",
      "Successfully built 314bae9f6fad\n",
      "Successfully tagged sagemaker-baichuan2-7b-base-finetune:latest\n",
      "The push refers to repository [687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-baichuan2-7b-base-finetune]\n",
      "e24603ced30c: Preparing\n",
      "8b6f3052a130: Preparing\n",
      "c9ea572ba183: Preparing\n",
      "ff37c276152d: Preparing\n",
      "a5ad883d9d7f: Preparing\n",
      "8403b2741d40: Preparing\n",
      "b8b2f58f17fe: Preparing\n",
      "0a86d2f63da9: Preparing\n",
      "e9149126e47f: Preparing\n",
      "81bcaebf20b7: Preparing\n",
      "b89ba47ef264: Preparing\n",
      "5e365e6e2026: Preparing\n",
      "30f10d0e1e2a: Preparing\n",
      "de6ad3f5baf9: Preparing\n",
      "8995be0bc275: Preparing\n",
      "7649740a6938: Preparing\n",
      "138718a88769: Preparing\n",
      "4c8ddbfabe2c: Preparing\n",
      "e11d715889d8: Preparing\n",
      "d2516bd9d454: Preparing\n",
      "ab91cb17a698: Preparing\n",
      "b8b2f58f17fe: Waiting\n",
      "375dafba5be7: Preparing\n",
      "eb2d5581a4b3: Preparing\n",
      "0a86d2f63da9: Waiting\n",
      "6e5ea4d3b078: Preparing\n",
      "a83e3f8647a8: Preparing\n",
      "629205717bfa: Preparing\n",
      "e9149126e47f: Waiting\n",
      "91962ccfdb56: Preparing\n",
      "e42093c82aca: Preparing\n",
      "81bcaebf20b7: Waiting\n",
      "de6ad3f5baf9: Waiting\n",
      "88f627f04385: Preparing\n",
      "53d4ef0348b1: Preparing\n",
      "b89ba47ef264: Waiting\n",
      "7dec9be1e6de: Preparing\n",
      "30f10d0e1e2a: Waiting\n",
      "5e365e6e2026: Waiting\n",
      "1c442bf32dda: Preparing\n",
      "8995be0bc275: Waiting\n",
      "7a4317d0452c: Preparing\n",
      "569b5fc6f9ba: Preparing\n",
      "7649740a6938: Waiting\n",
      "16acfff66e41: Preparing\n",
      "c2440becfb6e: Preparing\n",
      "93dc2ad27ff8: Preparing\n",
      "3b6112f80af1: Preparing\n",
      "138718a88769: Waiting\n",
      "1be54c625d9b: Preparing\n",
      "5d4d8e450a3a: Preparing\n",
      "4c8ddbfabe2c: Waiting\n",
      "aed2d71a436d: Preparing\n",
      "e11d715889d8: Waiting\n",
      "7af37e3e56a9: Preparing\n",
      "d2516bd9d454: Waiting\n",
      "e5167e76bf1b: Preparing\n",
      "a490a70ab1cd: Preparing\n",
      "ab91cb17a698: Waiting\n",
      "b3c248c52364: Preparing\n",
      "d543b8cad89e: Preparing\n",
      "375dafba5be7: Waiting\n",
      "e42093c82aca: Waiting\n",
      "569b5fc6f9ba: Waiting\n",
      "eb2d5581a4b3: Waiting\n",
      "88f627f04385: Waiting\n",
      "16acfff66e41: Waiting\n",
      "6e5ea4d3b078: Waiting\n",
      "c2440becfb6e: Waiting\n",
      "53d4ef0348b1: Waiting\n",
      "a83e3f8647a8: Waiting\n",
      "93dc2ad27ff8: Waiting\n",
      "7dec9be1e6de: Waiting\n",
      "3b6112f80af1: Waiting\n",
      "629205717bfa: Waiting\n",
      "91962ccfdb56: Waiting\n",
      "1c442bf32dda: Waiting\n",
      "1be54c625d9b: Waiting\n",
      "7a4317d0452c: Waiting\n",
      "5d4d8e450a3a: Waiting\n",
      "7af37e3e56a9: Waiting\n",
      "a490a70ab1cd: Waiting\n",
      "8403b2741d40: Waiting\n",
      "aed2d71a436d: Waiting\n",
      "e5167e76bf1b: Waiting\n",
      "b3c248c52364: Waiting\n",
      "d543b8cad89e: Waiting\n",
      "ff37c276152d: Layer already exists\n",
      "8b6f3052a130: Layer already exists\n",
      "a5ad883d9d7f: Layer already exists\n",
      "8403b2741d40: Layer already exists\n",
      "0a86d2f63da9: Layer already exists\n",
      "b8b2f58f17fe: Layer already exists\n",
      "81bcaebf20b7: Layer already exists\n",
      "b89ba47ef264: Layer already exists\n",
      "e9149126e47f: Layer already exists\n",
      "5e365e6e2026: Layer already exists\n",
      "30f10d0e1e2a: Layer already exists\n",
      "de6ad3f5baf9: Layer already exists\n",
      "8995be0bc275: Layer already exists\n",
      "7649740a6938: Layer already exists\n",
      "138718a88769: Layer already exists\n",
      "4c8ddbfabe2c: Layer already exists\n",
      "e11d715889d8: Layer already exists\n",
      "d2516bd9d454: Layer already exists\n",
      "ab91cb17a698: Layer already exists\n",
      "375dafba5be7: Layer already exists\n",
      "eb2d5581a4b3: Layer already exists\n",
      "6e5ea4d3b078: Layer already exists\n",
      "a83e3f8647a8: Layer already exists\n",
      "629205717bfa: Layer already exists\n",
      "91962ccfdb56: Layer already exists\n",
      "e42093c82aca: Layer already exists\n",
      "88f627f04385: Layer already exists\n",
      "53d4ef0348b1: Layer already exists\n",
      "7dec9be1e6de: Layer already exists\n",
      "1c442bf32dda: Layer already exists\n",
      "7a4317d0452c: Layer already exists\n",
      "569b5fc6f9ba: Layer already exists\n",
      "16acfff66e41: Layer already exists\n",
      "c2440becfb6e: Layer already exists\n",
      "93dc2ad27ff8: Layer already exists\n",
      "3b6112f80af1: Layer already exists\n",
      "1be54c625d9b: Layer already exists\n",
      "5d4d8e450a3a: Layer already exists\n",
      "aed2d71a436d: Layer already exists\n",
      "7af37e3e56a9: Layer already exists\n",
      "e5167e76bf1b: Layer already exists\n",
      "a490a70ab1cd: Layer already exists\n",
      "b3c248c52364: Layer already exists\n",
      "d543b8cad89e: Layer already exists\n",
      "e24603ced30c: Pushed\n",
      "c9ea572ba183: Pushed\n",
      "latest: digest: sha256:fd5b99a4ed62e1e3fcb16552bb8df3f013fd9fb19079675f595bf2eb68ef173a size: 10002\n"
     ]
    }
   ],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb15e9-107b-46cf-9644-ad7bc4c3231d",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49aa83-316b-45aa-931e-f18dbe003766",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/llama/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7879c-074c-4537-ba40-ee8d41a6747c",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "We modified some parts of ```Baichuan2/fine-tune/fine-tune.py```, such as how to save model.\n",
    "\n",
    "The version is 2023-09-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7330e88-dc4b-4fdf-a2a5-bd560b3334a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv src/Baichuan2/fine-tune/fine-tune.py src/Baichuan2/fine-tune/fine-tune.py.bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b21e47-9de7-4c59-a47d-0032ce697891",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/Baichuan2/fine-tune/fine-tune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/Baichuan2/fine-tune/fine-tune.py\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Optional, Dict\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"baichuan-inc/Baichuan2-7B-Base\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    use_lora: bool = field(default=False)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        tokenizer,\n",
    "        model_max_length,\n",
    "        user_tokens=[195],\n",
    "        assistant_tokens=[196],\n",
    "    ):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        self.data = json.load(open(data_path))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = model_max_length\n",
    "        self.user_tokens = user_tokens\n",
    "        self.assistant_tokens = assistant_tokens\n",
    "        self.ignore_index = -100\n",
    "        item = self.preprocessing(self.data[0])\n",
    "        print(\"input:\", self.tokenizer.decode(item[\"input_ids\"]))\n",
    "        labels = []\n",
    "        for id_ in item[\"labels\"]:\n",
    "            if id_ == -100:\n",
    "                continue\n",
    "\n",
    "            labels.append(id_)\n",
    "        print(\"label:\", self.tokenizer.decode(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def preprocessing(self, example):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "\n",
    "        for message in example[\"conversations\"]:\n",
    "            from_ = message[\"from\"]\n",
    "            value = message[\"value\"]\n",
    "            value_ids = self.tokenizer.encode(value)\n",
    "\n",
    "            if from_ == \"human\":\n",
    "                input_ids += self.user_tokens + value_ids\n",
    "                labels += [self.tokenizer.eos_token_id] + [self.ignore_index] * len(\n",
    "                    value_ids\n",
    "                )\n",
    "            else:\n",
    "                input_ids += self.assistant_tokens + value_ids\n",
    "                labels += [self.ignore_index] + value_ids\n",
    "        input_ids.append(self.tokenizer.eos_token_id)\n",
    "        labels.append(self.tokenizer.eos_token_id)\n",
    "        input_ids = input_ids[: self.model_max_length]\n",
    "        labels = labels[: self.model_max_length]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (\n",
    "            self.model_max_length - len(input_ids)\n",
    "        )\n",
    "        labels += [self.ignore_index] * (self.model_max_length - len(labels))\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        return self.preprocessing(self.data[idx])\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    if training_args.use_lora:\n",
    "        from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"W_pack\"],\n",
    "            inference_mode=False,\n",
    "            r=1,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "        )\n",
    "        model.enable_input_require_grads()\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    dataset = SupervisedDataset(\n",
    "        data_args.data_path, tokenizer, training_args.model_max_length\n",
    "    )\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model, args=training_args, train_dataset=dataset, tokenizer=tokenizer\n",
    "    )\n",
    "    trainer.train()\n",
    "    # trainer.save_state()\n",
    "    # trainer.save_model(output_dir=training_args.output_dir)\n",
    "    \n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86395940-1bda-4cb8-9465-60bf38bef57a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./src/ds-train-dist.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./src/ds-train-dist.sh\n",
    "#!/bin/bash\n",
    "export WANDB_API_KEY=\"298b59ce8a416fd45b5fa9ffc17fe72327854e0c\"\n",
    "export WANDB_WATCH=\"all\"\n",
    "export WANDB_PROJECT=\"baichuan\"\n",
    "\n",
    "CURRENT_HOST=\"${SM_CURRENT_HOST}\"\n",
    "\n",
    "\n",
    "IFS=',' read -ra hosts_array <<< \"${SM_HOSTS}\"\n",
    "NNODES=${#hosts_array[@]}\n",
    "NODE_RANK=0\n",
    "\n",
    "for i in \"${!hosts_array[@]}\"; do\n",
    "    if [[ \"${hosts_array[$i]}\" == *${CURRENT_HOST}* ]]; then\n",
    "        echo \"host index：$i\"\n",
    "        NODE_RANK=\"$i\" \n",
    "    fi\n",
    "done\n",
    "   \n",
    "    \n",
    "MASTER_PORT=\"13579\"\n",
    "export NCCL_SOCKET_IFNAME=\"eth0\"\n",
    "\n",
    "#Configure the distributed arguments for torch.distributed.launch.\n",
    "GPUS_PER_NODE=\"$SM_NUM_GPUS\"\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES \\\n",
    "                  --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/* /tmp/baichuan2/\n",
    "\n",
    "    \n",
    "DEEPSPEED_OPTS=\"\"\"\n",
    "    Baichuan2/fine-tune/fine-tune.py \n",
    "    --deepspeed Baichuan2/fine-tune/ds_config.json \n",
    "    --model_name_or_path \"/tmp/baichuan2/\" \n",
    "    --data_path \"Baichuan2/fine-tune/data/belle_chat_ramdon_10k.json\" \n",
    "    --output_dir \"/tmp/baichuan2_out\" \n",
    "    --num_train_epochs 1 \n",
    "    --per_device_train_batch_size 1 \n",
    "    --per_device_eval_batch_size  1 \n",
    "    --gradient_accumulation_steps 1 \n",
    "    --evaluation_strategy \"no\" \n",
    "    --save_strategy \"steps\" \n",
    "    --save_steps 2000 \n",
    "    --save_total_limit 2 \n",
    "    --learning_rate 2e-5 \n",
    "    --adam_beta1 0.9 \n",
    "    --adam_beta2 0.98 \n",
    "    --adam_epsilon 1e-8 \n",
    "    --weight_decay 1e-4 \n",
    "    --max_grad_norm 1.0 \n",
    "    --warmup_ratio 0.0 \n",
    "    --lr_scheduler_type \"constant\" \n",
    "    --logging_steps 1 \n",
    "    --cache_dir '/tmp' \n",
    "    --model_max_length 512 \n",
    "    --gradient_checkpointing True \n",
    "    --bf16 True \n",
    "    --report_to \"wandb\"\n",
    "\"\"\"    \n",
    "\n",
    "CMD=\"torchrun ${DISTRIBUTED_ARGS} ${DEEPSPEED_OPTS}\"\n",
    "echo ${CMD}\n",
    "${CMD} 2>&1 \n",
    "\n",
    "if [[ \"${CURRENT_HOST}\" == \"${MASTER_ADDR}\" ]]; then  \n",
    "    ./s5cmd sync /tmp/baichuan2_out s3://$MODEL_S3_BUCKET/llm/models/baichuan2/output/baichuan-inc/Baichuan-7B-Base/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74742cbf-3de0-4221-b09d-24559370c65d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'687912291502.dkr.ecr.us-west-2.amazonaws.com/sagemaker-baichuan2-7b-base-finetune:latest'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04ef0432-8dd5-4113-997b-50211448f654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "environment = {\n",
    "    'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'baichuan2-7b-base-finetune'\n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train-dist.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5f8bd-fedc-4bba-9874-909195b762e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: baichuan2-7b-base-finetune-2023-10-23-00-36-29-218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-23 00:36:32 Starting - Starting the training job...\n",
      "2023-10-23 00:36:47 Pending - Training job waiting for capacity...\n",
      "2023-10-23 00:37:22 Pending - Preparing the instances for training........................\n",
      "2023-10-23 00:41:28 Downloading - Downloading input data\n",
      "2023-10-23 00:41:28 Training - Downloading the training image..............................\n",
      "2023-10-23 00:46:25 Training - Training image download completed. Training in progress............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:12,310 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:12,367 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:12,374 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:12,376 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:14,384 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:14,447 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:14,510 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:14,518 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"baichuan2-7b-base-finetune-2023-10-23-00-36-29-218\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-687912291502/baichuan2-7b-base-finetune-2023-10-23-00-36-29-218/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"ds-train-dist.sh\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds-train-dist.sh\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds-train-dist.sh\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds-train-dist.sh\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-687912291502/baichuan2-7b-base-finetune-2023-10-23-00-36-29-218/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"baichuan2-7b-base-finetune-2023-10-23-00-36-29-218\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-687912291502/baichuan2-7b-base-finetune-2023-10-23-00-36-29-218/source/sourcedir.tar.gz\",\"module_name\":\"ds-train-dist.sh\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds-train-dist.sh\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/bin/sh -c \"./ds-train-dist.sh \"\u001b[0m\n",
      "\u001b[34m2023-10-23 00:48:14,545 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mhost index：0\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/added_tokens.json /tmp/baichuan2/added_tokens.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/config.json /tmp/baichuan2/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/generation_utils.py /tmp/baichuan2/generation_utils.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/configuration_baichuan.py /tmp/baichuan2/configuration_baichuan.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/modeling_baichuan.py /tmp/baichuan2/modeling_baichuan.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/tokenization_baichuan.py /tmp/baichuan2/tokenization_baichuan.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/tokenizer_config.json /tmp/baichuan2/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/special_tokens_map.json /tmp/baichuan2/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/pytorch_model.bin.index.json /tmp/baichuan2/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/quantizer.py /tmp/baichuan2/quantizer.py\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/tokenizer.model /tmp/baichuan2/tokenizer.model\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/pytorch_model-00002-of-00002.bin /tmp/baichuan2/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-west-2-687912291502/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/pytorch_model-00001-of-00002.bin /tmp/baichuan2/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mtorchrun --nproc_per_node 8 --nnodes 1 --node_rank 0 --master_addr algo-1 --master_port 13579 Baichuan2/fine-tune/fine-tune.py --deepspeed Baichuan2/fine-tune/ds_config.json --model_name_or_path /tmp/baichuan2/ --data_path Baichuan2/fine-tune/data/belle_chat_ramdon_10k.json --output_dir /tmp/baichuan2_out --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 2000 --save_total_limit 2 --learning_rate 2e-5 --adam_beta1 0.9 --adam_beta2 0.98 --adam_epsilon 1e-8 --weight_decay 1e-4 --max_grad_norm 1.0 --warmup_ratio 0.0 --lr_scheduler_type constant --logging_steps 1 --cache_dir '/tmp' --model_max_length 512 --gradient_checkpointing True --bf16 True --tf32 True --report_to wandb\u001b[0m\n",
      "\u001b[34mWARNING:torch.distributed.run:\u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34mSetting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m*****************************************\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,625] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,625] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,625] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,625] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,625] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,633] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,633] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:03,634] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,881] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,881] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,881] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,920] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,920] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,926] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,926] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,928] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,928] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,934] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,934] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,937] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,937] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,939] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,939] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,939] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:04,939] [INFO] [comm.py:616:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mNCCL version 2.14.3+cuda11.7\u001b[0m\n",
      "\u001b[34m[2023-10-23 00:49:08,290] [INFO] [partition_parameters.py:326:__exit__] finished initializing model with 7.51B parameters\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.73s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.78s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.85s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.92s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.30s/it]\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.48s/it]\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.65s/it]\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.54s/it]\u001b[0m\n",
      "\u001b[34minput:  <reserved_106>写一篇关于气候变化对海洋生态的影响的文章。\u001b[0m\n",
      "\u001b[34m<reserved_107>好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m<reserved_106>从刚刚的文章中提取出气候变化对海洋生物繁殖的影响。\u001b[0m\n",
      "\u001b[34m<reserved_107>文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\u001b[0m\n",
      "\u001b[34mlabel: </s> 好的，以下是你要求的文章：\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态的影响\u001b[0m\n",
      "\u001b[34m气候变化对海洋生态产生了广泛的影响。全球变暖导致海洋温度上升，这可能会对许多水生生物的生存、繁殖和迁移造成巨大的影响。另外，海洋酸化也是一个问题，因为大气中二氧化碳的增加导致海水中的pH值下降，使得许多生物的外骨骼变得脆弱，从而影响它们的生存和繁殖。\u001b[0m\n",
      "\u001b[34m此外，气候变化还会引起海平面上升，这不仅对海岸线和沿海居民产生影响，还会对许多海洋生物的栖息地造成影响。一些岛屿上的陆地生物面临几乎完全消失的风险，而另一些生物范围则会扩大到更广泛的领域，这可能会导致新的竞争和生态平衡的改变。\u001b[0m\n",
      "\u001b[34m总之，气候变化并不只是一个地球气温的问题，它对海洋生态系统产生了广泛和深远的影响。\u001b[0m\n",
      "\u001b[34m</s> 文章已经提到过气候变化会对海洋生物的生存、繁殖和迁移造成影响。例如，海洋酸化会使得许多生物的外骨骼变得脆弱，进而影响它们的生存和繁殖。此外，海洋温度上升也可能会对很多生物的生存和繁殖产生负面影响。</s>\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 121102723 (jeff-llama-finetune). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.12\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20231023_004927-baichuan2-7b-base-finetune-2023-10-23-00-36-29-218-ed643z-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run baichuan2-7b-base-finetune-2023-10-23-00-36-29-218-ed643z-algo-1\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/jeff-llama-finetune/baichuan\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/jeff-llama-finetune/baichuan/runs/baichuan2-7b-base-finetune-2023-10-23-00-36-29-218-ed643z-algo-1\u001b[0m\n",
      "\u001b[34m0%|          | 0/1250 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\u001b[0m\n",
      "\u001b[34m0%|          | 1/1250 [00:03<1:09:53,  3.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5752, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/1250 [00:03<1:09:53,  3.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/1250 [00:04<40:12,  1.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.8086, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/1250 [00:04<40:12,  1.93s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/1250 [00:04<28:13,  1.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4258, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/1250 [00:04<28:13,  1.36s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/1250 [00:05<22:35,  1.09s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1797, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/1250 [00:05<22:35,  1.09s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/1250 [00:06<19:27,  1.07it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2734, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/1250 [00:06<19:27,  1.07it/s]\u001b[0m\n",
      "\u001b[34m0%|          | 6/1250 [00:06<17:34,  1.18it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1094, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/1250 [00:06<17:34,  1.18it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 7/1250 [00:07<16:22,  1.27it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0244, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 7/1250 [00:07<16:22,  1.27it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 8/1250 [00:08<15:34,  1.33it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3145, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 8/1250 [00:08<15:34,  1.33it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 9/1250 [00:09<15:03,  1.37it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3154, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 9/1250 [00:09<15:03,  1.37it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 10/1250 [00:09<14:42,  1.41it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4082, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 10/1250 [00:09<14:42,  1.41it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 11/1250 [00:10<14:26,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4688, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 11/1250 [00:10<14:26,  1.43it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 12/1250 [00:11<14:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4561, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 12/1250 [00:11<14:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 13/1250 [00:11<14:08,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1689, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 13/1250 [00:11<14:08,  1.46it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 14/1250 [00:12<14:02,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3545, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 14/1250 [00:12<14:02,  1.47it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 15/1250 [00:13<13:58,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3506, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|          | 15/1250 [00:13<13:58,  1.47it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 16/1250 [00:13<13:55,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0811, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 16/1250 [00:13<13:55,  1.48it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 17/1250 [00:14<13:52,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2559, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 17/1250 [00:14<13:52,  1.48it/s]\u001b[0m\n",
      "\u001b[34m1%|▏         | 18/1250 [00:15<13:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3916, 'learning_rate': 2e-05, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 18/1250 [00:15<13:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/1250 [00:15<13:49,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5195, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/1250 [00:15<13:49,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 20/1250 [00:16<13:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0264, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 20/1250 [00:16<13:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 21/1250 [00:17<13:48,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0732, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 21/1250 [00:17<13:48,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 22/1250 [00:17<13:47,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3066, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 22/1250 [00:17<13:47,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 23/1250 [00:18<13:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2871, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 23/1250 [00:18<13:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 24/1250 [00:19<13:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2217, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 24/1250 [00:19<13:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 25/1250 [00:19<13:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3906, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 25/1250 [00:19<13:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 26/1250 [00:20<13:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1982, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 26/1250 [00:20<13:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 27/1250 [00:21<13:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0801, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 27/1250 [00:21<13:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 28/1250 [00:21<13:43,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3291, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 28/1250 [00:21<13:43,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 29/1250 [00:22<13:42,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4609, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 29/1250 [00:22<13:42,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 30/1250 [00:23<13:41,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6797, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 30/1250 [00:23<13:41,  1.48it/s]\u001b[0m\n",
      "\u001b[34m2%|▏         | 31/1250 [00:23<13:41,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.082, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m2%|▏         | 31/1250 [00:23<13:41,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 32/1250 [00:24<13:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2676, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 32/1250 [00:24<13:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 33/1250 [00:25<13:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2959, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 33/1250 [00:25<13:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 34/1250 [00:25<13:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4658, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 34/1250 [00:25<13:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 35/1250 [00:26<13:39,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1221, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 35/1250 [00:26<13:39,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 36/1250 [00:27<13:38,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6133, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 36/1250 [00:27<13:38,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 37/1250 [00:27<13:38,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5332, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 37/1250 [00:27<13:38,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 38/1250 [00:28<13:37,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2744, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 38/1250 [00:28<13:37,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 39/1250 [00:29<13:35,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0469, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 39/1250 [00:29<13:35,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 40/1250 [00:29<13:34,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.418, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 40/1250 [00:29<13:34,  1.48it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 41/1250 [00:30<13:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 41/1250 [00:30<13:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 42/1250 [00:31<13:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3154, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 42/1250 [00:31<13:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 43/1250 [00:31<13:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.165, 'learning_rate': 2e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 43/1250 [00:31<13:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 44/1250 [00:32<13:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3438, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 44/1250 [00:32<13:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 45/1250 [00:33<13:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2832, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 45/1250 [00:33<13:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▎         | 46/1250 [00:33<13:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5117, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▎         | 46/1250 [00:33<13:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 47/1250 [00:34<13:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3984, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 47/1250 [00:34<13:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 48/1250 [00:35<13:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5029, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 48/1250 [00:35<13:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 49/1250 [00:35<13:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4658, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 49/1250 [00:35<13:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 50/1250 [00:36<13:26,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2344, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 50/1250 [00:36<13:26,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 51/1250 [00:37<13:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3359, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 51/1250 [00:37<13:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 52/1250 [00:37<13:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2734, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 52/1250 [00:37<13:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 53/1250 [00:38<13:24,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5518, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 53/1250 [00:38<13:24,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 54/1250 [00:39<13:23,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5381, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 54/1250 [00:39<13:23,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 55/1250 [00:39<13:23,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3535, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 55/1250 [00:39<13:23,  1.49it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 56/1250 [00:40<13:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5039, 'learning_rate': 2e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 56/1250 [00:40<13:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 57/1250 [00:41<13:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2285, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 57/1250 [00:41<13:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 58/1250 [00:41<13:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9424, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 58/1250 [00:41<13:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 59/1250 [00:42<13:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2095, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 59/1250 [00:42<13:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 60/1250 [00:43<13:19,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9663, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 60/1250 [00:43<13:19,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 61/1250 [00:43<13:17,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3828, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 61/1250 [00:43<13:17,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 62/1250 [00:44<13:16,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4629, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▍         | 62/1250 [00:44<13:16,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 63/1250 [00:45<13:15,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2627, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 63/1250 [00:45<13:15,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 64/1250 [00:45<13:13,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5176, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 64/1250 [00:45<13:13,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 65/1250 [00:46<13:13,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1875, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 65/1250 [00:46<13:13,  1.49it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 66/1250 [00:47<13:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0273, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 66/1250 [00:47<13:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 67/1250 [00:47<13:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9551, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 67/1250 [00:47<13:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 68/1250 [00:48<13:10,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2988, 'learning_rate': 2e-05, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 68/1250 [00:48<13:10,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 69/1250 [00:49<13:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3701, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 69/1250 [00:49<13:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 70/1250 [00:50<13:09,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3887, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 70/1250 [00:50<13:09,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 71/1250 [00:50<13:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3086, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 71/1250 [00:50<13:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 72/1250 [00:51<13:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.415, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 72/1250 [00:51<13:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 73/1250 [00:52<13:07,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3457, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 73/1250 [00:52<13:07,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 74/1250 [00:52<13:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0186, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 74/1250 [00:52<13:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 75/1250 [00:53<13:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1406, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 75/1250 [00:53<13:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 76/1250 [00:54<13:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.207, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 76/1250 [00:54<13:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 77/1250 [00:54<13:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0127, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 77/1250 [00:54<13:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 78/1250 [00:55<13:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5361, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 78/1250 [00:55<13:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 79/1250 [00:56<13:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.415, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 79/1250 [00:56<13:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 80/1250 [00:56<13:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4307, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 80/1250 [00:56<13:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▋         | 81/1250 [00:57<13:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2754, 'learning_rate': 2e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 81/1250 [00:57<13:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 82/1250 [00:58<13:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1367, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 82/1250 [00:58<13:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 83/1250 [00:58<13:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2979, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 83/1250 [00:58<13:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 84/1250 [00:59<13:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3105, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 84/1250 [00:59<13:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 85/1250 [01:00<12:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3906, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 85/1250 [01:00<12:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 86/1250 [01:00<12:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6133, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 86/1250 [01:00<12:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 87/1250 [01:01<12:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1582, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 87/1250 [01:01<12:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 88/1250 [01:02<12:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4238, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 88/1250 [01:02<12:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 89/1250 [01:02<12:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3447, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 89/1250 [01:02<12:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 90/1250 [01:03<12:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3135, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 90/1250 [01:03<12:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 91/1250 [01:04<12:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5254, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 91/1250 [01:04<12:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 92/1250 [01:04<12:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5967, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 92/1250 [01:04<12:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 93/1250 [01:05<12:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3516, 'learning_rate': 2e-05, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m7%|▋         | 93/1250 [01:05<12:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 94/1250 [01:06<12:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5322, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 94/1250 [01:06<12:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 95/1250 [01:06<12:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0459, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 95/1250 [01:06<12:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 96/1250 [01:07<12:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4395, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 96/1250 [01:07<12:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 97/1250 [01:08<12:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3789, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 97/1250 [01:08<12:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 98/1250 [01:08<12:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2715, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 98/1250 [01:08<12:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 99/1250 [01:09<12:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.498, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 99/1250 [01:09<12:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 100/1250 [04:23<18:42:40, 58.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.333, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 100/1250 [04:23<18:42:40, 58.57s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 101/1250 [04:23<13:09:24, 41.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.7197, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 101/1250 [04:23<13:09:24, 41.22s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 102/1250 [04:24<9:15:56, 29.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4404, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 102/1250 [04:24<9:15:56, 29.06s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 103/1250 [04:25<6:32:38, 20.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4121, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 103/1250 [04:25<6:32:38, 20.54s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 104/1250 [04:25<4:38:26, 14.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2168, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 104/1250 [04:25<4:38:26, 14.58s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 105/1250 [04:26<3:18:33, 10.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4805, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 105/1250 [04:26<3:18:33, 10.41s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 106/1250 [04:27<2:22:42,  7.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2627, 'learning_rate': 2e-05, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 106/1250 [04:27<2:22:42,  7.48s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 107/1250 [04:27<1:43:37,  5.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5508, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 107/1250 [04:27<1:43:37,  5.44s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 108/1250 [04:28<1:16:17,  4.01s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1416, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 108/1250 [04:28<1:16:17,  4.01s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 109/1250 [04:29<57:10,  3.01s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4248, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▊         | 109/1250 [04:29<57:10,  3.01s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 110/1250 [04:29<43:49,  2.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6738, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 110/1250 [04:29<43:49,  2.31s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 111/1250 [04:30<34:26,  1.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4854, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 111/1250 [04:30<34:26,  1.81s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 112/1250 [04:31<27:54,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2236, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 112/1250 [04:31<27:54,  1.47s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 113/1250 [04:31<23:18,  1.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2637, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 113/1250 [04:31<23:18,  1.23s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 114/1250 [04:32<20:07,  1.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3789, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 114/1250 [04:32<20:07,  1.06s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 115/1250 [04:33<17:51,  1.06it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2666, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 115/1250 [04:33<17:51,  1.06it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 116/1250 [04:33<16:17,  1.16it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2949, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 116/1250 [04:33<16:17,  1.16it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 117/1250 [04:34<15:10,  1.24it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6025, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 117/1250 [04:34<15:10,  1.24it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 118/1250 [04:35<14:23,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3848, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 118/1250 [04:35<14:23,  1.31it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 119/1250 [04:35<13:50,  1.36it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9648, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 119/1250 [04:35<13:50,  1.36it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 120/1250 [04:36<13:28,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2051, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 120/1250 [04:36<13:28,  1.40it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 121/1250 [04:37<13:12,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 121/1250 [04:37<13:12,  1.43it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 122/1250 [04:37<13:00,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2891, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 122/1250 [04:37<13:00,  1.45it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 123/1250 [04:38<12:50,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.251, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 123/1250 [04:38<12:50,  1.46it/s]\u001b[0m\n",
      "\u001b[34m10%|▉         | 124/1250 [04:39<12:45,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5244, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|▉         | 124/1250 [04:39<12:45,  1.47it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 125/1250 [04:39<12:41,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2705, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 125/1250 [04:39<12:41,  1.48it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 126/1250 [04:40<12:37,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2275, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 126/1250 [04:40<12:37,  1.48it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 127/1250 [04:41<12:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2266, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 127/1250 [04:41<12:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 128/1250 [04:41<12:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3896, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 128/1250 [04:41<12:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 129/1250 [04:42<12:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4824, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 129/1250 [04:42<12:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 130/1250 [04:43<12:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0234, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 130/1250 [04:43<12:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m10%|█         | 131/1250 [04:43<12:28,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0459, 'learning_rate': 2e-05, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 131/1250 [04:43<12:28,  1.49it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 132/1250 [04:44<12:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4707, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 132/1250 [04:44<12:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 133/1250 [04:45<12:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2031, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 133/1250 [04:45<12:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 134/1250 [04:45<12:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2256, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 134/1250 [04:45<12:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 135/1250 [04:46<12:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4316, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 135/1250 [04:46<12:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 136/1250 [04:47<12:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4658, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 136/1250 [04:47<12:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 137/1250 [04:47<12:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6523, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 137/1250 [04:47<12:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 138/1250 [04:48<12:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4609, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 138/1250 [04:48<12:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 139/1250 [04:49<12:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2266, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 139/1250 [04:49<12:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 140/1250 [04:49<12:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4268, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█         | 140/1250 [04:49<12:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 141/1250 [04:50<12:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3193, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 141/1250 [04:50<12:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 142/1250 [04:51<12:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2832, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 142/1250 [04:51<12:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 143/1250 [04:51<12:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2383, 'learning_rate': 2e-05, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 143/1250 [04:51<12:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 144/1250 [04:52<12:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2139, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 144/1250 [04:52<12:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 145/1250 [04:53<12:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4258, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 145/1250 [04:53<12:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 146/1250 [04:53<12:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3027, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 146/1250 [04:53<12:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 147/1250 [04:54<12:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5801, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 147/1250 [04:54<12:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 148/1250 [04:55<12:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0996, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 148/1250 [04:55<12:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 149/1250 [04:55<12:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.248, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 149/1250 [04:55<12:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 150/1250 [04:56<12:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5176, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 150/1250 [04:56<12:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 151/1250 [04:57<12:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1348, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 151/1250 [04:57<12:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 152/1250 [04:57<12:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.46, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 152/1250 [04:57<12:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 153/1250 [04:58<12:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2568, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 153/1250 [04:58<12:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 154/1250 [04:59<12:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1602, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 154/1250 [04:59<12:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 155/1250 [04:59<12:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1895, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 155/1250 [04:59<12:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 156/1250 [05:00<12:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0146, 'learning_rate': 2e-05, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 156/1250 [05:00<12:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 157/1250 [05:01<12:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2451, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 157/1250 [05:01<12:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 158/1250 [05:01<12:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4805, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 158/1250 [05:01<12:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 159/1250 [05:02<12:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0107, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 159/1250 [05:02<12:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 160/1250 [05:03<12:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3594, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 160/1250 [05:03<12:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 161/1250 [05:03<12:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0996, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 161/1250 [05:03<12:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 162/1250 [05:04<12:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5508, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 162/1250 [05:04<12:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 163/1250 [05:05<12:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.498, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 163/1250 [05:05<12:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 164/1250 [05:05<12:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1504, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 164/1250 [05:05<12:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 165/1250 [05:06<12:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.127, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 165/1250 [05:06<12:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 166/1250 [05:07<12:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3691, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 166/1250 [05:07<12:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 167/1250 [05:07<12:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8867, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 167/1250 [05:07<12:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 168/1250 [05:08<12:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0127, 'learning_rate': 2e-05, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 168/1250 [05:08<12:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 169/1250 [05:09<11:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.209, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 169/1250 [05:09<11:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 170/1250 [05:09<11:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2998, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 170/1250 [05:09<11:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 171/1250 [05:10<11:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5488, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 171/1250 [05:10<11:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 172/1250 [05:11<11:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6504, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 172/1250 [05:11<11:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 173/1250 [05:11<11:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3555, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 173/1250 [05:11<11:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 174/1250 [05:12<11:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5488, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 174/1250 [05:12<11:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 175/1250 [05:13<11:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3672, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 175/1250 [05:13<11:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 176/1250 [05:13<11:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5215, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 176/1250 [05:13<11:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 177/1250 [05:14<11:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1709, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 177/1250 [05:14<11:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 178/1250 [05:15<11:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2578, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 178/1250 [05:15<11:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 179/1250 [05:15<11:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2812, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 179/1250 [05:15<11:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 180/1250 [05:16<11:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1035, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 180/1250 [05:16<11:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 181/1250 [05:17<11:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4668, 'learning_rate': 2e-05, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 181/1250 [05:17<11:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 182/1250 [05:17<11:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1631, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 182/1250 [05:17<11:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 183/1250 [05:18<11:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1201, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 183/1250 [05:18<11:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 184/1250 [05:19<11:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.417, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 184/1250 [05:19<11:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 185/1250 [05:19<11:48,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2012, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 185/1250 [05:19<11:48,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 186/1250 [05:20<11:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2256, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 186/1250 [05:20<11:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 187/1250 [05:21<11:46,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3096, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 187/1250 [05:21<11:46,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 188/1250 [05:21<11:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2744, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 188/1250 [05:21<11:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 189/1250 [05:22<11:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4033, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 189/1250 [05:22<11:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 190/1250 [05:23<11:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2793, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 190/1250 [05:23<11:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 191/1250 [05:23<11:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4355, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 191/1250 [05:23<11:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 192/1250 [05:24<11:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4385, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 192/1250 [05:24<11:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 193/1250 [05:25<11:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1816, 'learning_rate': 2e-05, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 193/1250 [05:25<11:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 194/1250 [05:25<11:41,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1719, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 194/1250 [05:25<11:41,  1.51it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 195/1250 [05:26<11:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0059, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 195/1250 [05:26<11:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 196/1250 [05:27<11:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2891, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 196/1250 [05:27<11:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 197/1250 [05:27<11:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.707, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 197/1250 [05:27<11:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 198/1250 [05:28<11:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2715, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 198/1250 [05:28<11:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 199/1250 [05:29<11:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1758, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 199/1250 [05:29<11:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 200/1250 [08:43<17:10:13, 58.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3877, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 200/1250 [08:43<17:10:13, 58.87s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 201/1250 [08:44<12:04:05, 41.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1982, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 201/1250 [08:44<12:04:05, 41.42s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 202/1250 [08:45<8:29:55, 29.19s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4668, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 202/1250 [08:45<8:29:55, 29.19s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 203/1250 [08:45<6:00:07, 20.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0879, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 203/1250 [08:45<6:00:07, 20.64s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 204/1250 [08:46<4:15:22, 14.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.373, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 204/1250 [08:46<4:15:22, 14.65s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 205/1250 [08:47<3:02:07, 10.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 205/1250 [08:47<3:02:07, 10.46s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 206/1250 [08:47<2:10:53,  7.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1963, 'learning_rate': 2e-05, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m16%|█▋        | 206/1250 [08:47<2:10:53,  7.52s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 207/1250 [08:48<1:35:03,  5.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.373, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 207/1250 [08:48<1:35:03,  5.47s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 208/1250 [08:49<1:09:59,  4.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4746, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 208/1250 [08:49<1:09:59,  4.03s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 209/1250 [08:49<52:27,  3.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.542, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 209/1250 [08:49<52:27,  3.02s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 210/1250 [08:50<40:11,  2.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1924, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 210/1250 [08:50<40:11,  2.32s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 211/1250 [08:51<31:36,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2812, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 211/1250 [08:51<31:36,  1.83s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 212/1250 [08:52<25:35,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2549, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 212/1250 [08:52<25:35,  1.48s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 213/1250 [08:52<21:23,  1.24s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3926, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 213/1250 [08:52<21:23,  1.24s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 214/1250 [08:53<18:28,  1.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9502, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 214/1250 [08:53<18:28,  1.07s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 215/1250 [08:54<16:24,  1.05it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4688, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 215/1250 [08:54<16:24,  1.05it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 216/1250 [08:54<14:56,  1.15it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4072, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 216/1250 [08:54<14:56,  1.15it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 217/1250 [08:55<13:56,  1.23it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.501, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 217/1250 [08:55<13:56,  1.23it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 218/1250 [08:56<13:13,  1.30it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3154, 'learning_rate': 2e-05, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 218/1250 [08:56<13:13,  1.30it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 219/1250 [08:56<12:43,  1.35it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.085, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 219/1250 [08:56<12:43,  1.35it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 220/1250 [08:57<12:22,  1.39it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4004, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 220/1250 [08:57<12:22,  1.39it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 221/1250 [08:58<12:07,  1.41it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4893, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 221/1250 [08:58<12:07,  1.41it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 222/1250 [08:58<11:56,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1953, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 222/1250 [08:58<11:56,  1.44it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 223/1250 [08:59<11:48,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1104, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 223/1250 [08:59<11:48,  1.45it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 224/1250 [09:00<11:44,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.249, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 224/1250 [09:00<11:44,  1.46it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 225/1250 [09:00<11:39,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2637, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 225/1250 [09:00<11:39,  1.47it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 226/1250 [09:01<11:36,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1582, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 226/1250 [09:01<11:36,  1.47it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 227/1250 [09:02<11:34,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4863, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 227/1250 [09:02<11:34,  1.47it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 228/1250 [09:02<11:31,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3545, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 228/1250 [09:02<11:31,  1.48it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 229/1250 [09:03<11:30,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4033, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 229/1250 [09:03<11:30,  1.48it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 230/1250 [09:04<11:29,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3311, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 230/1250 [09:04<11:29,  1.48it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 231/1250 [09:04<11:27,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3965, 'learning_rate': 2e-05, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 231/1250 [09:04<11:27,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 232/1250 [09:05<11:27,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2285, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 232/1250 [09:05<11:27,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 233/1250 [09:06<11:26,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5625, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 233/1250 [09:06<11:26,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 234/1250 [09:06<11:25,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4336, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 234/1250 [09:06<11:25,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 235/1250 [09:07<11:25,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2959, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 235/1250 [09:07<11:25,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 236/1250 [09:08<11:24,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0986, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 236/1250 [09:08<11:24,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 237/1250 [09:08<11:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2842, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 237/1250 [09:08<11:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 238/1250 [09:09<11:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3896, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 238/1250 [09:09<11:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 239/1250 [09:10<11:22,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9902, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 239/1250 [09:10<11:22,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 240/1250 [09:10<11:21,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2656, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 240/1250 [09:10<11:21,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 241/1250 [09:11<11:21,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2451, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 241/1250 [09:11<11:21,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 242/1250 [09:12<11:20,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0869, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 242/1250 [09:12<11:20,  1.48it/s]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 243/1250 [09:12<11:19,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6201, 'learning_rate': 2e-05, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 243/1250 [09:12<11:19,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 244/1250 [09:13<11:19,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3311, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 244/1250 [09:13<11:19,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 245/1250 [09:14<11:18,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.875, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 245/1250 [09:14<11:18,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 246/1250 [09:14<11:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0605, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 246/1250 [09:14<11:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 247/1250 [09:15<11:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1094, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 247/1250 [09:15<11:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 248/1250 [09:16<11:16,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0977, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 248/1250 [09:16<11:16,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 249/1250 [09:16<11:15,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0596, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 249/1250 [09:16<11:15,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 250/1250 [09:17<11:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.8047, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 250/1250 [09:17<11:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 251/1250 [09:18<11:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4795, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 251/1250 [09:18<11:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 252/1250 [09:19<11:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1406, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 252/1250 [09:19<11:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 253/1250 [09:19<11:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4941, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 253/1250 [09:19<11:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 254/1250 [09:20<11:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2627, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 254/1250 [09:20<11:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 255/1250 [09:21<11:11,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0845, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 255/1250 [09:21<11:11,  1.48it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 256/1250 [09:21<11:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0762, 'learning_rate': 2e-05, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 256/1250 [09:21<11:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 257/1250 [09:22<11:11,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.291, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 257/1250 [09:22<11:11,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 258/1250 [09:23<11:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2031, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 258/1250 [09:23<11:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 259/1250 [09:23<11:09,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3574, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 259/1250 [09:23<11:09,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 260/1250 [09:24<11:09,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2793, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 260/1250 [09:24<11:09,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 261/1250 [09:25<11:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2646, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 261/1250 [09:25<11:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 262/1250 [09:25<11:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0049, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 262/1250 [09:25<11:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 263/1250 [09:26<11:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6426, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 263/1250 [09:26<11:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 264/1250 [09:27<11:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1963, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 264/1250 [09:27<11:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m21%|██        | 265/1250 [09:27<11:09,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3398, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 265/1250 [09:27<11:09,  1.47it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 266/1250 [09:28<11:09,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5791, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 266/1250 [09:28<11:09,  1.47it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 267/1250 [09:29<11:07,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9961, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 267/1250 [09:29<11:07,  1.47it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 268/1250 [09:29<11:04,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1777, 'learning_rate': 2e-05, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██▏       | 268/1250 [09:29<11:04,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 269/1250 [09:30<11:03,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6377, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 269/1250 [09:30<11:03,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 270/1250 [09:31<11:02,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5488, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 270/1250 [09:31<11:02,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 271/1250 [09:31<11:01,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2461, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 271/1250 [09:31<11:01,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 272/1250 [09:32<11:00,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2812, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 272/1250 [09:32<11:00,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 273/1250 [09:33<11:00,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0205, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 273/1250 [09:33<11:00,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 274/1250 [09:33<10:58,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2305, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 274/1250 [09:33<10:58,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 275/1250 [09:34<10:58,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5186, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 275/1250 [09:34<10:58,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 276/1250 [09:35<10:58,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2031, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 276/1250 [09:35<10:58,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 277/1250 [09:35<10:56,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0811, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 277/1250 [09:35<10:56,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 278/1250 [09:36<10:56,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1611, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 278/1250 [09:36<10:56,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 279/1250 [09:37<10:55,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1387, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 279/1250 [09:37<10:55,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 280/1250 [09:37<10:54,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2607, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 280/1250 [09:37<10:54,  1.48it/s]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 281/1250 [09:38<10:54,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2119, 'learning_rate': 2e-05, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 281/1250 [09:38<10:54,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 282/1250 [09:39<10:53,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2871, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 282/1250 [09:39<10:53,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 283/1250 [09:39<10:52,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1328, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 283/1250 [09:39<10:52,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 284/1250 [09:40<10:52,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0166, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 284/1250 [09:40<10:52,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 285/1250 [09:41<10:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2002, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 285/1250 [09:41<10:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 286/1250 [09:41<10:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2412, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 286/1250 [09:41<10:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m23%|██▎       | 287/1250 [09:42<10:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0947, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 287/1250 [09:42<10:51,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 288/1250 [09:43<10:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4678, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 288/1250 [09:43<10:50,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 289/1250 [09:44<10:48,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2578, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 289/1250 [09:44<10:48,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 290/1250 [09:44<10:48,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2363, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 290/1250 [09:44<10:48,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 291/1250 [09:45<10:47,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4775, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 291/1250 [09:45<10:47,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 292/1250 [09:46<10:46,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3506, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 292/1250 [09:46<10:46,  1.48it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 293/1250 [09:46<10:46,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2646, 'learning_rate': 2e-05, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 293/1250 [09:46<10:46,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 294/1250 [09:47<10:46,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0537, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 294/1250 [09:47<10:46,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 295/1250 [09:48<10:44,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0332, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 295/1250 [09:48<10:44,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 296/1250 [09:48<10:44,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1465, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 296/1250 [09:48<10:44,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 297/1250 [09:49<10:44,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3008, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 297/1250 [09:49<10:44,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 298/1250 [09:50<10:42,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1035, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 298/1250 [09:50<10:42,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 299/1250 [09:50<10:42,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9294, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 299/1250 [09:50<10:42,  1.48it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 300/1250 [13:05<15:34:39, 59.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2559, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 300/1250 [13:05<15:34:39, 59.03s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 301/1250 [13:06<10:56:50, 41.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3213, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 301/1250 [13:06<10:56:50, 41.53s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 302/1250 [13:07<7:42:33, 29.28s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.209, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 302/1250 [13:07<7:42:33, 29.28s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 303/1250 [13:08<5:26:38, 20.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0889, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 303/1250 [13:08<5:26:38, 20.70s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 304/1250 [13:08<3:51:37, 14.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2783, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 304/1250 [13:08<3:51:37, 14.69s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 305/1250 [13:09<2:45:10, 10.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1475, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 305/1250 [13:09<2:45:10, 10.49s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 306/1250 [13:10<1:58:40,  7.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3223, 'learning_rate': 2e-05, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 306/1250 [13:10<1:58:40,  7.54s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 307/1250 [13:10<1:26:10,  5.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2559, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 307/1250 [13:10<1:26:10,  5.48s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 308/1250 [13:11<1:03:27,  4.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 308/1250 [13:11<1:03:27,  4.04s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 309/1250 [13:12<47:33,  3.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3506, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 309/1250 [13:12<47:33,  3.03s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 310/1250 [13:12<36:26,  2.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3271, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 310/1250 [13:12<36:26,  2.33s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 311/1250 [13:13<28:40,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2197, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 311/1250 [13:13<28:40,  1.83s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 312/1250 [13:14<23:12,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0527, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 312/1250 [13:14<23:12,  1.48s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 313/1250 [13:14<19:23,  1.24s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1914, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 313/1250 [13:14<19:23,  1.24s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 314/1250 [13:15<16:43,  1.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4297, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 314/1250 [13:15<16:43,  1.07s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 315/1250 [13:16<14:51,  1.05it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0928, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 315/1250 [13:16<14:51,  1.05it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 316/1250 [13:16<13:32,  1.15it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3379, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 316/1250 [13:16<13:32,  1.15it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 317/1250 [13:17<12:37,  1.23it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2744, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 317/1250 [13:17<12:37,  1.23it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 318/1250 [13:18<11:58,  1.30it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1084, 'learning_rate': 2e-05, 'epoch': 0.25}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 318/1250 [13:18<11:58,  1.30it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 319/1250 [13:18<11:30,  1.35it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3076, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 319/1250 [13:18<11:30,  1.35it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 320/1250 [13:19<11:12,  1.38it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2139, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 320/1250 [13:19<11:12,  1.38it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 321/1250 [13:20<10:58,  1.41it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.543, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 321/1250 [13:20<10:58,  1.41it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 322/1250 [13:20<10:47,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4141, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 322/1250 [13:20<10:47,  1.43it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 323/1250 [13:21<10:41,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9395, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 323/1250 [13:21<10:41,  1.44it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 324/1250 [13:22<10:36,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.333, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 324/1250 [13:22<10:36,  1.46it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 325/1250 [13:22<10:32,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1699, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 325/1250 [13:22<10:32,  1.46it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 326/1250 [13:23<10:29,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3301, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 326/1250 [13:23<10:29,  1.47it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 327/1250 [13:24<10:27,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4648, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 327/1250 [13:24<10:27,  1.47it/s]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 328/1250 [13:24<10:25,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2529, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 328/1250 [13:24<10:25,  1.48it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 329/1250 [13:25<10:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5488, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 329/1250 [13:25<10:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 330/1250 [13:26<10:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5518, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 330/1250 [13:26<10:23,  1.48it/s]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 331/1250 [13:26<10:23,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1016, 'learning_rate': 2e-05, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 331/1250 [13:26<10:23,  1.47it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 332/1250 [13:27<10:23,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3184, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 332/1250 [13:27<10:23,  1.47it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 333/1250 [13:28<10:22,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2236, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 333/1250 [13:28<10:22,  1.47it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 334/1250 [13:28<10:20,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2236, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 334/1250 [13:28<10:20,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 335/1250 [13:29<10:19,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2178, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 335/1250 [13:29<10:19,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 336/1250 [13:30<10:18,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3535, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 336/1250 [13:30<10:18,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 337/1250 [13:31<10:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1943, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 337/1250 [13:31<10:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 338/1250 [13:31<10:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1768, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 338/1250 [13:31<10:17,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 339/1250 [13:32<10:15,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1064, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 339/1250 [13:32<10:15,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 340/1250 [13:33<10:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.873, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 340/1250 [13:33<10:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 341/1250 [13:33<10:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3115, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 341/1250 [13:33<10:14,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 342/1250 [13:34<10:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.085, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 342/1250 [13:34<10:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 343/1250 [13:35<10:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2744, 'learning_rate': 2e-05, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 343/1250 [13:35<10:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 344/1250 [13:35<10:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1973, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 344/1250 [13:35<10:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 345/1250 [13:36<10:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0078, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 345/1250 [13:36<10:12,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 346/1250 [13:37<10:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4082, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 346/1250 [13:37<10:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 347/1250 [13:37<10:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 347/1250 [13:37<10:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m28%|██▊       | 348/1250 [13:38<10:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1113, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 348/1250 [13:38<10:10,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 349/1250 [13:39<10:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5391, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 349/1250 [13:39<10:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 350/1250 [13:39<10:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0205, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 350/1250 [13:39<10:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 351/1250 [13:40<10:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4883, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 351/1250 [13:40<10:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 352/1250 [13:41<10:06,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4424, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 352/1250 [13:41<10:06,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 353/1250 [13:41<10:06,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2129, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 353/1250 [13:41<10:06,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 354/1250 [13:42<10:06,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1797, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 354/1250 [13:42<10:06,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 355/1250 [13:43<10:05,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1465, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 355/1250 [13:43<10:05,  1.48it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 356/1250 [13:43<10:03,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4814, 'learning_rate': 2e-05, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 356/1250 [13:43<10:03,  1.48it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 357/1250 [13:44<10:03,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2598, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 357/1250 [13:44<10:03,  1.48it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 358/1250 [13:45<10:01,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4727, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 358/1250 [13:45<10:01,  1.48it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 359/1250 [13:45<09:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2188, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 359/1250 [13:45<09:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 360/1250 [13:46<09:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2559, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 360/1250 [13:46<09:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 361/1250 [13:47<09:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1924, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 361/1250 [13:47<09:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 362/1250 [13:47<09:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1494, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 362/1250 [13:47<09:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 363/1250 [13:48<09:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0264, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 363/1250 [13:48<09:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 364/1250 [13:49<09:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1445, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 364/1250 [13:49<09:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 365/1250 [13:49<09:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 365/1250 [13:49<09:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 366/1250 [13:50<09:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2344, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 366/1250 [13:50<09:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 367/1250 [13:51<09:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2578, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 367/1250 [13:51<09:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 368/1250 [13:51<09:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1846, 'learning_rate': 2e-05, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 368/1250 [13:51<09:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 369/1250 [13:52<09:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5068, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 369/1250 [13:52<09:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 370/1250 [13:53<09:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2207, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 370/1250 [13:53<09:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 371/1250 [13:53<09:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3574, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 371/1250 [13:53<09:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 372/1250 [13:54<09:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2734, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 372/1250 [13:54<09:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 373/1250 [13:55<09:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0186, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 373/1250 [13:55<09:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 374/1250 [13:55<09:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3799, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 374/1250 [13:55<09:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 375/1250 [13:56<09:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.001, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 375/1250 [13:56<09:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 376/1250 [13:57<09:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0952, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 376/1250 [13:57<09:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 377/1250 [13:57<09:46,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1777, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 377/1250 [13:57<09:46,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 378/1250 [13:58<09:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2422, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 378/1250 [13:58<09:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 379/1250 [13:59<09:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2021, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 379/1250 [13:59<09:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 380/1250 [13:59<09:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3223, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 380/1250 [13:59<09:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m30%|███       | 381/1250 [14:00<09:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2686, 'learning_rate': 2e-05, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|███       | 381/1250 [14:00<09:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 382/1250 [14:01<09:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1548, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 382/1250 [14:01<09:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 383/1250 [14:01<09:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0371, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 383/1250 [14:01<09:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 384/1250 [14:02<09:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2734, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 384/1250 [14:02<09:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 385/1250 [14:03<09:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4297, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 385/1250 [14:03<09:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 386/1250 [14:04<09:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4863, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 386/1250 [14:04<09:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 387/1250 [14:04<09:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.293, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 387/1250 [14:04<09:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 388/1250 [14:05<09:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4688, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 388/1250 [14:05<09:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 389/1250 [14:06<09:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3564, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 389/1250 [14:06<09:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 390/1250 [14:06<09:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2812, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 390/1250 [14:06<09:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 391/1250 [14:07<09:36,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4346, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 391/1250 [14:07<09:36,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 392/1250 [14:08<09:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.332, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 392/1250 [14:08<09:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 393/1250 [14:08<09:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4297, 'learning_rate': 2e-05, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 393/1250 [14:08<09:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 394/1250 [14:09<09:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3643, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 394/1250 [14:09<09:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 395/1250 [14:10<09:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9238, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 395/1250 [14:10<09:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 396/1250 [14:10<09:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2998, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 396/1250 [14:10<09:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 397/1250 [14:11<09:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1816, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 397/1250 [14:11<09:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 398/1250 [14:12<09:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2451, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 398/1250 [14:12<09:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 399/1250 [14:12<09:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3672, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 399/1250 [14:12<09:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 400/1250 [17:33<14:18:59, 60.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2402, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 400/1250 [17:33<14:18:59, 60.63s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 401/1250 [17:33<10:03:36, 42.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2021, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 401/1250 [17:33<10:03:36, 42.66s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 402/1250 [17:34<7:04:56, 30.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2383, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 402/1250 [17:34<7:04:56, 30.07s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 403/1250 [17:35<4:59:56, 21.25s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1182, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 403/1250 [17:35<4:59:56, 21.25s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 404/1250 [17:36<3:32:33, 15.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5459, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 404/1250 [17:36<3:32:33, 15.07s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 405/1250 [17:36<2:31:27, 10.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1836, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 405/1250 [17:36<2:31:27, 10.75s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 406/1250 [17:37<1:48:43,  7.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2188, 'learning_rate': 2e-05, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 406/1250 [17:37<1:48:43,  7.73s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 407/1250 [17:38<1:18:50,  5.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2676, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 407/1250 [17:38<1:18:50,  5.61s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 408/1250 [17:38<57:56,  4.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 408/1250 [17:38<57:56,  4.13s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 409/1250 [17:39<43:20,  3.09s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5664, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 409/1250 [17:39<43:20,  3.09s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 410/1250 [17:40<33:06,  2.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.624, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 410/1250 [17:40<33:06,  2.37s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 411/1250 [17:40<25:58,  1.86s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9219, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 411/1250 [17:40<25:58,  1.86s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 412/1250 [17:41<20:58,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1592, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 412/1250 [17:41<20:58,  1.50s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 413/1250 [17:42<17:28,  1.25s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3564, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 413/1250 [17:42<17:28,  1.25s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 414/1250 [17:42<15:01,  1.08s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 414/1250 [17:42<15:01,  1.08s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 415/1250 [17:43<13:18,  1.05it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1699, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 415/1250 [17:43<13:18,  1.05it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 416/1250 [17:44<12:06,  1.15it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1846, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 416/1250 [17:44<12:06,  1.15it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 417/1250 [17:44<11:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4609, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 417/1250 [17:44<11:15,  1.23it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 418/1250 [17:45<10:39,  1.30it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1328, 'learning_rate': 2e-05, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 418/1250 [17:45<10:39,  1.30it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 419/1250 [17:46<10:14,  1.35it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.332, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 419/1250 [17:46<10:14,  1.35it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 420/1250 [17:46<09:57,  1.39it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3018, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 420/1250 [17:46<09:57,  1.39it/s]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 421/1250 [17:47<09:44,  1.42it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2217, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 421/1250 [17:47<09:44,  1.42it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 422/1250 [17:48<09:35,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5879, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 422/1250 [17:48<09:35,  1.44it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 423/1250 [17:48<09:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.7207, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 423/1250 [17:48<09:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 424/1250 [17:49<09:24,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1348, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 424/1250 [17:49<09:24,  1.46it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 425/1250 [17:50<09:20,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5439, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 425/1250 [17:50<09:20,  1.47it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 426/1250 [17:50<09:18,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3779, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 426/1250 [17:50<09:18,  1.48it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 427/1250 [17:51<09:15,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4102, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 427/1250 [17:51<09:15,  1.48it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 428/1250 [17:52<09:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4082, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 428/1250 [17:52<09:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 429/1250 [17:52<09:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4287, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 429/1250 [17:52<09:13,  1.48it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 430/1250 [17:53<09:11,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2598, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 430/1250 [17:53<09:11,  1.49it/s]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 431/1250 [17:54<09:10,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1543, 'learning_rate': 2e-05, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 431/1250 [17:54<09:10,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 432/1250 [17:54<09:09,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3223, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 432/1250 [17:54<09:09,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 433/1250 [17:55<09:09,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4697, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 433/1250 [17:55<09:09,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 434/1250 [17:56<09:08,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4893, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 434/1250 [17:56<09:08,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 435/1250 [17:56<09:07,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1885, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 435/1250 [17:56<09:07,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 436/1250 [17:57<09:06,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.207, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 436/1250 [17:57<09:06,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 437/1250 [17:58<09:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3271, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 437/1250 [17:58<09:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 438/1250 [17:58<09:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1963, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 438/1250 [17:58<09:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 439/1250 [17:59<09:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2129, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 439/1250 [17:59<09:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 440/1250 [18:00<09:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5342, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 440/1250 [18:00<09:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 441/1250 [18:00<09:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4189, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 441/1250 [18:00<09:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 442/1250 [18:01<09:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3965, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 442/1250 [18:01<09:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 443/1250 [18:02<09:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4268, 'learning_rate': 2e-05, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 443/1250 [18:02<09:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 444/1250 [18:02<09:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1787, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 444/1250 [18:02<09:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 445/1250 [18:03<09:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0215, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 445/1250 [18:03<09:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 446/1250 [18:04<08:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9404, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 446/1250 [18:04<08:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 447/1250 [18:04<08:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3447, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 447/1250 [18:04<08:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 448/1250 [18:05<08:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1836, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 448/1250 [18:05<08:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 449/1250 [18:06<08:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0918, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 449/1250 [18:06<08:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 450/1250 [18:06<08:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0166, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 450/1250 [18:06<08:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 451/1250 [18:07<08:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0459, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 451/1250 [18:07<08:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 452/1250 [18:08<08:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2822, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 452/1250 [18:08<08:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 453/1250 [18:08<08:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 453/1250 [18:08<08:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 454/1250 [18:09<08:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.085, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 454/1250 [18:09<08:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 455/1250 [18:10<08:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2842, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 455/1250 [18:10<08:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 456/1250 [18:10<08:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5488, 'learning_rate': 2e-05, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▋      | 456/1250 [18:10<08:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 457/1250 [18:11<08:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2695, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 457/1250 [18:11<08:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 458/1250 [18:12<08:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2236, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 458/1250 [18:12<08:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 459/1250 [18:12<08:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3945, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 459/1250 [18:12<08:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 460/1250 [18:13<08:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9521, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 460/1250 [18:13<08:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 461/1250 [18:14<08:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1348, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 461/1250 [18:14<08:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 462/1250 [18:14<08:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6357, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 462/1250 [18:14<08:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 463/1250 [18:15<08:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5459, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 463/1250 [18:15<08:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 464/1250 [18:16<08:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4141, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 464/1250 [18:16<08:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 465/1250 [18:16<08:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1914, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 465/1250 [18:16<08:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 466/1250 [18:17<08:46,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1963, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 466/1250 [18:17<08:46,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 467/1250 [18:18<08:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3662, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 467/1250 [18:18<08:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 468/1250 [18:18<08:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2305, 'learning_rate': 2e-05, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 468/1250 [18:18<08:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 469/1250 [18:19<08:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6348, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 469/1250 [18:19<08:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 470/1250 [18:20<08:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1309, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 470/1250 [18:20<08:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 471/1250 [18:21<08:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4668, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 471/1250 [18:21<08:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 472/1250 [18:21<08:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3115, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 472/1250 [18:21<08:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 473/1250 [18:22<08:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1484, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 473/1250 [18:22<08:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 474/1250 [18:23<08:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3281, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 474/1250 [18:23<08:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 475/1250 [18:23<08:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4424, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 475/1250 [18:23<08:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 476/1250 [18:24<08:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3076, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 476/1250 [18:24<08:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 477/1250 [18:25<08:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2227, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 477/1250 [18:25<08:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 478/1250 [18:25<08:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2861, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 478/1250 [18:25<08:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 479/1250 [18:26<08:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4824, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 479/1250 [18:26<08:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 480/1250 [18:27<08:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2334, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 480/1250 [18:27<08:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 481/1250 [18:27<08:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5674, 'learning_rate': 2e-05, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 481/1250 [18:27<08:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 482/1250 [18:28<08:36,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0586, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 482/1250 [18:28<08:36,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 483/1250 [18:29<08:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3467, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 483/1250 [18:29<08:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 484/1250 [18:29<08:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3115, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 484/1250 [18:29<08:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 485/1250 [18:30<08:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5098, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 485/1250 [18:30<08:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 486/1250 [18:31<08:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2236, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 486/1250 [18:31<08:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 487/1250 [18:31<08:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2256, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 487/1250 [18:31<08:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 488/1250 [18:32<08:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2061, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 488/1250 [18:32<08:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 489/1250 [18:33<08:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1816, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 489/1250 [18:33<08:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 490/1250 [18:33<08:28,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2031, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 490/1250 [18:33<08:28,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 491/1250 [18:34<08:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3145, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 491/1250 [18:34<08:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 492/1250 [18:35<08:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4453, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 492/1250 [18:35<08:27,  1.49it/s]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 493/1250 [18:35<08:26,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1035, 'learning_rate': 2e-05, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 493/1250 [18:35<08:26,  1.49it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 494/1250 [18:36<08:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9307, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 494/1250 [18:36<08:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 495/1250 [18:37<08:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0859, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 495/1250 [18:37<08:25,  1.49it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 496/1250 [18:37<08:24,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1973, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 496/1250 [18:37<08:24,  1.49it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 497/1250 [18:38<08:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4453, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 497/1250 [18:38<08:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 498/1250 [18:39<08:23,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3047, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 498/1250 [18:39<08:23,  1.49it/s]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 499/1250 [18:39<08:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3809, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 499/1250 [18:39<08:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 500/1250 [21:56<12:21:42, 59.34s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3135, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 500/1250 [21:56<12:21:42, 59.34s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 501/1250 [21:56<8:41:06, 41.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1357, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 501/1250 [21:56<8:41:06, 41.74s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 502/1250 [21:57<6:06:47, 29.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5547, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 502/1250 [21:57<6:06:47, 29.42s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 503/1250 [21:58<4:18:54, 20.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4648, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 503/1250 [21:58<4:18:54, 20.80s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 504/1250 [21:58<3:03:29, 14.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2871, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 504/1250 [21:58<3:03:29, 14.76s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 505/1250 [21:59<2:10:45, 10.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2246, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 505/1250 [21:59<2:10:45, 10.53s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 506/1250 [22:00<1:33:54,  7.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1465, 'learning_rate': 2e-05, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|████      | 506/1250 [22:00<1:33:54,  7.57s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 507/1250 [22:00<1:08:07,  5.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3867, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 507/1250 [22:00<1:08:07,  5.50s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 508/1250 [22:01<50:06,  4.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2178, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 508/1250 [22:01<50:06,  4.05s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 509/1250 [22:02<37:30,  3.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2969, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 509/1250 [22:02<37:30,  3.04s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 510/1250 [22:02<28:42,  2.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8105, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 510/1250 [22:02<28:42,  2.33s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 511/1250 [22:03<22:32,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.085, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 511/1250 [22:03<22:32,  1.83s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 512/1250 [22:04<18:13,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2568, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 512/1250 [22:04<18:13,  1.48s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 513/1250 [22:04<15:12,  1.24s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9521, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 513/1250 [22:04<15:12,  1.24s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 514/1250 [22:05<13:05,  1.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4062, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 514/1250 [22:05<13:05,  1.07s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 515/1250 [22:06<11:36,  1.06it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1943, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 515/1250 [22:06<11:36,  1.06it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 516/1250 [22:06<10:34,  1.16it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1357, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 516/1250 [22:06<10:34,  1.16it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 517/1250 [22:07<09:50,  1.24it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4355, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 517/1250 [22:07<09:50,  1.24it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 518/1250 [22:08<09:20,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2412, 'learning_rate': 2e-05, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 518/1250 [22:08<09:20,  1.31it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 519/1250 [22:08<08:58,  1.36it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2139, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 519/1250 [22:08<08:58,  1.36it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 520/1250 [22:09<08:43,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1445, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 520/1250 [22:09<08:43,  1.40it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 521/1250 [22:10<08:32,  1.42it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3555, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 521/1250 [22:10<08:32,  1.42it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 522/1250 [22:10<08:24,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7822, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 522/1250 [22:10<08:24,  1.44it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 523/1250 [22:11<08:18,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.248, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 523/1250 [22:11<08:18,  1.46it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 524/1250 [22:12<08:14,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0449, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 524/1250 [22:12<08:14,  1.47it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 525/1250 [22:12<08:11,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1997, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 525/1250 [22:12<08:11,  1.48it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 526/1250 [22:13<08:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4307, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 526/1250 [22:13<08:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 527/1250 [22:14<08:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0889, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 527/1250 [22:14<08:07,  1.48it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 528/1250 [22:14<08:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1455, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 528/1250 [22:14<08:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 529/1250 [22:15<08:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0312, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 529/1250 [22:15<08:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 530/1250 [22:16<08:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4229, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 530/1250 [22:16<08:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 531/1250 [22:16<08:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0664, 'learning_rate': 2e-05, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 531/1250 [22:16<08:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 532/1250 [22:17<08:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1689, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 532/1250 [22:17<08:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 533/1250 [22:18<08:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0615, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 533/1250 [22:18<08:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 534/1250 [22:18<08:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3701, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 534/1250 [22:18<08:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 535/1250 [22:19<07:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3672, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 535/1250 [22:19<07:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 536/1250 [22:20<07:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0752, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 536/1250 [22:20<07:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 537/1250 [22:20<07:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4727, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 537/1250 [22:20<07:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 538/1250 [22:21<07:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.332, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 538/1250 [22:21<07:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 539/1250 [22:22<07:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2588, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 539/1250 [22:22<07:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 540/1250 [22:22<07:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2197, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 540/1250 [22:22<07:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 541/1250 [22:23<07:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9385, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 541/1250 [22:23<07:55,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 542/1250 [22:24<07:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4541, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 542/1250 [22:24<07:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 543/1250 [22:24<07:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.916, 'learning_rate': 2e-05, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 543/1250 [22:24<07:54,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 544/1250 [22:25<07:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0254, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 544/1250 [22:25<07:53,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 545/1250 [22:26<07:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1924, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 545/1250 [22:26<07:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 546/1250 [22:26<07:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2881, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 546/1250 [22:26<07:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 547/1250 [22:27<07:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2158, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 547/1250 [22:27<07:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 548/1250 [22:28<07:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4062, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 548/1250 [22:28<07:52,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 549/1250 [22:28<07:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4775, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 549/1250 [22:28<07:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 550/1250 [22:29<07:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1719, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 550/1250 [22:29<07:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 551/1250 [22:30<07:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1523, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 551/1250 [22:30<07:49,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 552/1250 [22:30<07:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2559, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 552/1250 [22:30<07:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 553/1250 [22:31<07:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7949, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 553/1250 [22:31<07:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 554/1250 [22:32<07:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1494, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 554/1250 [22:32<07:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 555/1250 [22:32<07:46,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1514, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 555/1250 [22:32<07:46,  1.49it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 556/1250 [22:33<07:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3877, 'learning_rate': 2e-05, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 556/1250 [22:33<07:45,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 557/1250 [22:34<07:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0293, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 557/1250 [22:34<07:44,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 558/1250 [22:34<07:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1025, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 558/1250 [22:34<07:43,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 559/1250 [22:35<07:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3115, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 559/1250 [22:35<07:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 560/1250 [22:36<07:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2178, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 560/1250 [22:36<07:42,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 561/1250 [22:36<07:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1406, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 561/1250 [22:36<07:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 562/1250 [22:37<07:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3135, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 562/1250 [22:37<07:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 563/1250 [22:38<07:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3135, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 563/1250 [22:38<07:40,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 564/1250 [22:38<07:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2544, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 564/1250 [22:38<07:39,  1.49it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 565/1250 [22:39<07:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3877, 'learning_rate': 2e-05, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 565/1250 [22:39<07:38,  1.49it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 600/1250 [26:17<10:40:37, 59.14s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1729, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 600/1250 [26:17<10:40:37, 59.14s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 601/1250 [26:18<7:29:59, 41.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4482, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 601/1250 [26:18<7:29:59, 41.60s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 602/1250 [26:19<5:16:40, 29.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0635, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 602/1250 [26:19<5:16:40, 29.32s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 603/1250 [26:19<3:43:28, 20.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1309, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 603/1250 [26:19<3:43:28, 20.72s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 604/1250 [26:20<2:38:20, 14.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.498, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 604/1250 [26:20<2:38:20, 14.71s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 605/1250 [26:21<1:52:48, 10.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2314, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 605/1250 [26:21<1:52:48, 10.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 606/1250 [26:21<1:20:58,  7.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1729, 'learning_rate': 2e-05, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 606/1250 [26:21<1:20:58,  7.54s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 607/1250 [26:22<58:44,  5.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2539, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 607/1250 [26:22<58:44,  5.48s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 608/1250 [26:23<43:10,  4.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6909, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 608/1250 [26:23<43:10,  4.04s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 609/1250 [26:23<32:18,  3.02s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.04, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 609/1250 [26:23<32:18,  3.02s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 610/1250 [26:24<24:42,  2.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1982, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 610/1250 [26:24<24:42,  2.32s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 611/1250 [26:25<19:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9448, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 611/1250 [26:25<19:23,  1.82s/it]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m49%|████▉     | 612/1250 [26:25<15:40,  1.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1152, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 612/1250 [26:25<15:40,  1.47s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 613/1250 [26:26<13:04,  1.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1836, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 613/1250 [26:26<13:04,  1.23s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 614/1250 [26:27<11:15,  1.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2656, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 614/1250 [26:27<11:15,  1.06s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 615/1250 [26:27<09:59,  1.06it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4893, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 615/1250 [26:27<09:59,  1.06it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 616/1250 [26:28<09:06,  1.16it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.207, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 616/1250 [26:28<09:06,  1.16it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 617/1250 [26:29<08:27,  1.25it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3574, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 617/1250 [26:29<08:27,  1.25it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 618/1250 [26:29<08:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1445, 'learning_rate': 2e-05, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 618/1250 [26:29<08:00,  1.31it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 619/1250 [26:30<07:41,  1.37it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2568, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 619/1250 [26:30<07:41,  1.37it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 620/1250 [26:31<07:28,  1.41it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1465, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 620/1250 [26:31<07:28,  1.41it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 621/1250 [26:31<07:18,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.333, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 621/1250 [26:31<07:18,  1.43it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 622/1250 [26:32<07:11,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2773, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 622/1250 [26:32<07:11,  1.45it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 623/1250 [26:33<07:06,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1699, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 623/1250 [26:33<07:06,  1.47it/s]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 624/1250 [26:33<07:02,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.416, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 624/1250 [26:33<07:02,  1.48it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 625/1250 [26:34<07:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1816, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 625/1250 [26:34<07:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 626/1250 [26:35<06:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4111, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 626/1250 [26:35<06:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 627/1250 [26:35<06:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3672, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 627/1250 [26:35<06:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 628/1250 [26:36<06:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3574, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 628/1250 [26:36<06:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 629/1250 [26:37<06:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1904, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 629/1250 [26:37<06:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 630/1250 [26:37<06:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1465, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 630/1250 [26:37<06:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m50%|█████     | 631/1250 [26:38<06:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5645, 'learning_rate': 2e-05, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 631/1250 [26:38<06:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 632/1250 [26:39<06:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4141, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 632/1250 [26:39<06:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 633/1250 [26:39<06:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5127, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 633/1250 [26:39<06:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 634/1250 [26:40<06:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4551, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 634/1250 [26:40<06:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 635/1250 [26:41<06:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4453, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 635/1250 [26:41<06:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 636/1250 [26:41<06:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2217, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 636/1250 [26:41<06:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 637/1250 [26:42<06:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8203, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 637/1250 [26:42<06:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 638/1250 [26:43<06:48,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4717, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 638/1250 [26:43<06:48,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 639/1250 [26:43<06:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2539, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 639/1250 [26:43<06:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 640/1250 [26:44<06:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1562, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 640/1250 [26:44<06:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 641/1250 [26:45<06:46,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3008, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 641/1250 [26:45<06:46,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 642/1250 [26:45<06:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0918, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 642/1250 [26:45<06:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 643/1250 [26:46<06:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3027, 'learning_rate': 2e-05, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 643/1250 [26:46<06:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 644/1250 [26:47<06:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2324, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 644/1250 [26:47<06:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 645/1250 [26:47<06:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3232, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 645/1250 [26:47<06:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 646/1250 [26:48<06:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1094, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 646/1250 [26:48<06:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 647/1250 [26:49<06:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.125, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 647/1250 [26:49<06:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 648/1250 [26:49<06:41,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.7178, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 648/1250 [26:49<06:41,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 649/1250 [26:50<06:40,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3467, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 649/1250 [26:50<06:40,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 650/1250 [26:51<06:40,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 650/1250 [26:51<06:40,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 651/1250 [26:51<06:39,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2354, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 651/1250 [26:51<06:39,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 652/1250 [26:52<06:39,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3652, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 652/1250 [26:52<06:39,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 653/1250 [26:53<06:38,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2373, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 653/1250 [26:53<06:38,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 654/1250 [26:53<06:37,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3906, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 654/1250 [26:53<06:37,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 655/1250 [26:54<06:37,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.75, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 655/1250 [26:54<06:37,  1.50it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 656/1250 [26:55<06:36,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.333, 'learning_rate': 2e-05, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 656/1250 [26:55<06:36,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 657/1250 [26:55<06:35,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2471, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 657/1250 [26:55<06:35,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 658/1250 [26:56<06:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9766, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 658/1250 [26:56<06:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 659/1250 [26:57<06:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2852, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 659/1250 [26:57<06:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 660/1250 [26:57<06:33,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1729, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 660/1250 [26:57<06:33,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 661/1250 [26:58<06:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3701, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 661/1250 [26:58<06:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 662/1250 [26:59<06:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3174, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 662/1250 [26:59<06:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 663/1250 [26:59<06:31,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3213, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 663/1250 [26:59<06:31,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 664/1250 [27:00<06:30,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 664/1250 [27:00<06:30,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 665/1250 [27:01<06:30,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2812, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 665/1250 [27:01<06:30,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 666/1250 [27:01<06:29,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3789, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 666/1250 [27:01<06:29,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 667/1250 [27:02<06:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9521, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 667/1250 [27:02<06:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 668/1250 [27:03<06:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6543, 'learning_rate': 2e-05, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 668/1250 [27:03<06:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 669/1250 [27:03<06:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3101, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 669/1250 [27:03<06:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 670/1250 [27:04<06:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3838, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 670/1250 [27:04<06:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 671/1250 [27:05<06:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2754, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 671/1250 [27:05<06:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 672/1250 [27:05<06:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 672/1250 [27:05<06:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 673/1250 [27:06<06:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8633, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 673/1250 [27:06<06:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 674/1250 [27:07<06:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2793, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 674/1250 [27:07<06:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 675/1250 [27:07<06:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5166, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 675/1250 [27:07<06:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 676/1250 [27:08<06:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0632, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 676/1250 [27:08<06:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 677/1250 [27:09<06:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.957, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 677/1250 [27:09<06:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 678/1250 [27:09<06:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.083, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 678/1250 [27:09<06:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 679/1250 [27:10<06:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2285, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 679/1250 [27:10<06:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 680/1250 [27:11<06:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4805, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 680/1250 [27:11<06:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 681/1250 [27:11<06:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0879, 'learning_rate': 2e-05, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 681/1250 [27:11<06:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 682/1250 [27:12<06:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1318, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 682/1250 [27:12<06:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 683/1250 [27:13<06:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2344, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 683/1250 [27:13<06:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 684/1250 [27:13<06:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9375, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 684/1250 [27:13<06:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 685/1250 [27:14<06:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5186, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 685/1250 [27:14<06:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 686/1250 [27:15<06:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1865, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 686/1250 [27:15<06:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 687/1250 [27:15<06:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2676, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 687/1250 [27:15<06:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 688/1250 [27:16<06:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3975, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 688/1250 [27:16<06:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 689/1250 [27:17<06:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0635, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 689/1250 [27:17<06:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 690/1250 [27:17<06:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0957, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 690/1250 [27:17<06:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 691/1250 [27:18<06:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5029, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 691/1250 [27:18<06:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 692/1250 [27:19<06:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2354, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 692/1250 [27:19<06:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 693/1250 [27:19<06:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2129, 'learning_rate': 2e-05, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 693/1250 [27:19<06:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 694/1250 [27:20<06:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2656, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 694/1250 [27:20<06:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 695/1250 [27:21<06:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0469, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 695/1250 [27:21<06:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 696/1250 [27:21<06:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3408, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 696/1250 [27:21<06:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 697/1250 [27:22<06:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9531, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 697/1250 [27:22<06:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 698/1250 [27:23<06:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0493, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 698/1250 [27:23<06:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 699/1250 [27:23<06:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0166, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 699/1250 [27:23<06:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 700/1250 [30:40<9:05:56, 59.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3857, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 700/1250 [30:40<9:05:56, 59.56s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 701/1250 [30:41<6:23:24, 41.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0762, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 701/1250 [30:41<6:23:24, 41.90s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 702/1250 [30:42<4:29:43, 29.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0327, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 702/1250 [30:42<4:29:43, 29.53s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 703/1250 [30:42<3:10:17, 20.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3115, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 703/1250 [30:42<3:10:17, 20.87s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 704/1250 [30:43<2:14:47, 14.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4238, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 704/1250 [30:43<2:14:47, 14.81s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 705/1250 [30:44<1:35:59, 10.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1934, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 705/1250 [30:44<1:35:59, 10.57s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 706/1250 [30:44<1:08:53,  7.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1089, 'learning_rate': 2e-05, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 706/1250 [30:44<1:08:53,  7.60s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 707/1250 [30:45<49:56,  5.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1963, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 707/1250 [30:45<49:56,  5.52s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 708/1250 [30:46<36:42,  4.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0625, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 708/1250 [30:46<36:42,  4.06s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 709/1250 [30:46<27:26,  3.04s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 709/1250 [30:46<27:26,  3.04s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 710/1250 [30:47<20:59,  2.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9971, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 710/1250 [30:47<20:59,  2.33s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 711/1250 [30:48<16:27,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3701, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 711/1250 [30:48<16:27,  1.83s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 712/1250 [30:48<13:17,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1611, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 712/1250 [30:48<13:17,  1.48s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 713/1250 [30:49<11:04,  1.24s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2246, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 713/1250 [30:49<11:04,  1.24s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 714/1250 [30:50<09:31,  1.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4307, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 714/1250 [30:50<09:31,  1.07s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 715/1250 [30:50<08:26,  1.06it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3311, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 715/1250 [30:50<08:26,  1.06it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 716/1250 [30:51<07:40,  1.16it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1777, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 716/1250 [30:51<07:40,  1.16it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 717/1250 [30:52<07:08,  1.24it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4756, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 717/1250 [30:52<07:08,  1.24it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 718/1250 [30:52<06:46,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2725, 'learning_rate': 2e-05, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 718/1250 [30:52<06:46,  1.31it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 719/1250 [30:53<06:30,  1.36it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3223, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 719/1250 [30:53<06:30,  1.36it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 720/1250 [30:54<06:19,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2061, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 720/1250 [30:54<06:19,  1.40it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 721/1250 [30:54<06:10,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1221, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 721/1250 [30:54<06:10,  1.43it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 722/1250 [30:55<06:05,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0518, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 722/1250 [30:55<06:05,  1.45it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 723/1250 [30:56<06:01,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2363, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 723/1250 [30:56<06:01,  1.46it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 724/1250 [30:56<05:57,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 724/1250 [30:56<05:57,  1.47it/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (809 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 725/1250 [30:57<05:55,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1758, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 725/1250 [30:57<05:55,  1.48it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 726/1250 [30:58<05:53,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9634, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 726/1250 [30:58<05:53,  1.48it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 727/1250 [30:58<05:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6289, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 727/1250 [30:58<05:51,  1.49it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 728/1250 [30:59<05:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0518, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 728/1250 [30:59<05:50,  1.49it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 729/1250 [31:00<05:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3936, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 729/1250 [31:00<05:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 730/1250 [31:00<05:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4775, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 730/1250 [31:00<05:48,  1.49it/s]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 731/1250 [31:01<05:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4277, 'learning_rate': 2e-05, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 731/1250 [31:01<05:47,  1.49it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 732/1250 [31:02<05:46,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1836, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 732/1250 [31:02<05:46,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 733/1250 [31:02<05:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0635, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 733/1250 [31:02<05:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 734/1250 [31:03<05:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3662, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 734/1250 [31:03<05:44,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 735/1250 [31:04<05:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0786, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 735/1250 [31:04<05:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 736/1250 [31:04<05:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3496, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 736/1250 [31:04<05:43,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 737/1250 [31:05<05:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9512, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 737/1250 [31:05<05:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 738/1250 [31:06<05:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1753, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 738/1250 [31:06<05:42,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 739/1250 [31:06<05:41,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3105, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 739/1250 [31:06<05:41,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 740/1250 [31:07<05:41,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1113, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 740/1250 [31:07<05:41,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 741/1250 [31:08<05:40,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3809, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 741/1250 [31:08<05:40,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 742/1250 [31:08<05:39,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2412, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 742/1250 [31:08<05:39,  1.50it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 743/1250 [31:09<05:38,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3574, 'learning_rate': 2e-05, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 743/1250 [31:09<05:38,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 744/1250 [31:10<05:38,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2695, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 744/1250 [31:10<05:38,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 745/1250 [31:10<05:37,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3652, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 745/1250 [31:10<05:37,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 746/1250 [31:11<05:36,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3291, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 746/1250 [31:11<05:36,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 747/1250 [31:12<05:35,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.126, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 747/1250 [31:12<05:35,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 748/1250 [31:12<05:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2451, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 748/1250 [31:12<05:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 749/1250 [31:13<05:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1025, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 749/1250 [31:13<05:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 750/1250 [31:14<05:33,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.373, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 750/1250 [31:14<05:33,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 751/1250 [31:14<05:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2188, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 751/1250 [31:14<05:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 752/1250 [31:15<05:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3105, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 752/1250 [31:15<05:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 753/1250 [31:16<05:31,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2256, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 753/1250 [31:16<05:31,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 754/1250 [31:16<05:31,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2725, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 754/1250 [31:16<05:31,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 755/1250 [31:17<05:30,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2285, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 755/1250 [31:17<05:30,  1.50it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 756/1250 [31:18<05:29,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.248, 'learning_rate': 2e-05, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 756/1250 [31:18<05:29,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 757/1250 [31:18<05:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1016, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 757/1250 [31:18<05:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 758/1250 [31:19<05:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1826, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 758/1250 [31:19<05:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 759/1250 [31:20<05:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0732, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 759/1250 [31:20<05:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 760/1250 [31:20<05:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0957, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 760/1250 [31:20<05:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 761/1250 [31:21<05:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1758, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 761/1250 [31:21<05:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 762/1250 [31:22<05:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8682, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 762/1250 [31:22<05:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 763/1250 [31:22<05:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4199, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 763/1250 [31:22<05:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 764/1250 [31:23<05:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 764/1250 [31:23<05:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████    | 765/1250 [31:24<05:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.417, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████    | 765/1250 [31:24<05:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 766/1250 [31:24<05:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0322, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 766/1250 [31:24<05:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 767/1250 [31:25<05:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2754, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 767/1250 [31:25<05:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 768/1250 [31:26<05:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.29, 'learning_rate': 2e-05, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 768/1250 [31:26<05:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 769/1250 [31:26<05:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1631, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 769/1250 [31:26<05:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 770/1250 [31:27<05:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1924, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 770/1250 [31:27<05:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 771/1250 [31:28<05:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5469, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 771/1250 [31:28<05:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 772/1250 [31:28<05:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.499, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 772/1250 [31:28<05:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 773/1250 [31:29<05:19,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5117, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 773/1250 [31:29<05:19,  1.49it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 774/1250 [31:30<05:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2061, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 774/1250 [31:30<05:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 775/1250 [31:30<05:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3184, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 775/1250 [31:30<05:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 776/1250 [31:31<05:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1348, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 776/1250 [31:31<05:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 777/1250 [31:32<05:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2324, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 777/1250 [31:32<05:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 778/1250 [31:32<05:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0176, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 778/1250 [31:32<05:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 779/1250 [31:33<05:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3018, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 779/1250 [31:33<05:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 780/1250 [31:34<05:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1064, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 780/1250 [31:34<05:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 781/1250 [31:34<05:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5488, 'learning_rate': 2e-05, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 781/1250 [31:34<05:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 782/1250 [31:35<05:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1802, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 782/1250 [31:35<05:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 783/1250 [31:36<05:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4111, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 783/1250 [31:36<05:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 784/1250 [31:36<05:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1963, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 784/1250 [31:36<05:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 785/1250 [31:37<05:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3232, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 785/1250 [31:37<05:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 786/1250 [31:38<05:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2207, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 786/1250 [31:38<05:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 787/1250 [31:38<05:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9683, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 787/1250 [31:38<05:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 788/1250 [31:39<05:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5801, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 788/1250 [31:39<05:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 789/1250 [31:40<05:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0068, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 789/1250 [31:40<05:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 790/1250 [31:40<05:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8247, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 790/1250 [31:40<05:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 791/1250 [31:41<05:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1074, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 791/1250 [31:41<05:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 792/1250 [31:42<05:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1416, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 792/1250 [31:42<05:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 793/1250 [31:42<05:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1416, 'learning_rate': 2e-05, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 793/1250 [31:42<05:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 794/1250 [31:43<05:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1123, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 794/1250 [31:43<05:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 795/1250 [31:44<05:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1045, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 795/1250 [31:44<05:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 796/1250 [31:44<05:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3828, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 796/1250 [31:44<05:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 797/1250 [31:45<05:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0791, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 797/1250 [31:45<05:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 798/1250 [31:46<05:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.917, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 798/1250 [31:46<05:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 799/1250 [31:46<05:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3682, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 799/1250 [31:46<05:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 800/1250 [35:04<7:26:56, 59.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6436, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 800/1250 [35:04<7:26:56, 59.59s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 801/1250 [35:04<5:13:46, 41.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1309, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 801/1250 [35:04<5:13:46, 41.93s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 802/1250 [35:05<3:40:40, 29.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1611, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 802/1250 [35:05<3:40:40, 29.56s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 803/1250 [35:06<2:35:39, 20.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2393, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 803/1250 [35:06<2:35:39, 20.89s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 804/1250 [35:06<1:50:14, 14.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0957, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 804/1250 [35:06<1:50:14, 14.83s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 805/1250 [35:07<1:18:31, 10.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4395, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 805/1250 [35:07<1:18:31, 10.59s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 806/1250 [35:08<56:21,  7.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3535, 'learning_rate': 2e-05, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 806/1250 [35:08<56:21,  7.62s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 807/1250 [35:08<40:52,  5.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1523, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 807/1250 [35:08<40:52,  5.54s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 808/1250 [35:09<30:03,  4.08s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.103, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 808/1250 [35:09<30:03,  4.08s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 809/1250 [35:10<22:29,  3.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2275, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 809/1250 [35:10<22:29,  3.06s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 810/1250 [35:10<17:12,  2.35s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1895, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 810/1250 [35:10<17:12,  2.35s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 811/1250 [35:11<13:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1738, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 811/1250 [35:11<13:31,  1.85s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 812/1250 [35:12<10:56,  1.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1699, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 812/1250 [35:12<10:56,  1.50s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 813/1250 [35:12<09:08,  1.25s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0977, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 813/1250 [35:12<09:08,  1.25s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 814/1250 [35:13<07:52,  1.08s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.998, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 814/1250 [35:13<07:52,  1.08s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 815/1250 [35:14<06:58,  1.04it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2393, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 815/1250 [35:14<06:58,  1.04it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 816/1250 [35:14<06:21,  1.14it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4453, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 816/1250 [35:14<06:21,  1.14it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 817/1250 [35:15<05:55,  1.22it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8809, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 817/1250 [35:15<05:55,  1.22it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 818/1250 [35:16<05:36,  1.28it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8555, 'learning_rate': 2e-05, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 818/1250 [35:16<05:36,  1.28it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 819/1250 [35:17<05:23,  1.33it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6155, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 819/1250 [35:17<05:23,  1.33it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 820/1250 [35:17<05:13,  1.37it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6621, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 820/1250 [35:17<05:13,  1.37it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 821/1250 [35:18<05:07,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2012, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 821/1250 [35:18<05:07,  1.40it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 822/1250 [35:19<05:03,  1.41it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.167, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 822/1250 [35:19<05:03,  1.41it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 823/1250 [35:19<04:59,  1.42it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2773, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 823/1250 [35:19<04:59,  1.42it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 824/1250 [35:20<04:57,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4961, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 824/1250 [35:20<04:57,  1.43it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 825/1250 [35:21<04:55,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9033, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 825/1250 [35:21<04:55,  1.44it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 826/1250 [35:21<04:54,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2617, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 826/1250 [35:21<04:54,  1.44it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 827/1250 [35:22<04:53,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.377, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 827/1250 [35:22<04:53,  1.44it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 828/1250 [35:23<04:51,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.252, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 828/1250 [35:23<04:51,  1.45it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 829/1250 [35:23<04:51,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4102, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 829/1250 [35:23<04:51,  1.45it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 830/1250 [35:24<04:50,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2402, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 830/1250 [35:24<04:50,  1.45it/s]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 831/1250 [35:25<04:49,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.124, 'learning_rate': 2e-05, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 831/1250 [35:25<04:49,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 832/1250 [35:25<04:48,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0117, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 832/1250 [35:25<04:48,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 833/1250 [35:26<04:48,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4395, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 833/1250 [35:26<04:48,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 834/1250 [35:27<04:47,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.7207, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 834/1250 [35:27<04:47,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 835/1250 [35:28<04:47,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3721, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 835/1250 [35:28<04:47,  1.44it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 836/1250 [35:28<04:46,  1.44it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4199, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 836/1250 [35:28<04:46,  1.44it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 837/1250 [35:29<04:45,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4482, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 837/1250 [35:29<04:45,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 838/1250 [35:30<04:44,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2207, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 838/1250 [35:30<04:44,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 839/1250 [35:30<04:43,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2588, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 839/1250 [35:30<04:43,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 840/1250 [35:31<04:42,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.165, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 840/1250 [35:31<04:42,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 841/1250 [35:32<04:42,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2324, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 841/1250 [35:32<04:42,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 842/1250 [35:32<04:41,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2715, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 842/1250 [35:32<04:41,  1.45it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 843/1250 [35:33<04:40,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1006, 'learning_rate': 2e-05, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 843/1250 [35:33<04:40,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 844/1250 [35:34<04:39,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1592, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 844/1250 [35:34<04:39,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 845/1250 [35:34<04:39,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4839, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 845/1250 [35:34<04:39,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 846/1250 [35:35<04:38,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1323, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 846/1250 [35:35<04:38,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 847/1250 [35:36<04:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4834, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 847/1250 [35:36<04:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 848/1250 [35:37<04:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4727, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 848/1250 [35:37<04:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 849/1250 [35:37<04:36,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2314, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 849/1250 [35:37<04:36,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 850/1250 [35:38<04:35,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1787, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 850/1250 [35:38<04:35,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 851/1250 [35:39<04:34,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1514, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 851/1250 [35:39<04:34,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 852/1250 [35:39<04:34,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2188, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 852/1250 [35:39<04:34,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 853/1250 [35:40<04:33,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.333, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 853/1250 [35:40<04:33,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 854/1250 [35:41<04:32,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3877, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 854/1250 [35:41<04:32,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 855/1250 [35:41<04:32,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0957, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 855/1250 [35:41<04:32,  1.45it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 856/1250 [35:42<04:31,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9756, 'learning_rate': 2e-05, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 856/1250 [35:42<04:31,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 857/1250 [35:43<04:30,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2959, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 857/1250 [35:43<04:30,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 858/1250 [35:43<04:30,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2451, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 858/1250 [35:43<04:30,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 859/1250 [35:44<04:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2529, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 859/1250 [35:44<04:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 860/1250 [35:45<04:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1724, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 860/1250 [35:45<04:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 861/1250 [35:45<04:28,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3379, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 861/1250 [35:45<04:28,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 862/1250 [35:46<04:27,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0205, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 862/1250 [35:46<04:27,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 863/1250 [35:47<04:26,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3711, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 863/1250 [35:47<04:26,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 864/1250 [35:48<04:26,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1201, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 864/1250 [35:48<04:26,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 865/1250 [35:48<04:25,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2256, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 865/1250 [35:48<04:25,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 866/1250 [35:49<04:24,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 866/1250 [35:49<04:24,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 867/1250 [35:50<04:23,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3545, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 867/1250 [35:50<04:23,  1.45it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 868/1250 [35:50<04:23,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1211, 'learning_rate': 2e-05, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 868/1250 [35:50<04:23,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 869/1250 [35:51<04:22,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3389, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 869/1250 [35:51<04:22,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 870/1250 [35:52<04:21,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1387, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 870/1250 [35:52<04:21,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 871/1250 [35:52<04:21,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4111, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 871/1250 [35:52<04:21,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 872/1250 [35:53<04:20,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.7168, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 872/1250 [35:53<04:20,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 873/1250 [35:54<04:19,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1533, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 873/1250 [35:54<04:19,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 874/1250 [35:54<04:19,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2979, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 874/1250 [35:54<04:19,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 875/1250 [35:55<04:18,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1689, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 875/1250 [35:55<04:18,  1.45it/s]\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mToken indices sequence length is longer than the specified maximum sequence length for this model (556 > 512). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m70%|███████   | 876/1250 [35:56<04:17,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0957, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 876/1250 [35:56<04:17,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 877/1250 [35:57<04:17,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3008, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 877/1250 [35:57<04:17,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 878/1250 [35:57<04:16,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3398, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 878/1250 [35:57<04:16,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 879/1250 [35:58<04:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0137, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 879/1250 [35:58<04:15,  1.45it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 880/1250 [35:59<04:13,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.334, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 880/1250 [35:59<04:13,  1.46it/s]\u001b[0m\n",
      "\u001b[34m70%|███████   | 881/1250 [35:59<04:10,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0918, 'learning_rate': 2e-05, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 881/1250 [35:59<04:10,  1.47it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 882/1250 [36:00<04:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1494, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 882/1250 [36:00<04:08,  1.48it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 883/1250 [36:01<04:06,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1455, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 883/1250 [36:01<04:06,  1.49it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 884/1250 [36:01<04:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2002, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 884/1250 [36:01<04:05,  1.49it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 885/1250 [36:02<04:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2031, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 885/1250 [36:02<04:04,  1.49it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 886/1250 [36:03<04:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4199, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 886/1250 [36:03<04:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 887/1250 [36:03<04:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4541, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 887/1250 [36:03<04:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 888/1250 [36:04<04:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5156, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 888/1250 [36:04<04:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 889/1250 [36:05<04:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0742, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 889/1250 [36:05<04:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 890/1250 [36:05<04:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0537, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 890/1250 [36:05<04:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 891/1250 [36:06<03:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.541, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 891/1250 [36:06<03:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 892/1250 [36:07<03:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3232, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 892/1250 [36:07<03:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 893/1250 [36:07<03:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9854, 'learning_rate': 2e-05, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 893/1250 [36:07<03:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 894/1250 [36:08<03:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1616, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 894/1250 [36:08<03:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 895/1250 [36:09<03:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1162, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 895/1250 [36:09<03:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 896/1250 [36:09<03:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.209, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 896/1250 [36:09<03:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 897/1250 [36:10<03:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2168, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 897/1250 [36:10<03:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 898/1250 [36:11<03:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0674, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 898/1250 [36:11<03:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 899/1250 [36:11<03:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2881, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 899/1250 [36:11<03:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 900/1250 [39:29<5:48:25, 59.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3545, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 900/1250 [39:29<5:48:25, 59.73s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 901/1250 [39:30<4:05:17, 42.17s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0137, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 901/1250 [39:30<4:05:17, 42.17s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 902/1250 [39:31<2:52:22, 29.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2178, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 902/1250 [39:31<2:52:22, 29.72s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 903/1250 [39:32<2:02:12, 21.13s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3926, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 903/1250 [39:32<2:02:12, 21.13s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 904/1250 [39:32<1:26:27, 14.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.333, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 904/1250 [39:32<1:26:27, 14.99s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 905/1250 [39:33<1:01:29, 10.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1982, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 905/1250 [39:33<1:01:29, 10.70s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 906/1250 [39:34<44:04,  7.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3643, 'learning_rate': 2e-05, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 906/1250 [39:34<44:04,  7.69s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 907/1250 [39:34<31:54,  5.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.376, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 907/1250 [39:34<31:54,  5.58s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 908/1250 [39:35<23:24,  4.11s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3486, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 908/1250 [39:35<23:24,  4.11s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 909/1250 [39:36<17:28,  3.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5234, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 909/1250 [39:36<17:28,  3.07s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 910/1250 [39:36<13:19,  2.35s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3105, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 910/1250 [39:36<13:19,  2.35s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 911/1250 [39:37<10:26,  1.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3672, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 911/1250 [39:37<10:26,  1.85s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 912/1250 [39:38<08:24,  1.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1943, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 912/1250 [39:38<08:24,  1.49s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 913/1250 [39:38<07:00,  1.25s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4404, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 913/1250 [39:38<07:00,  1.25s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 914/1250 [39:39<06:00,  1.07s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0601, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 914/1250 [39:39<06:00,  1.07s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 915/1250 [39:40<05:18,  1.05it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.835, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 915/1250 [39:40<05:18,  1.05it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 916/1250 [39:40<04:49,  1.15it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2988, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 916/1250 [39:40<04:49,  1.15it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 917/1250 [39:41<04:28,  1.24it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6191, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 917/1250 [39:41<04:28,  1.24it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 918/1250 [39:42<04:13,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2656, 'learning_rate': 2e-05, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 918/1250 [39:42<04:13,  1.31it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 919/1250 [39:42<04:03,  1.36it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3467, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 919/1250 [39:42<04:03,  1.36it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 920/1250 [39:43<03:55,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6797, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 920/1250 [39:43<03:55,  1.40it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 921/1250 [39:44<03:50,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1533, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 921/1250 [39:44<03:50,  1.43it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 922/1250 [39:44<03:46,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9951, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 922/1250 [39:44<03:46,  1.45it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 923/1250 [39:45<03:43,  1.46it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0811, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 923/1250 [39:45<03:43,  1.46it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 924/1250 [39:46<03:41,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0977, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 924/1250 [39:46<03:41,  1.47it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 925/1250 [39:46<03:39,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 925/1250 [39:46<03:39,  1.48it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 926/1250 [39:47<03:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1807, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 926/1250 [39:47<03:37,  1.49it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 927/1250 [39:48<03:36,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1221, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 927/1250 [39:48<03:36,  1.49it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 928/1250 [39:48<03:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4258, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 928/1250 [39:48<03:35,  1.49it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 929/1250 [39:49<03:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.249, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 929/1250 [39:49<03:34,  1.50it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 930/1250 [39:50<03:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0547, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 930/1250 [39:50<03:34,  1.49it/s]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 931/1250 [39:50<03:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1631, 'learning_rate': 2e-05, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 931/1250 [39:50<03:33,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 932/1250 [39:51<03:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8291, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 932/1250 [39:51<03:32,  1.50it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 933/1250 [39:52<03:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3369, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 933/1250 [39:52<03:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 934/1250 [39:52<03:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2607, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 934/1250 [39:52<03:32,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 935/1250 [39:53<03:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3389, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 935/1250 [39:53<03:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 936/1250 [39:54<03:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3496, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 936/1250 [39:54<03:31,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 937/1250 [39:54<03:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3154, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 937/1250 [39:54<03:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 938/1250 [39:55<03:28,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2305, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 938/1250 [39:55<03:28,  1.49it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 939/1250 [39:56<04:08,  1.25it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1016, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 939/1250 [39:56<04:08,  1.25it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 940/1250 [39:57<03:56,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3281, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 940/1250 [39:57<03:56,  1.31it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 941/1250 [39:58<03:46,  1.36it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4805, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 941/1250 [39:58<03:46,  1.36it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 942/1250 [39:58<03:40,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2686, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 942/1250 [39:58<03:40,  1.40it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 943/1250 [39:59<03:35,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4561, 'learning_rate': 2e-05, 'epoch': 0.75}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 943/1250 [39:59<03:35,  1.43it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 944/1250 [40:00<03:31,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9033, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 944/1250 [40:00<03:31,  1.45it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 945/1250 [40:00<03:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2637, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 945/1250 [40:00<03:29,  1.45it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 946/1250 [40:01<03:26,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2676, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 946/1250 [40:01<03:26,  1.47it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 947/1250 [40:02<03:24,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.168, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 947/1250 [40:02<03:24,  1.48it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 948/1250 [40:02<03:24,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1748, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 948/1250 [40:02<03:24,  1.48it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 949/1250 [40:03<03:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2993, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 949/1250 [40:03<03:22,  1.49it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 950/1250 [40:04<03:21,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 950/1250 [40:04<03:21,  1.49it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 951/1250 [40:04<03:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0049, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 951/1250 [40:04<03:20,  1.49it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 952/1250 [40:05<03:19,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3691, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 952/1250 [40:05<03:19,  1.49it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 953/1250 [40:06<03:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5049, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 953/1250 [40:06<03:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 954/1250 [40:06<03:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4326, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 954/1250 [40:06<03:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 955/1250 [40:07<03:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0928, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 955/1250 [40:07<03:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 956/1250 [40:08<03:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5098, 'learning_rate': 2e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 956/1250 [40:08<03:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 957/1250 [40:08<03:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2549, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 957/1250 [40:08<03:15,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 958/1250 [40:09<03:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2051, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 958/1250 [40:09<03:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 959/1250 [40:10<03:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1846, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 959/1250 [40:10<03:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 960/1250 [40:10<03:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5244, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 960/1250 [40:10<03:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 961/1250 [40:11<03:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2734, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 961/1250 [40:11<03:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 962/1250 [40:12<03:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0186, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 962/1250 [40:12<03:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 963/1250 [40:12<03:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1729, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 963/1250 [40:12<03:11,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 964/1250 [40:13<03:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3447, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 964/1250 [40:13<03:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 965/1250 [40:14<03:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3076, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 965/1250 [40:14<03:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 966/1250 [40:14<03:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1191, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 966/1250 [40:14<03:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 967/1250 [40:15<03:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8633, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 967/1250 [40:15<03:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 968/1250 [40:16<03:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1377, 'learning_rate': 2e-05, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 968/1250 [40:16<03:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 969/1250 [40:16<03:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9805, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 969/1250 [40:16<03:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 970/1250 [40:17<03:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.459, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 970/1250 [40:17<03:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 971/1250 [40:18<03:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9561, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 971/1250 [40:18<03:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 972/1250 [40:18<03:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1729, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 972/1250 [40:18<03:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 973/1250 [40:19<03:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0879, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 973/1250 [40:19<03:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 974/1250 [40:20<03:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.229, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 974/1250 [40:20<03:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 975/1250 [40:20<03:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3545, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 975/1250 [40:20<03:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 976/1250 [40:21<03:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1709, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 976/1250 [40:21<03:03,  1.49it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 977/1250 [40:22<03:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3799, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 977/1250 [40:22<03:02,  1.49it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 978/1250 [40:22<03:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5537, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 978/1250 [40:22<03:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 979/1250 [40:23<03:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3564, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 979/1250 [40:23<03:01,  1.49it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 980/1250 [40:24<03:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3506, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 980/1250 [40:24<03:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 981/1250 [40:24<02:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1777, 'learning_rate': 2e-05, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 981/1250 [40:24<02:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 982/1250 [40:25<02:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.04, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 982/1250 [40:25<02:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 983/1250 [40:26<02:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1157, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 983/1250 [40:26<02:59,  1.49it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 984/1250 [40:26<02:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2979, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 984/1250 [40:26<02:58,  1.49it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 985/1250 [40:27<02:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9854, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 985/1250 [40:27<02:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 986/1250 [40:28<02:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3203, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 986/1250 [40:28<02:57,  1.49it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 987/1250 [40:28<02:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5439, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 987/1250 [40:28<02:56,  1.49it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 988/1250 [40:29<02:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3125, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 988/1250 [40:29<02:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 989/1250 [40:30<02:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9453, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 989/1250 [40:30<02:54,  1.50it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 990/1250 [40:30<02:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4893, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 990/1250 [40:30<02:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 991/1250 [40:31<02:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0146, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 991/1250 [40:31<02:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 992/1250 [40:32<02:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2744, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 992/1250 [40:32<02:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 993/1250 [40:32<02:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5234, 'learning_rate': 2e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 993/1250 [40:32<02:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 994/1250 [40:33<02:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.251, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 994/1250 [40:33<02:50,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 995/1250 [40:34<02:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6045, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 995/1250 [40:34<02:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 996/1250 [40:34<02:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3779, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 996/1250 [40:34<02:49,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 997/1250 [40:35<02:48,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1377, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 997/1250 [40:35<02:48,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 998/1250 [40:36<02:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2168, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 998/1250 [40:36<02:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 999/1250 [40:36<02:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5674, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 999/1250 [40:36<02:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1000/1250 [43:52<4:06:56, 59.27s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2676, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1000/1250 [43:52<4:06:56, 59.27s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1001/1250 [43:53<2:53:02, 41.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4512, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1001/1250 [43:53<2:53:02, 41.70s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1002/1250 [43:54<2:01:28, 29.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0674, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1002/1250 [43:54<2:01:28, 29.39s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1003/1250 [43:54<1:25:30, 20.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8584, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1003/1250 [43:54<1:25:30, 20.77s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1004/1250 [43:55<1:00:26, 14.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2422, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1004/1250 [43:55<1:00:26, 14.74s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1005/1250 [43:56<42:57, 10.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3447, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1005/1250 [43:56<42:57, 10.52s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 1006/1250 [43:56<30:45,  7.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0752, 'learning_rate': 2e-05, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 1006/1250 [43:56<30:45,  7.56s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1007/1250 [43:57<22:15,  5.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5459, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1007/1250 [43:57<22:15,  5.49s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1008/1250 [43:58<16:19,  4.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1504, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1008/1250 [43:58<16:19,  4.05s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1009/1250 [43:58<12:10,  3.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5391, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1009/1250 [43:58<12:10,  3.03s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1010/1250 [43:59<09:17,  2.32s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4902, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1010/1250 [43:59<09:17,  2.32s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1011/1250 [44:00<07:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1211, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1011/1250 [44:00<07:16,  1.83s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1012/1250 [44:00<05:51,  1.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3887, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1012/1250 [44:00<05:51,  1.48s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1013/1250 [44:01<04:52,  1.23s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2734, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1013/1250 [44:01<04:52,  1.23s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1014/1250 [44:02<04:11,  1.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1562, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1014/1250 [44:02<04:11,  1.06s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 1015/1250 [44:02<03:42,  1.06it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9863, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 1015/1250 [44:02<03:42,  1.06it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1016/1250 [44:03<03:21,  1.16it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0889, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1016/1250 [44:03<03:21,  1.16it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1017/1250 [44:04<03:06,  1.25it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.915, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1017/1250 [44:04<03:06,  1.25it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1018/1250 [44:04<02:56,  1.31it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3184, 'learning_rate': 2e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 1018/1250 [44:04<02:56,  1.31it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1019/1250 [44:05<02:49,  1.37it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9102, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1019/1250 [44:05<02:49,  1.37it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1020/1250 [44:06<02:43,  1.40it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.498, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1020/1250 [44:06<02:43,  1.40it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1021/1250 [44:06<02:40,  1.43it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.208, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1021/1250 [44:06<02:40,  1.43it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1022/1250 [44:07<02:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2275, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1022/1250 [44:07<02:37,  1.45it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1023/1250 [44:08<02:34,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1875, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1023/1250 [44:08<02:34,  1.47it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1024/1250 [44:08<02:33,  1.47it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2217, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1024/1250 [44:08<02:33,  1.47it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1025/1250 [44:09<02:31,  1.48it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0918, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1025/1250 [44:09<02:31,  1.48it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1026/1250 [44:10<02:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1919, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1026/1250 [44:10<02:30,  1.49it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1027/1250 [44:10<02:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4414, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1027/1250 [44:10<02:29,  1.49it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1028/1250 [44:11<02:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9316, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1028/1250 [44:11<02:28,  1.50it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1029/1250 [44:12<02:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0107, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1029/1250 [44:12<02:27,  1.50it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1030/1250 [44:12<02:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0986, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1030/1250 [44:12<02:26,  1.50it/s]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1031/1250 [44:13<02:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3037, 'learning_rate': 2e-05, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 1031/1250 [44:13<02:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1032/1250 [44:14<02:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2637, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1032/1250 [44:14<02:25,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1033/1250 [44:14<02:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1992, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1033/1250 [44:14<02:24,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1034/1250 [44:15<02:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.374, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1034/1250 [44:15<02:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1035/1250 [44:16<02:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.873, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1035/1250 [44:16<02:23,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1036/1250 [44:16<02:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1279, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1036/1250 [44:16<02:22,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1037/1250 [44:17<02:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0898, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1037/1250 [44:17<02:21,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1038/1250 [44:18<02:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9531, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1038/1250 [44:18<02:20,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1039/1250 [44:18<02:20,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.374, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1039/1250 [44:18<02:20,  1.51it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1040/1250 [44:19<02:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9883, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1040/1250 [44:19<02:19,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1041/1250 [44:20<02:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0591, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1041/1250 [44:20<02:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1042/1250 [44:20<02:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.7344, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1042/1250 [44:20<02:18,  1.50it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1043/1250 [44:21<02:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2422, 'learning_rate': 2e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 1043/1250 [44:21<02:17,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1044/1250 [44:22<02:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2598, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1044/1250 [44:22<02:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1045/1250 [44:22<02:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4219, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1045/1250 [44:22<02:16,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1046/1250 [44:23<02:15,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0264, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 1046/1250 [44:23<02:15,  1.51it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1047/1250 [44:24<02:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1533, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1047/1250 [44:24<02:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1048/1250 [44:24<02:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1836, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1048/1250 [44:24<02:14,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1049/1250 [44:25<02:13,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.373, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1049/1250 [44:25<02:13,  1.51it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1050/1250 [44:26<02:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3955, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1050/1250 [44:26<02:13,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1051/1250 [44:26<02:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3564, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1051/1250 [44:26<02:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1052/1250 [44:27<02:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4922, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1052/1250 [44:27<02:12,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1053/1250 [44:28<02:11,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9463, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1053/1250 [44:28<02:11,  1.49it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1054/1250 [44:28<02:11,  1.49it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2842, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1054/1250 [44:28<02:11,  1.49it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1055/1250 [44:29<02:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0586, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1055/1250 [44:29<02:10,  1.50it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1056/1250 [44:30<02:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0938, 'learning_rate': 2e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 1056/1250 [44:30<02:09,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1057/1250 [44:30<02:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1143, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1057/1250 [44:30<02:08,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1058/1250 [44:31<02:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1084, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1058/1250 [44:31<02:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1059/1250 [44:32<02:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2227, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1059/1250 [44:32<02:07,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1060/1250 [44:32<02:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3174, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1060/1250 [44:32<02:06,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1061/1250 [44:33<02:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.541, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1061/1250 [44:33<02:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1062/1250 [44:34<02:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4453, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 1062/1250 [44:34<02:05,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1063/1250 [44:34<02:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.6162, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1063/1250 [44:34<02:04,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1064/1250 [44:35<02:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.957, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1064/1250 [44:35<02:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1065/1250 [44:36<02:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8643, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1065/1250 [44:36<02:03,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1066/1250 [44:36<02:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2939, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1066/1250 [44:36<02:02,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1067/1250 [44:37<02:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0078, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1067/1250 [44:37<02:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1068/1250 [44:38<02:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 1.9658, 'learning_rate': 2e-05, 'epoch': 0.85}\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 1068/1250 [44:38<02:01,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1069/1250 [44:38<02:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.271, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1069/1250 [44:38<02:00,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1070/1250 [44:39<01:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4414, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1070/1250 [44:39<01:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1071/1250 [44:40<01:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2529, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1071/1250 [44:40<01:59,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1072/1250 [44:40<01:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1768, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1072/1250 [44:40<01:58,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1073/1250 [44:41<01:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.5791, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1073/1250 [44:41<01:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1074/1250 [44:42<01:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3564, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1074/1250 [44:42<01:57,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1075/1250 [44:42<01:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1582, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1075/1250 [44:42<01:56,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1076/1250 [44:43<01:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.124, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1076/1250 [44:43<01:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1077/1250 [44:44<01:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2812, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1077/1250 [44:44<01:55,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1078/1250 [44:44<01:54,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2188, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 1078/1250 [44:44<01:54,  1.51it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1079/1250 [44:45<01:53,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0771, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1079/1250 [44:45<01:53,  1.51it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1080/1250 [44:46<01:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3496, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1080/1250 [44:46<01:53,  1.50it/s]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1081/1250 [44:46<01:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3848, 'learning_rate': 2e-05, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 1081/1250 [44:46<01:52,  1.50it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1082/1250 [44:47<01:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1846, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1082/1250 [44:47<01:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1083/1250 [44:48<01:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3428, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1083/1250 [44:48<01:51,  1.50it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1084/1250 [44:48<01:50,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3047, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1084/1250 [44:48<01:50,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1085/1250 [44:49<01:49,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.542, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1085/1250 [44:49<01:49,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1086/1250 [44:50<01:48,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3652, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1086/1250 [44:50<01:48,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1087/1250 [44:50<01:48,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1943, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1087/1250 [44:50<01:48,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1088/1250 [44:51<01:47,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.3486, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1088/1250 [44:51<01:47,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1089/1250 [44:52<01:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4385, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1089/1250 [44:52<01:47,  1.50it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1090/1250 [44:52<01:46,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2979, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1090/1250 [44:52<01:46,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1091/1250 [44:53<01:45,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.252, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1091/1250 [44:53<01:45,  1.51it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1092/1250 [44:54<01:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.0811, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1092/1250 [44:54<01:45,  1.50it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1093/1250 [44:54<01:44,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4512, 'learning_rate': 2e-05, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 1093/1250 [44:54<01:44,  1.51it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1094/1250 [44:55<01:43,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2109, 'learning_rate': 2e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1094/1250 [44:55<01:43,  1.51it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1095/1250 [44:56<01:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.2568, 'learning_rate': 2e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1095/1250 [44:56<01:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1096/1250 [44:56<01:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1992, 'learning_rate': 2e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1096/1250 [44:56<01:42,  1.51it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1097/1250 [44:57<01:41,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4287, 'learning_rate': 2e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1097/1250 [44:57<01:41,  1.51it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1098/1250 [44:58<01:40,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.1035, 'learning_rate': 2e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1098/1250 [44:58<01:40,  1.51it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1099/1250 [44:58<01:40,  1.51it/s]\u001b[0m\n",
      "\u001b[34m{'loss': 2.4805, 'learning_rate': 2e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 1099/1250 [44:58<01:40,  1.51it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6f7d2-80b1-4332-81f5-65e8336910c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35aceef6-e0f4-4d12-8546-d0793db373e7",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DJL DeepSpeed Container\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using from DeepSpeed and DJL. DJL provides for the serving framework while DeepSpeed is the key sharding library we leverage to enable hosting of large models.We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy the open source llama 7B model across GPU's on a ml.g5.48xlarge instance. Note that the llama 7B fp16 model can be deployed on single GPU such as g5.2xlarge (24GB VRAM), we jsut show the code which can deploy the llm accross multiple GPUs in SageMaker. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to https://arxiv.org/pdf/2207.00032.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedb008-79dd-4c20-9509-53a374a2a478",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. \n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. \n",
    "- `requirements.txt` has the required libraries needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07349695-9935-4d1e-bebf-22a8d95623c0",
   "metadata": {},
   "source": [
    "#### Serving.properties has engine parameter which tells the DJL model server to use the DeepSpeed engine to load the model.\n",
    "\n",
    "option.tensor_parallel_degree:  if we use the g5.48xlarge which has 8 GPUs, so we set the tensor_parallel_degree to 8.\n",
    "\n",
    "option.s3url:  you should use your model path here. And the s3 path must be ended with \"/\".\n",
    "\n",
    "batch_size:   it is for server side batch based on request level. You can set batch_size to the large value which can not result in the OOM. The current code about model.py is just demo for one prompt per client request.\n",
    "\n",
    "max_batch_delay:   it is counted by millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf83dcf-c882-4d60-b75b-95141cafb804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77005d-396c-4030-a12a-e87bde01bb83",
   "metadata": {},
   "source": [
    "**Copy python files from original model to finetuned model path**\n",
    "\n",
    "for examples:\n",
    "\n",
    "!aws s3 cp s3://sagemaker-us-west-2-928808346782/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/ s3://sagemaker-us-west-2-928808346782/llm/models/baichuan/output/baichuan-inc/Baichuan-7B/2023-08-31-06-50-39/baichuan_out/ --recursive --exclude \"*\" --include \"*.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b18efd-fe51-4133-a655-ba4b338adf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original *.py files for finetuned model\n",
    "!aws s3 cp ${s3_url_original_model} ${s3_url_finetuned_model} --recursive --exclude \"*\" --include \"*.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd206b4-6cb7-44a2-b9b5-36f57e8dfa8b",
   "metadata": {},
   "source": [
    "**Copy finetune cell output above, modify 'option.s3url' value**\n",
    "\n",
    "for example, option.s3url=s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-10-05-32-33/llama_out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91feb354-4023-40ce-83a8-3c683c83a8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url=${option.s3url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10af817-98d3-41d1-8725-00246ca42668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "transformers==4.29.2\n",
    "sagemaker\n",
    "nvgpu\n",
    "xformers\n",
    "peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a727878-67c1-4e2f-8b5f-e059d10e54bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import deepspeed\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, torch_dtype=torch.float16, use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    " \n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    \n",
    "    try:\n",
    "        if not model:\n",
    "            model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        if inputs.is_empty():\n",
    "            return None\n",
    "\n",
    "        data = inputs.get_as_json()    \n",
    "\n",
    "        input_data = data[\"inputs\"]\n",
    "        params = data[\"parameters\"]\n",
    "\n",
    "        logging.info(f\"data               : {data}\")\n",
    "        logging.info(f\"input_data         : {input_data}\")\n",
    "        logging.info(f\"params             : {params}\")\n",
    "        \n",
    "        inputs_tokenized = tokenizer(input_data, return_tensors='pt')\n",
    "        inputs_tokenized = inputs_tokenized.to('cuda:0')\n",
    "        logging.info(f\"inputs_tokenized   : {inputs_tokenized}\")\n",
    "\n",
    "        pred = model.generate(**inputs_tokenized, **params)\n",
    "        logging.info(f\"pred               : {pred}\")\n",
    "        \n",
    "        response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)\n",
    "        # response = tokenizer.decode(pred[0], skip_special_tokens=True)\n",
    "        logging.info(f\"response           : {response}\")\n",
    "\n",
    "        result = {\"outputs\": response}\n",
    "        logging.info(f\"result             : {result}\")\n",
    "\n",
    "        return Output().add({\"code\":0,\"msg\":\"ok\",\"data\":result})\n",
    "    except Exception as e:\n",
    "        return Output().add_as_json({\"code\":-1,\"msg\":e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff846e6-2a3b-4305-a77e-d7797770297f",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727e6d5-074f-4ac6-b553-b4a97f62056a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sage_session = sagemaker.Session()\n",
    "model_bucket = sage_session.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"llm/models/baichuan2/code-7b-base\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ed5da-49db-44c0-8f1f-2a416b6c4de9",
   "metadata": {},
   "source": [
    "**Image URI for the DJL container is being used here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1fdc1-1718-41d1-b06e-44075a51ceea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Note that: you can modify the image url according to your specific region.\n",
    "# inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    "\n",
    "inference_image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a87eb4-0b3b-4199-8438-55f160d19b64",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44652f16-8321-4b1a-a9bd-143eb80ff0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!tar czvf model.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb8c4d-b4fd-444f-951c-25b7f01dc9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sage_session.upload_data(\"model.tar.gz\", model_bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bb4e8-ca46-49da-a1d3-59fbc930f250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"S3 Model Bucket is -- > {model_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b9914-3879-4839-a568-322d66bd0ea2",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.g5.2xlarge \n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 15*60 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd73c8ca-835f-4300-9506-674376102c82",
   "metadata": {
    "tags": []
   },
   "source": [
    "One of the key parameters here is **TENSOR_PARALLEL_DEGREE** which essentially tells the DeepSpeed library to partition the models along 8 GPU's. This is a tunable and configurable parameter.\n",
    "\n",
    "This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeed you can follow the link https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08367121-837e-4dc0-9be5-97a3a83adeff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "model_name = name_from_base(f\"baichuan2-7b-base-finetuned\")\n",
    "print(model_name)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c503b35-787f-4e22-af96-cdfdc254f84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"VolumeSizeInGB\" : 300,\n",
    "            #\"ModelDataDownloadTimeoutInSeconds\": 15*60,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e824c3-2472-4e1f-8853-061a55e1ade6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d72d7-8b49-4b21-be6c-83c9f29ecc8a",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abc448-d60e-4b59-bfc6-65fde9858c25",
   "metadata": {},
   "source": [
    "### This step can take ~ 15 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d330f0-4929-4cb2-93cf-2e595c23ca63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf97ea-474b-46fb-b6c5-3e85b910dc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84bfaff5-b4a0-4875-95b7-266320e2d30b",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83cc51-172e-44b8-a00e-e26f669e957c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "parameters = {\n",
    "  \"early_stopping\": True,\n",
    "  \"max_new_tokens\": 128,\n",
    "  # \"min_new_tokens\": 128,\n",
    "  \"do_sample\": True,\n",
    "  # \"temperature\": 1.0,\n",
    "}\n",
    "\n",
    "prompt = \"世界上第二高的山峰是哪座\"\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\"    : prompt,\n",
    "                \"parameters\": parameters\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "response_model['Body'].read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5d7bd-defe-45fa-985c-1f7cf0958ccb",
   "metadata": {},
   "source": [
    "### Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb6bdb-6449-4b88-8b62-9c8d069ccd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153590f9-2659-487a-b002-554c745d7aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
